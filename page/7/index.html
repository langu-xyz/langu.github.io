<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="网络安全,黑客,JAVA安全,代码审计,渗透测试,入侵,SRC,扫描,WEB安全,移动安全,PHP">
    <meta name="description" content="蓝骨, langu.xyz">
    <meta name="author" content="langu_xyz">
    
        <title>
            
                        langu_xyz
        </title>
        
<link rel="stylesheet" href="/css/style.css">

            <link rel="shortcut icon" href="/images/logo.jpeg">
                
<link rel="stylesheet" href="/css/font-awesome.min.css">

                    <script id="hexo-configurations">
    let KEEP = window.KEEP || {};
    KEEP.hexo_config = {"hostname":"blog.langu.xyz","root":"/","language":"zh","path":"search.json"};
    KEEP.theme_config = {"toc":{"enable":true,"number":false,"expand_all":false,"init_open":true},"style":{"primary_color":"#0066CC","avatar":"/images/logo.jpeg","favicon":"/images/logo.jpeg","article_img_align":"center","left_side_width":"260px","content_max_width":"920px","hover":{"shadow":true,"scale":false},"first_screen":{"enable":true,"background_img":"/images/shamo.jpg","description":"XYZ(无限未来)"},"scroll":{"progress_bar":{"enable":true},"percent":{"enable":false}}},"local_search":{"enable":true,"preload":true},"code_copy":{"enable":true,"style":"default"},"pjax":{"enable":true},"lazyload":{"enable":true},"version":"3.4.3"};
    KEEP.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
  </script>

                    <script>
                        var _hmt = _hmt || [];
                        (function() {
                          var hm = document.createElement("script");
                          hm.src = "https://hm.baidu.com/hm.js?1cd6e64ff252acf24b707985fcec8850";
                          var s = document.getElementsByTagName("script")[0]; 
                          s.parentNode.insertBefore(hm, s);
                        })();
                        </script>
                        
<meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="langu_xyz" type="application/atom+xml">
</head>
<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
        <span class="pjax-progress-bar"></span>
        <span class="pjax-progress-icon">
            <i class="fas fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            <header class="header-wrapper">

    <div class="header-content">
        <div class="left">
            
                    <a class="logo-title" href="/">
                        langu_xyz
                    </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            <a class="active" href="/" >
                                HOME
                            </a>
                        </li>
                        
                        <li class="menu-item">
                            <a class="" href="/tags/THINK/" >
                                THINK
                            </a>
                        </li>
                        
                            
                                <li class="menu-item search search-popup-trigger">
                                    <i class="fas fa-search"></i>
                                </li>
                                
                                
                </ul>
            </div>
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                    
                        <div class="icon-item menu-bar">
                            <div class="menu-bar-middle"></div>
                        </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    <a class="active" href="/" >
                        HOME
                    </a>
                </li>
                
                <li class="drawer-menu-item flex-center">
                    <a class="" href="/tags/THINK/" >
                        THINK
                    </a>
                </li>
                
        </ul>
    </div>

    <div class="window-mask"></div>

</header>
        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="home-content-container fade-in-down-animation">
    <ul class="home-article-list">
        
            <li class="home-article-item">

                

                <h3 class="home-article-title">
                    <a href="/2020%20IoT%20Threat%20Report%20%E8%A7%A3%E8%AF%BB/">
                        2020 IoT Threat Report 解读
                    </a>
                </h3>

                <!-- <div class="home-article-content markdown-body">
                    
                        今天看到PALO ALTO和Unit 42联合发布了这个不到20页的报告，读了一下，虽然篇幅稍短，但是内容还是很有价值的，结合笔者去年在做的事情，浅薄的解读下，有兴趣的可以阅读原文
0x01 概要报告主要分为现阶段的IoT安全格局、Top IoT Threats、结论建议三个部分。
第一部分主要讲现阶段IoT的安全格局，现在企业缺乏完善的IoT资产管理，缺少相应的安全产品去保护IoT设备，人力资源缺乏，整体风险很高，其中健康医疗行业风险特别高；
第二部分重点讲了现阶段IoT的网络攻击、密码攻击、蠕虫等Top 威胁，同时还提到了因为没打补丁的设备以及老协议，导致攻击的横向移动，越来越多的威胁演化为专门针对IoT的场景；
第三部分主要是讲如何解决这些威胁，4个步骤和2个最佳实践，下文会细讲。
0x02 IoT Security LandscapeIoT是快速发展的，同时存在着很大的安全问题（数据支撑这里就不详细列了，例如2019比2018增长了21.5%的IoT设备数量、98%的IoT传输没有加密等）。
1、企业缺少工具去识别资产和保护IoT设备

IT 无法准确识别IoT资产

报告中认为像传统IT依靠IP和OS来进行资产管理的方式对于IoT场景是不完善的。只有准确的识别出IoT设备的类型，才可以准确的进行网络规划、安全策略部署等，可以连续的跟踪IoT设备的行为，而不是仅仅根据一个动态的IP。
笔者按：从笔者这一年的经历来看，这点说的非常贴切，IoT资产的管理和准入一直是个很大的痛点，通过IP来定位机器很容易丢掉，例如今天发现环境里某个IP的设备上有木马，然后去排查的时候，却发现因为DHCP随机分配IP的缘故，无法定位到问题设备了。当然这个例子有点极端，在泛IoT的场景下，mac地址也是一个非常核心的设备资产数据，但是也存在一些不足，因为泛IoT场景下设备的多样性，要准确的实现IoT设备资产管理需要多种方式的综合运用，例如流量、网络运维设备、人工排查等。

现有的安全产品大多不支持IoT设备

EDR等安全产品不支持IoT设备，PC上的安全产品会将IoT设备识别为未知类型，无法准确的识别风险和处置。基于网络的安全产品可以发现一部分风险，但是无法准确的识别、追踪IoT设备。
笔者按：这里指的IoT设备和现在国内大多数场景下的泛IoT设备含义有点不同，和监控摄像头这类产品比较类似。面对IoT环境下的设备多样性，基于流量的威胁检测成了大多数厂商的共同选择，例如本报告的发布者平底锅，当然还有笔者也在做这方面的尝试。对于发现威胁后的准确识别追踪，就需要先将IoT设备资产进行准确的识别和有效的管理。

在IT和OT之间，企业缺少足够的人力资源

IT主要关注IT资产,例如电脑、网络、打印机等，OT(operational technology)主要关注非IT设备，也就是上文笔者提到的泛IoT设备。因为IT和OT往往团队不在一个，而且因为电脑等IT资产发展迅速，可以获得更多的资源。而IoT设备为了稳定性(和原文有点差别，这里来自笔者实际经历的解读)，漏洞往往没有人去主动修复，存在着很大的风险。
2、企业现在面临着巨大的IoT安全风险
公司内的IoT设备(摄像头、打印机等)因为缺少IT维护，存在着巨大的风险。

3、医疗保健行业的状况是非常危急的
  医疗设备运行着过时的操作系统
  组织内缺少安全防御能力
  医疗设备的操作系统是非常脆弱的

4、最基本的网络隔离最佳实践没有遵守
最简单的IoT风险补救措施就是网络细分，可以有效组织风险的横向移动。但是更多的情况下，网络划分时，没有严格细分，例如在医疗保健行业，将医疗设备和打印机等划分到一起。同时还提到了最理想的情况是进行微网络划分(在某些高危场景下确实应该这样)。
0x03 Top IoT Threats针对IoT设备的威胁伴随着新的攻击技术在不断的演化，例如僵尸网络和蠕虫等
1、网络攻击、密码攻击、IoT蠕虫威胁位居榜首


利用目标设备的漏洞

  IoT设备的特性特别容易成为被攻击的目标，它们往往成为攻击者入侵其它系统的跳板。

密码攻击

  笔者按：默认密码和弱密码是真的痛，无论是在应用上还是在IoT设备上。

IoT蠕虫变得比IoT僵尸网络更常见

  笔者按：随着这几年勒索病毒的兴起，针对泛IoT设备的攻击主要都变成了这个，当然挖矿木马也非常常见。利用IoT僵尸网络的DDOS由于了解的不深，这里就不班门弄斧了。
2、没打补丁的设备、老旧的协议：横向移动的入口

补丁覆盖率低

  笔者按：IoT设备往往会因为版本迭代，逐渐放弃对老版本的更新支持，同时因为设备运行环境及稳定性需求，往往会放弃给设备打安全补丁。

老旧的OT协议

  这种情况更多的出现的工控环境下，随着网络边界的消失，这些老协议的风险正在暴露出来。

横向移动

  57%的IoT设备容易受到中等或高强度的攻击，使得IoT设备成为攻击者的进攻入口。
3、许多威胁正在演化为专门针对IoT环境

P2P通信的特点

  使得攻击可以最小化的与外界通信来控制内网环境下的IoT设备集群。

为host而战

  病毒之间会互相干掉对方，争夺资源。

病毒的变种

  例如Mirai系列
0x04 总结和建议1、4个步骤来降低IoT风险(虽然不全面，但是很大程度下降低了IoT的风险)
  1、IoT设备资产发现；
  2、打补丁；
  3、细划分VLANs；

  4、实时监控。

  笔者吐槽：这几个步骤无理反驳，还是去买他家的盒子吧。吐槽归吐槽，这几个步骤对于现在大多数的泛IoT环境是非常有效的，但是如何做到是个难题，也是笔者去年和未来要努力去达到的。
最佳实践1：整体思考IoT的生命周期


1、识别：设备准入
2、边界：NAC和Firewall结合（据笔者了解有些团队已经在做了）
3、安全：基于流量的威胁发现（笔者正在做的事情）
4、最优化：提高IoT设备的使用率
5、管理：实时监控、报警
6、回收：IoT设备的回收审计流程

最佳实践2：通过产品集成将安全性扩展到所有的IoT设备
安全产品集成包括以下：

 Asset management and computerized maintenance management systems (CMMS)
 Security information and event management (SIEM) 
 Security orchestration, automation, and response (SOAR) 
 Next-generation firewalls (NGFW) 
 Network access control (NAC) 
 Wireless/Network management solutions

笔者总结
~~ 这个报告虽然篇幅较短，但是不得不说平底锅的盒子贵有贵的道理，这篇报告的绝大部分都击中了现在IoT环境，特别是泛IoT环境所面临的安全威胁，整体解决思路和笔者正在做的大致相同。不过报告并没有说到具体如何落地，和绝大多数安全厂商一样，有点空中楼阁的感觉。但是经过笔者去年的验证，这两个落地实践的可行性是没有问题的，但是如何落地，长路漫漫，一点一点来了。~~
报告地址：https://start.paloaltonetworks.com/unit-42-iot-threat-report?utm_source=marketo&amp;utm_medium=email&amp;utm_campaign=AMERICAS-DA-EN-20-03-10-7010g000001JJOZAA4-P3-Strata-Unit%2042%20IoT%20Report.Americas-DA-EN-20-03-10-XX-P3-Strata_IoT%20Report%20A/B

                    
                </div> -->
                <div class="article-content markdown-body">
                    <p>今天看到PALO ALTO和Unit 42联合发布了这个不到20页的报告，读了一下，虽然篇幅稍短，但是内容还是很有价值的，结合笔者去年在做的事情，浅薄的解读下，有兴趣的可以阅读原文</p>
<h2 id="0x01-概要"><a href="#0x01-概要" class="headerlink" title="0x01 概要"></a>0x01 概要</h2><p>报告主要分为现阶段的IoT安全格局、Top IoT Threats、结论建议三个部分。</p>
<p>第一部分主要讲现阶段IoT的安全格局，现在企业缺乏完善的IoT资产管理，缺少相应的安全产品去保护IoT设备，人力资源缺乏，整体风险很高，其中健康医疗行业风险特别高；</p>
<p>第二部分重点讲了现阶段IoT的网络攻击、密码攻击、蠕虫等Top 威胁，同时还提到了因为没打补丁的设备以及老协议，导致攻击的横向移动，越来越多的威胁演化为专门针对IoT的场景；</p>
<p>第三部分主要是讲如何解决这些威胁，4个步骤和2个最佳实践，下文会细讲。</p>
<h2 id="0x02-IoT-Security-Landscape"><a href="#0x02-IoT-Security-Landscape" class="headerlink" title="0x02 IoT Security Landscape"></a>0x02 IoT Security Landscape</h2><p>IoT是快速发展的，同时存在着很大的安全问题（数据支撑这里就不详细列了，例如2019比2018增长了21.5%的IoT设备数量、98%的IoT传输没有加密等）。</p>
<p><strong>1、企业缺少工具去识别资产和保护IoT设备</strong></p>
<ul>
<li>IT 无法准确识别IoT资产</li>
</ul>
<p>报告中认为像传统IT依靠IP和OS来进行资产管理的方式对于IoT场景是不完善的。只有准确的识别出IoT设备的类型，才可以准确的进行网络规划、安全策略部署等，可以连续的跟踪IoT设备的行为，而不是仅仅根据一个动态的IP。</p>
<p>笔者按：从笔者这一年的经历来看，这点说的非常贴切，IoT资产的管理和准入一直是个很大的痛点，通过IP来定位机器很容易丢掉，例如今天发现环境里某个IP的设备上有木马，然后去排查的时候，却发现因为DHCP随机分配IP的缘故，无法定位到问题设备了。当然这个例子有点极端，在泛IoT的场景下，mac地址也是一个非常核心的设备资产数据，但是也存在一些不足，因为泛IoT场景下设备的多样性，要准确的实现IoT设备资产管理需要多种方式的综合运用，例如流量、网络运维设备、人工排查等。</p>
<ul>
<li>现有的安全产品大多不支持IoT设备</li>
</ul>
<p>EDR等安全产品不支持IoT设备，PC上的安全产品会将IoT设备识别为未知类型，无法准确的识别风险和处置。基于网络的安全产品可以发现一部分风险，但是无法准确的识别、追踪IoT设备。</p>
<p>笔者按：这里指的IoT设备和现在国内大多数场景下的泛IoT设备含义有点不同，和监控摄像头这类产品比较类似。面对IoT环境下的设备多样性，基于流量的威胁检测成了大多数厂商的共同选择，例如本报告的发布者平底锅，当然还有笔者也在做这方面的尝试。对于发现威胁后的准确识别追踪，就需要先将IoT设备资产进行准确的识别和有效的管理。</p>
<ul>
<li>在IT和OT之间，企业缺少足够的人力资源</li>
</ul>
<p>IT主要关注IT资产,例如电脑、网络、打印机等，OT(operational technology)主要关注非IT设备，也就是上文笔者提到的泛IoT设备。因为IT和OT往往团队不在一个，而且因为电脑等IT资产发展迅速，可以获得更多的资源。而IoT设备为了稳定性(和原文有点差别，这里来自笔者实际经历的解读)，漏洞往往没有人去主动修复，存在着很大的风险。</p>
<p><strong>2、企业现在面临着巨大的IoT安全风险</strong></p>
<p>公司内的IoT设备(摄像头、打印机等)因为缺少IT维护，存在着巨大的风险。</p>
<p><img lazyload src="/images/loading.svg" data-src="2020-03-12-01-03-32.png" alt="2020-03-12-01-03-32"></p>
<p><strong>3、医疗保健行业的状况是非常危急的</strong></p>
<pre><code>  医疗设备运行着过时的操作系统
  组织内缺少安全防御能力
  医疗设备的操作系统是非常脆弱的
</code></pre>
<p><strong>4、最基本的网络隔离最佳实践没有遵守</strong></p>
<p>最简单的IoT风险补救措施就是网络细分，可以有效组织风险的横向移动。但是更多的情况下，网络划分时，没有严格细分，例如在医疗保健行业，将医疗设备和打印机等划分到一起。同时还提到了最理想的情况是进行微网络划分(在某些高危场景下确实应该这样)。</p>
<h2 id="0x03-Top-IoT-Threats"><a href="#0x03-Top-IoT-Threats" class="headerlink" title="0x03 Top IoT Threats"></a>0x03 Top IoT Threats</h2><p>针对IoT设备的威胁伴随着新的攻击技术在不断的演化，例如僵尸网络和蠕虫等</p>
<p><strong>1、网络攻击、密码攻击、IoT蠕虫威胁位居榜首</strong></p>
<p><img lazyload src="/images/loading.svg" data-src="2020-03-12-01-23-24.png" alt="2020-03-12-01-23-24"></p>
<ul>
<li>利用目标设备的漏洞</li>
</ul>
<p>  IoT设备的特性特别容易成为被攻击的目标，它们往往成为攻击者入侵其它系统的跳板。</p>
<ul>
<li>密码攻击</li>
</ul>
<p>  笔者按：默认密码和弱密码是真的痛，无论是在应用上还是在IoT设备上。</p>
<ul>
<li>IoT蠕虫变得比IoT僵尸网络更常见</li>
</ul>
<p>  笔者按：随着这几年勒索病毒的兴起，针对泛IoT设备的攻击主要都变成了这个，当然挖矿木马也非常常见。利用IoT僵尸网络的DDOS由于了解的不深，这里就不班门弄斧了。</p>
<p><strong>2、没打补丁的设备、老旧的协议：横向移动的入口</strong></p>
<ul>
<li>补丁覆盖率低</li>
</ul>
<p>  笔者按：IoT设备往往会因为版本迭代，逐渐放弃对老版本的更新支持，同时因为设备运行环境及稳定性需求，往往会放弃给设备打安全补丁。</p>
<ul>
<li>老旧的OT协议</li>
</ul>
<p>  这种情况更多的出现的工控环境下，随着网络边界的消失，这些老协议的风险正在暴露出来。</p>
<ul>
<li>横向移动</li>
</ul>
<p>  57%的IoT设备容易受到中等或高强度的攻击，使得IoT设备成为攻击者的进攻入口。</p>
<p><strong>3、许多威胁正在演化为专门针对IoT环境</strong></p>
<ul>
<li>P2P通信的特点</li>
</ul>
<p>  使得攻击可以最小化的与外界通信来控制内网环境下的IoT设备集群。</p>
<ul>
<li>为host而战</li>
</ul>
<p>  病毒之间会互相干掉对方，争夺资源。</p>
<ul>
<li>病毒的变种</li>
</ul>
<p>  例如Mirai系列</p>
<h2 id="0x04-总结和建议"><a href="#0x04-总结和建议" class="headerlink" title="0x04 总结和建议"></a>0x04 总结和建议</h2><p><strong>1、4个步骤来降低IoT风险</strong>(虽然不全面，但是很大程度下降低了IoT的风险)</p>
<pre><code>  1、IoT设备资产发现；
  2、打补丁；
  3、细划分VLANs；

  4、实时监控。
</code></pre>
<p>  笔者吐槽：这几个步骤无理反驳，还是去买他家的盒子吧。吐槽归吐槽，这几个步骤对于现在大多数的泛IoT环境是非常有效的，但是如何做到是个难题，也是笔者去年和未来要努力去达到的。</p>
<p><strong>最佳实践1：整体思考IoT的生命周期</strong></p>
<p><img lazyload src="/images/loading.svg" data-src="2020-03-12-01-53-44.png" alt="2020-03-12-01-53-44"></p>
<ul>
<li>1、识别：设备准入</li>
<li>2、边界：NAC和Firewall结合（据笔者了解有些团队已经在做了）</li>
<li>3、安全：基于流量的威胁发现（笔者正在做的事情）</li>
<li>4、最优化：提高IoT设备的使用率</li>
<li>5、管理：实时监控、报警</li>
<li>6、回收：IoT设备的回收审计流程</li>
</ul>
<p><strong>最佳实践2：通过产品集成将安全性扩展到所有的IoT设备</strong></p>
<p>安全产品集成包括以下：</p>
<ul>
<li> Asset management and computerized maintenance management systems (CMMS)</li>
<li> Security information and event management (SIEM) </li>
<li> Security orchestration, automation, and response (SOAR) </li>
<li> Next-generation firewalls (NGFW) </li>
<li> Network access control (NAC) </li>
<li> Wireless/Network management solutions</li>
</ul>
<p><del>笔者总结</del></p>
<p>~~ 这个报告虽然篇幅较短，但是不得不说平底锅的盒子贵有贵的道理，这篇报告的绝大部分都击中了现在IoT环境，特别是泛IoT环境所面临的安全威胁，整体解决思路和笔者正在做的大致相同。不过报告并没有说到具体如何落地，和绝大多数安全厂商一样，有点空中楼阁的感觉。但是经过笔者去年的验证，这两个落地实践的可行性是没有问题的，但是如何落地，长路漫漫，一点一点来了。~~</p>
<p>报告地址：<a class="link" target="_blank" rel="noopener" href="https://start.paloaltonetworks.com/unit-42-iot-threat-report?utm_source=marketo&amp;utm_medium=email&amp;utm_campaign=AMERICAS-DA-EN-20-03-10-7010g000001JJOZAA4-P3-Strata-Unit%2042%20IoT%20Report.Americas-DA-EN-20-03-10-XX-P3-Strata_IoT%20Report%20A/B">https://start.paloaltonetworks.com/unit-42-iot-threat-report?utm_source=marketo&amp;utm_medium=email&amp;utm_campaign=AMERICAS-DA-EN-20-03-10-7010g000001JJOZAA4-P3-Strata-Unit%2042%20IoT%20Report.Americas-DA-EN-20-03-10-XX-P3-Strata_IoT%20Report%20A/B<i class="fas fa-external-link-alt"></i></a></p>

                </div>

                <div class="home-article-meta-info-container">
    <div class="home-article-meta-info">
        <span><i class="fas fa-history"></i>&nbsp;<span class="home-article-date" data-date="Thu Mar 12 2020 12:00:00 GMT+0800">2020-03-12</span></span>
        
            <span class="home-article-category"><i class="fas fa-folder"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/categories/IoT%E5%AE%89%E5%85%A8/">IoT安全</a>&nbsp;
                        </li>
                    
                    
                </ul>
            </span>
        
        
            <span class="home-article-tag">
                <i class="fas fa-tags"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/tags/IoT/">IoT</a>&nbsp;
                        </li>
                    
                </ul>
            </span>
        
    </div>

    <a href="/2020%20IoT%20Threat%20Report%20%E8%A7%A3%E8%AF%BB/">Read more&nbsp;<i class="fas fa-angle-right"></i></a>
</div>

            </li>
        
            <li class="home-article-item">

                

                <h3 class="home-article-title">
                    <a href="/%E5%BA%94%E7%94%A8%E5%AE%89%E5%85%A8%E8%AF%84%E5%AE%A1%E4%B8%AD%E7%9A%84%E4%B8%89%E4%B8%AA%E5%85%B3%E9%94%AE%E8%8A%82%E7%82%B9%E5%8F%8A%E6%8A%93%E6%89%8B%E5%AE%9E%E7%8E%B0/">
                        应用安全评审中的三个关键节点及抓手实现
                    </a>
                </h3>

                <!-- <div class="home-article-content markdown-body">
                    
                        如何快速感知项目立项？如何感知应用上线？如何跟踪应用迭代？越权漏洞频发如何解决？

前言应用安全如何做？这是一个老生常谈的问题，那为什么还要提这个话题呢？在笔者经历了短暂的两年多的应用安全建设来看，SDL的完整落地是一个很大的难题。当然，像其中的培训、代码扫描以及应急响应这几部分，各种落地方案很成熟，也就不提了。应用安全建设的本质就是运营，最难落地的差不多就是安全评审了。
如何进行安全评审，从方法论来看也不是什么难题，通过STRIDE威胁建模模型和DREAD威胁评级模型，再融入公司的实际情况，一份定制化的评审CheckList差不多就可以出炉了。然后问题就来了，在哪个环节切入？通过什么方式？如何持续运营？
安全评审需要介入的三个节点立项时、上线时、迭代时。
立项时：根基不牢地动山摇，这个阶段需要进行架构安全评审。架构安全评审的必要性在于可以用最小的成本解决最大的风险，如果架构性安全问题在这个阶段未被发现，后续会随着一次次迭代，修复成本和风险都会急剧上升。
上线时：现在的白盒扫描，更多的是发现代码层漏洞，但是对于架构相关和业务相关的风险，就有心无力了。这也就引出了应用上线安全评审的必要性，验证立项评审阶段的风险是否存在，同时评审其有没有业务逻辑相关风险(越权、敏感信息等)。
迭代时：现在的开发思路大多都是快速立项、快速上线、持续迭代，也就导致了大部分的功能是在后期迭代过程中上线的，经过笔者的简单统计，当应用完成度在90%以上时，迭代新增的接口约占80%以上，换句话说，绝大多数的web接口都没有经过安全评审就暴露到公网当中去了，成为无数的攻击面，进而导致了权限相关安全风险的频发。
立项时–快速感知如何做到快速感知？笔者了解到的有这么几种：
一是和PD混熟，有新项目时及时同步，这种方法具有局限性，适合应用比较少的公司或者公司的某条业务线；
二是利用现有平台，往往在中台支撑部门、工程效能部门等，会有一些环节可以感知到立项，财务部门也是一个非常好的环节。这个时候就要发挥敏锐的嗅觉，找到这样的点，然后形成联动。这些平台大概率会接受合作，一则可以提升该平台的价值，二来可以提升其影响力；
三是自立门户成为入口，这种方案的思想是让项目在安全平台上立项，想要实现需要运气。为什么这么说呢，想要做这件事，需要大量的人员投入和强制的流程更改，能决定这事的往往需要CXO的支持，要想获得他们的支持，就需要一个影响足够大的安全风险。不过据笔者所知，有几家大公司就是这么做的。
第一种方案，灵活性太强，效果时好时坏，笔者在很长一段时间里都是用这种方法，最后的结果很惨，随着业务的迅速发展，项目评审率跌至很低的水位。
第二种方案的可行性非常强，是一个成本低效果好的抓手。但是有一个问题是，项目非常多怎么办？笔者现在的思路是按照项目的等级来评审，项目等级的划分有很多种方法，例如人日、业务线等等
第三种方案就不提了，时刻准备着，机会来了抓紧。
当找到了有效的抓手时，一定要记得带上数据安全、业务安全等，这个阶段的主要风险往往集中在风控、合规等。
上线时–发布卡点上线发布卡点，这个做起来就非常容易了，嵌入到应用构建平台中去就可以实现。有一个问题是，为什么不每次发布都卡点呢？
其实去看一下构建平台的发布记录就知道了，过于频繁，完全无法运营，所以只能退而求其次，卡住第一次上线发布。
迭代时–持续跟踪上边说到“迭代新增的接口约占80%以上”，这就是一个超级大风险，迭代接口的安全性全依赖于开发的安全意识和应急响应。虽然在上线后会有持续的黑盒扫描，但是目前还没有哪个工具可以低误报、低风险的发现权限相关漏洞吧。
在上线评审那里有说到过，构建平台的发布记录非常多，如果依赖这个去评审迭代，会消耗大量的精力，当应用只有两位数时还能勉强运营，但是当应用数量上升到几百、几千的时候，每天最多迭代几十万行代码，怕是不吃不喝也搞不定了。
这里讲一下笔者的思路，commit监控：
12341、每隔一段时间自动拉取commit记录；2、获取应用源码进行解析（白盒代码扫描工具中大多都可以做到源码解析，笔者是自己实现的）；3、解析diff记录，获取新增web接口；4、通过污点跟踪结合关键方法(permission等)大致判断风险指数(这个笔者实现起来效果不是很好，几乎每个应用都有自己独特的鉴权逻辑，通用程度低)。

只需要这几步，即可以实现应用新增web接口的跟踪。可能还有一些其它通用接口平台，其实跟踪思想也是类似。
不过在笔者的运营过程中发现，还是会存在新增接口过多的情况，现在采用的是优先级的方案（重点应用、发生过高危风险的应用等），发现新增接口后，大部分情况下，人工快速审计下代码就可以发现风险了，当然还是会存在各种奇葩的鉴权逻辑，这时候就要和开发交流了。
这段时间的运营感受就是，随着覆盖应用的增多，每天要读代码量也开始快速上升，不过效果还是很明显的。第4点的自动化分析需求愈加迫切。
总结总结下就是，三个节点，关键之处在于找到这三个节点的抓手，充分利用现有资源，如果实在没有，那就创造抓手。用技术的思路去做运营，用创业的心态去做产品。
本文纯属笔者的经验之谈，如有偏颇之处，还望指出，不甚感谢。

                    
                </div> -->
                <div class="article-content markdown-body">
                    <p>如何快速感知项目立项？如何感知应用上线？如何跟踪应用迭代？越权漏洞频发如何解决？</p>
<p><img lazyload src="/images/loading.svg" data-src="15982649151296.jpg"></p>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>应用安全如何做？这是一个老生常谈的问题，那为什么还要提这个话题呢？在笔者经历了短暂的两年多的应用安全建设来看，SDL的完整落地是一个很大的难题。当然，像其中的培训、代码扫描以及应急响应这几部分，各种落地方案很成熟，也就不提了。应用安全建设的本质就是运营，最难落地的差不多就是安全评审了。</p>
<p>如何进行安全评审，从方法论来看也不是什么难题，通过STRIDE威胁建模模型和DREAD威胁评级模型，再融入公司的实际情况，一份定制化的评审CheckList差不多就可以出炉了。然后问题就来了，在哪个环节切入？通过什么方式？如何持续运营？</p>
<h2 id="安全评审需要介入的三个节点"><a href="#安全评审需要介入的三个节点" class="headerlink" title="安全评审需要介入的三个节点"></a>安全评审需要介入的三个节点</h2><p>立项时、上线时、迭代时。</p>
<p><strong>立项时</strong>：根基不牢地动山摇，这个阶段需要进行架构安全评审。架构安全评审的必要性在于可以用最小的成本解决最大的风险，如果架构性安全问题在这个阶段未被发现，后续会随着一次次迭代，修复成本和风险都会急剧上升。</p>
<p><strong>上线时</strong>：现在的白盒扫描，更多的是发现代码层漏洞，但是对于架构相关和业务相关的风险，就有心无力了。这也就引出了应用上线安全评审的必要性，验证立项评审阶段的风险是否存在，同时评审其有没有业务逻辑相关风险(越权、敏感信息等)。</p>
<p><strong>迭代时</strong>：现在的开发思路大多都是快速立项、快速上线、持续迭代，也就导致了大部分的功能是在后期迭代过程中上线的，经过笔者的简单统计，当应用完成度在90%以上时，迭代新增的接口约占80%以上，换句话说，绝大多数的web接口都没有经过安全评审就暴露到公网当中去了，成为无数的攻击面，进而导致了权限相关安全风险的频发。</p>
<h2 id="立项时–快速感知"><a href="#立项时–快速感知" class="headerlink" title="立项时–快速感知"></a>立项时–快速感知</h2><p>如何做到快速感知？笔者了解到的有这么几种：</p>
<p><strong>一是和PD混熟</strong>，有新项目时及时同步，这种方法具有局限性，适合应用比较少的公司或者公司的某条业务线；</p>
<p><strong>二是利用现有平台</strong>，往往在中台支撑部门、工程效能部门等，会有一些环节可以感知到立项，财务部门也是一个非常好的环节。这个时候就要发挥敏锐的嗅觉，找到这样的点，然后形成联动。这些平台大概率会接受合作，一则可以提升该平台的价值，二来可以提升其影响力；</p>
<p><strong>三是自立门户成为入口</strong>，这种方案的思想是让项目在安全平台上立项，想要实现需要运气。为什么这么说呢，想要做这件事，需要大量的人员投入和强制的流程更改，能决定这事的往往需要CXO的支持，要想获得他们的支持，就需要一个影响足够大的安全风险。不过据笔者所知，有几家大公司就是这么做的。</p>
<p>第一种方案，灵活性太强，效果时好时坏，笔者在很长一段时间里都是用这种方法，最后的结果很惨，随着业务的迅速发展，项目评审率跌至很低的水位。</p>
<p>第二种方案的可行性非常强，是一个成本低效果好的抓手。但是有一个问题是，项目非常多怎么办？笔者现在的思路是按照项目的等级来评审，项目等级的划分有很多种方法，例如人日、业务线等等</p>
<p>第三种方案就不提了，时刻准备着，机会来了抓紧。</p>
<p>当找到了有效的抓手时，一定要记得带上数据安全、业务安全等，这个阶段的主要风险往往集中在风控、合规等。</p>
<h2 id="上线时–发布卡点"><a href="#上线时–发布卡点" class="headerlink" title="上线时–发布卡点"></a>上线时–发布卡点</h2><p>上线发布卡点，这个做起来就非常容易了，嵌入到应用构建平台中去就可以实现。有一个问题是，为什么不每次发布都卡点呢？</p>
<p>其实去看一下构建平台的发布记录就知道了，过于频繁，完全无法运营，所以只能退而求其次，卡住第一次上线发布。</p>
<h2 id="迭代时–持续跟踪"><a href="#迭代时–持续跟踪" class="headerlink" title="迭代时–持续跟踪"></a>迭代时–持续跟踪</h2><p>上边说到“<strong>迭代新增的接口约占80%以上</strong>”，这就是一个超级大风险，迭代接口的安全性全依赖于开发的安全意识和应急响应。虽然在上线后会有持续的黑盒扫描，但是目前还没有哪个工具可以低误报、低风险的发现权限相关漏洞吧。</p>
<p>在上线评审那里有说到过，构建平台的发布记录非常多，如果依赖这个去评审迭代，会消耗大量的精力，当应用只有两位数时还能勉强运营，但是当应用数量上升到几百、几千的时候，每天最多迭代几十万行代码，怕是不吃不喝也搞不定了。</p>
<p>这里讲一下笔者的思路，<strong>commit监控</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1、每隔一段时间自动拉取commit记录；</span><br><span class="line">2、获取应用源码进行解析（白盒代码扫描工具中大多都可以做到源码解析，笔者是自己实现的）；</span><br><span class="line">3、解析diff记录，获取新增web接口；</span><br><span class="line">4、通过污点跟踪结合关键方法(permission等)大致判断风险指数(这个笔者实现起来效果不是很好，几乎每个应用都有自己独特的鉴权逻辑，通用程度低)。</span><br></pre></td></tr></table></figure>

<p>只需要这几步，即可以实现应用新增web接口的跟踪。可能还有一些其它通用接口平台，其实跟踪思想也是类似。</p>
<p>不过在笔者的运营过程中发现，还是会存在新增接口过多的情况，现在采用的是优先级的方案（重点应用、发生过高危风险的应用等），发现新增接口后，大部分情况下，人工快速审计下代码就可以发现风险了，当然还是会存在各种奇葩的鉴权逻辑，这时候就要和开发交流了。</p>
<p>这段时间的运营感受就是，随着覆盖应用的增多，每天要读代码量也开始快速上升，不过效果还是很明显的。第4点的自动化分析需求愈加迫切。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总结下就是，三个节点，关键之处在于找到这三个节点的抓手，充分利用现有资源，如果实在没有，那就创造抓手。用技术的思路去做运营，用创业的心态去做产品。</p>
<p>本文纯属笔者的经验之谈，如有偏颇之处，还望指出，不甚感谢。</p>

                </div>

                <div class="home-article-meta-info-container">
    <div class="home-article-meta-info">
        <span><i class="fas fa-history"></i>&nbsp;<span class="home-article-date" data-date="Fri Feb 07 2020 21:00:00 GMT+0800">2020-02-07</span></span>
        
            <span class="home-article-category"><i class="fas fa-folder"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/categories/%E5%BA%94%E7%94%A8%E5%AE%89%E5%85%A8/">应用安全</a>&nbsp;
                        </li>
                    
                    
                </ul>
            </span>
        
        
            <span class="home-article-tag">
                <i class="fas fa-tags"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/tags/THINK/">THINK</a>&nbsp;
                        </li>
                    
                </ul>
            </span>
        
    </div>

    <a href="/%E5%BA%94%E7%94%A8%E5%AE%89%E5%85%A8%E8%AF%84%E5%AE%A1%E4%B8%AD%E7%9A%84%E4%B8%89%E4%B8%AA%E5%85%B3%E9%94%AE%E8%8A%82%E7%82%B9%E5%8F%8A%E6%8A%93%E6%89%8B%E5%AE%9E%E7%8E%B0/">Read more&nbsp;<i class="fas fa-angle-right"></i></a>
</div>

            </li>
        
            <li class="home-article-item">

                

                <h3 class="home-article-title">
                    <a href="/%E8%B4%A6%E5%8F%B7%E7%94%9F%E5%91%BD%E5%AE%89%E5%85%A8%E5%91%A8%E6%9C%9F/">
                        账号生命安全周期
                    </a>
                </h3>

                <!-- <div class="home-article-content markdown-body">
                    
                        注册风险
垃圾注册
账号检存
弱密码
人机

防护
滑动验证
短信验证
强制改密

授权登入风险
撞库
盗号
弱密码
人机行为

防护
强制改密
滑动验证
短信验证
高危账号强制验证或者禁止登陆
重要账号强制验证
白名单
沉睡账号强制验证

登出
session未失效

找回密码
盗号
信息重放
账号检存
逻辑缺陷

高危操作
更改密码
更换手机号
更换邮箱

防护
强弹二次验证
线下核身

注销
欠款用户不得注销（to do）

整体
可信体系


                    
                </div> -->
                <div class="article-content markdown-body">
                    <h3 id="注册"><a href="#注册" class="headerlink" title="注册"></a>注册</h3><h4 id="风险"><a href="#风险" class="headerlink" title="风险"></a>风险</h4><ul>
<li>垃圾注册</li>
<li>账号检存</li>
<li>弱密码</li>
<li>人机</li>
</ul>
<h4 id="防护"><a href="#防护" class="headerlink" title="防护"></a>防护</h4><ul>
<li>滑动验证</li>
<li>短信验证</li>
<li>强制改密</li>
</ul>
<h3 id="授权"><a href="#授权" class="headerlink" title="授权"></a>授权</h3><h3 id="登入"><a href="#登入" class="headerlink" title="登入"></a>登入</h3><h4 id="风险-1"><a href="#风险-1" class="headerlink" title="风险"></a>风险</h4><ul>
<li>撞库</li>
<li>盗号</li>
<li>弱密码</li>
<li>人机行为</li>
</ul>
<h4 id="防护-1"><a href="#防护-1" class="headerlink" title="防护"></a>防护</h4><ul>
<li>强制改密</li>
<li>滑动验证</li>
<li>短信验证</li>
<li>高危账号强制验证或者禁止登陆</li>
<li>重要账号强制验证</li>
<li>白名单</li>
<li>沉睡账号强制验证</li>
</ul>
<h3 id="登出"><a href="#登出" class="headerlink" title="登出"></a>登出</h3><ul>
<li>session未失效</li>
</ul>
<h3 id="找回密码"><a href="#找回密码" class="headerlink" title="找回密码"></a>找回密码</h3><ul>
<li>盗号</li>
<li>信息重放</li>
<li>账号检存</li>
<li>逻辑缺陷</li>
</ul>
<h3 id="高危操作"><a href="#高危操作" class="headerlink" title="高危操作"></a>高危操作</h3><ul>
<li>更改密码</li>
<li>更换手机号</li>
<li>更换邮箱</li>
</ul>
<h4 id="防护-2"><a href="#防护-2" class="headerlink" title="防护"></a>防护</h4><ul>
<li>强弹二次验证</li>
<li>线下核身</li>
</ul>
<h3 id="注销"><a href="#注销" class="headerlink" title="注销"></a>注销</h3><ul>
<li>欠款用户不得注销（to do）</li>
</ul>
<h3 id="整体"><a href="#整体" class="headerlink" title="整体"></a>整体</h3><ul>
<li>可信体系</li>
</ul>

                </div>

                <div class="home-article-meta-info-container">
    <div class="home-article-meta-info">
        <span><i class="fas fa-history"></i>&nbsp;<span class="home-article-date" data-date="Tue Oct 22 2019 21:00:00 GMT+0800">2019-10-22</span></span>
        
            <span class="home-article-category"><i class="fas fa-folder"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/categories/%E5%AE%89%E5%85%A8%E6%8A%80%E6%9C%AF/">安全技术</a>&nbsp;
                        </li>
                    
                    
                </ul>
            </span>
        
        
            <span class="home-article-tag">
                <i class="fas fa-tags"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/tags/Web/">Web</a>&nbsp;
                        </li>
                    
                </ul>
            </span>
        
    </div>

    <a href="/%E8%B4%A6%E5%8F%B7%E7%94%9F%E5%91%BD%E5%AE%89%E5%85%A8%E5%91%A8%E6%9C%9F/">Read more&nbsp;<i class="fas fa-angle-right"></i></a>
</div>

            </li>
        
            <li class="home-article-item">

                

                <h3 class="home-article-title">
                    <a href="/DGA%20Domain%20Detection/">
                        DGA Domain Detection
                    </a>
                </h3>

                <!-- <div class="home-article-content markdown-body">
                    
                        
0x01 Domain Generating AlgorithmDomain generation algorithms (DGA) are algorithms seen in various families of malware that are used to periodically generate a large number of domain names that can be used as rendezvous points with their command and control servers. 
Example

Torpig
ZeusBot
Cryptolocker
Necurs
Symmi
Ranbyus

0x02 Random Forestrandom forest = bagging + decision trees
0x03 code
Random Forest

MultinomialNB


123456789101112131415161718import os, sysimport tracebackimport jsonimport optparseimport pickleimport collectionsimport sklearnimport sklearn.feature_extractionimport sklearn.ensembleimport sklearn.metricsimport pandas as pdimport numpy as npimport tldextractimport mathimport operatorfrom sklearn.model_selection import train_test_splitfrom matplotlib import pylabfrom pylab import *

收集数据
123alexa_dataframe = pd.read_csv(&#x27;data/alexa_100k.csv&#x27;, names=[&#x27;rank&#x27;,&#x27;uri&#x27;], header=None, encoding=&#x27;utf-8&#x27;)alexa_dataframe.info()alexa_dataframe.head()

&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 100000 entries, 0 to 99999
Data columns (total 2 columns):
rank    100000 non-null int64
uri     100000 non-null object
dtypes: int64(1), object(1)
memory usage: 1.5+ MB



    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;



  
    
      
      rank
      uri
    
  
  
    
      0
      1
      facebook.com
    
    
      1
      2
      google.com
    
    
      2
      3
      youtube.com
    
    
      3
      4
      yahoo.com
    
    
      4
      5
      baidu.com
    
  






123dga_dataframe = pd.read_csv(&#x27;data/dga_domains.txt&#x27;, names=[&#x27;raw_domain&#x27;], header=None, encoding=&#x27;utf-8&#x27;)dga_dataframe.info()dga_dataframe.head()

&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 2669 entries, 0 to 2668
Data columns (total 1 columns):
raw_domain    2669 non-null object
dtypes: object(1)
memory usage: 20.9+ KB



    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;



  
    
      
      raw_domain
    
  
  
    
      0
      04055051be412eea5a61b7da8438be3d.info
    
    
      1
      1cb8a5f36f.info
    
    
      2
      30acd347397c34fc273e996b22951002.org
    
    
      3
      336c986a284e2b3bc0f69f949cb437cb.info
    
    
      4
      336c986a284e2b3bc0f69f949cb437cb.org
    
  






123word_dataframe = pd.read_csv(&#x27;data/words.txt&#x27;, names=[&#x27;word&#x27;], header=None, dtype=&#123;&#x27;word&#x27;: np.str&#125;, encoding=&#x27;utf-8&#x27;)word_dataframe.info()word_dataframe.head(10)

&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 479623 entries, 0 to 479622
Data columns (total 1 columns):
word    479619 non-null object
dtypes: object(1)
memory usage: 3.7+ MB



    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;



  
    
      
      word
    
  
  
    
      0
      1080
    
    
      1
      10-point
    
    
      2
      10th
    
    
      3
      11-point
    
    
      4
      12-point
    
    
      5
      16-point
    
    
      6
      18-point
    
    
      7
      1st
    
    
      8
      2
    
    
      9
      20-point
    
  





准备数据
1234567891011121314def domain_extract(uri):    ext = tldextract.extract(uri)    if (not ext.suffix):        return None    else:        return ext.domain    alexa_dataframe[&#x27;domain&#x27;] = [ domain_extract(uri) for uri in alexa_dataframe[&#x27;uri&#x27;]]del alexa_dataframe[&#x27;rank&#x27;]del alexa_dataframe[&#x27;uri&#x27;]alexa_dataframe = alexa_dataframe.dropna()alexa_dataframe = alexa_dataframe.drop_duplicates()alexa_dataframe.info()alexa_dataframe.head()

&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
Int64Index: 91377 entries, 0 to 99999
Data columns (total 1 columns):
domain    91377 non-null object
dtypes: object(1)
memory usage: 1.4+ MB



    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;



  
    
      
      domain
    
  
  
    
      0
      facebook
    
    
      1
      google
    
    
      2
      youtube
    
    
      3
      yahoo
    
    
      4
      baidu
    
  






123alexa_dataframe[&#x27;class&#x27;] = &#x27;legit&#x27;#对正常数据打标legitalexa_dataframe.head()






    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;



  
    
      
      domain
      class
    
  
  
    
      0
      facebook
      legit
    
    
      1
      google
      legit
    
    
      2
      youtube
      legit
    
    
      3
      yahoo
      legit
    
    
      4
      baidu
      legit
    
  






123456# Shuffle the data (important for training/testing)alexa_dataframe = alexa_dataframe.reindex(np.random.permutation(alexa_dataframe.index))#打乱循序，重新索引#Randomly permute a sequence, or return a permuted rangealexa_total = alexa_dataframe.shape[0]print(&#x27;Total Alexa domains %d&#x27; % alexa_total)

Total Alexa domains 91377

123dga_dataframe[&#x27;domain&#x27;] = dga_dataframe.applymap(lambda x: x.split(&#x27;.&#x27;)[0].strip().lower())#This method applies a function that accepts and returns a scalar to every element of a DataFrame.del dga_dataframe[&#x27;raw_domain&#x27;]




1234dga_dataframe = dga_dataframe.dropna()dga_dataframe = dga_dataframe.drop_duplicates()dga_total = dga_dataframe.shape[0]print(&#x27;Total DGA domains %d&#x27; % dga_total)

Total DGA domains 2664

12dga_dataframe[&#x27;class&#x27;] = &#x27;dga&#x27;dga_dataframe.head()






    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;



  
    
      
      domain
      class
    
  
  
    
      0
      04055051be412eea5a61b7da8438be3d
      dga
    
    
      1
      1cb8a5f36f
      dga
    
    
      2
      30acd347397c34fc273e996b22951002
      dga
    
    
      3
      336c986a284e2b3bc0f69f949cb437cb
      dga
    
    
      5
      40a43e61e56a5c218cf6c22aca27f7ee
      dga
    
  





123456def entropy(s):    &#x27;&#x27;&#x27;    熵计算    &#x27;&#x27;&#x27;    p, lns = collections.Counter(s), float(len(s))    return -sum( count/lns * math.log(count/lns, 2) for count in p.values())


12345678all_domains = pd.concat([alexa_dataframe, dga_dataframe], ignore_index=True)#将数据根据不同的轴作简单的融合#如果两个表的index都没有实际含义，使用ignore_index=Trueall_domains[&#x27;length&#x27;] = [len(x) for x in all_domains[&#x27;domain&#x27;]]all_domains = all_domains[all_domains[&#x27;length&#x27;] &gt; 6]#排除短domain的干扰all_domains[&#x27;entropy&#x27;] = [entropy(x) for x in all_domains[&#x27;domain&#x27;]]all_domains.head(10)






    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;



  
    
      
      domain
      class
      length
      entropy
    
  
  
    
      0
      facebook
      legit
      8
      2.750000
    
    
      2
      youtube
      legit
      7
      2.521641
    
    
      5
      wikipedia
      legit
      9
      2.641604
    
    
      10
      blogspot
      legit
      8
      2.750000
    
    
      11
      twitter
      legit
      7
      2.128085
    
    
      12
      linkedin
      legit
      8
      2.500000
    
    
      19
      wordpress
      legit
      9
      2.725481
    
    
      23
      microsoft
      legit
      9
      2.947703
    
    
      27
      xvideos
      legit
      7
      2.807355
    
    
      28
      googleusercontent
      legit
      17
      3.175123
    
  





分析数据
12345#箱线图all_domains.boxplot(&#x27;length&#x27;,&#x27;class&#x27;)pylab.ylabel(&#x27;Domain Length&#x27;)all_domains.boxplot(&#x27;entropy&#x27;,&#x27;class&#x27;)pylab.ylabel(&#x27;Domain Entropy&#x27;)




Text(0,0.5,&#39;Domain Entropy&#39;)



123456789cond = all_domains[&#x27;class&#x27;] == &#x27;dga&#x27;dga = all_domains[cond]alexa = all_domains[~cond]plt.scatter(alexa[&#x27;length&#x27;], alexa[&#x27;entropy&#x27;], s=140, c=&#x27;#aaaaff&#x27;, label=&#x27;Alexa&#x27;, alpha=.2)plt.scatter(dga[&#x27;length&#x27;], dga[&#x27;entropy&#x27;], s=40, c=&#x27;r&#x27;, label=&#x27;DGA&#x27;, alpha=.3)plt.legend()#放置图例pylab.xlabel(&#x27;Domain Length&#x27;)pylab.ylabel(&#x27;Domain Entropy&#x27;)




Text(0,0.5,&#39;Domain Entropy&#39;)


1all_domains.tail(10)






    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;



  
    
      
      domain
      class
      length
      entropy
    
  
  
    
      94031
      xcfwwghb
      dga
      8
      2.750000
    
    
      94032
      xcgqdfyrkgihlrmfmfib
      dga
      20
      3.684184
    
    
      94033
      xclqwzcfcx
      dga
      10
      2.646439
    
    
      94034
      xcpfxzuf
      dga
      8
      2.500000
    
    
      94035
      xcvxhxze
      dga
      8
      2.405639
    
    
      94036
      xdbrbsbm
      dga
      8
      2.405639
    
    
      94037
      xdfjryydcfwvkvui
      dga
      16
      3.500000
    
    
      94038
      xdjlvcgw
      dga
      8
      3.000000
    
    
      94039
...
                    
                </div> -->
                <div class="article-content markdown-body">
                    <p><img lazyload src="/images/loading.svg" data-src="15982653615268.jpg"></p>
<h4 id="0x01-Domain-Generating-Algorithm"><a href="#0x01-Domain-Generating-Algorithm" class="headerlink" title="0x01 Domain Generating Algorithm"></a>0x01 Domain Generating Algorithm</h4><p>Domain generation algorithms (DGA) are algorithms seen in various families of malware that are used to periodically generate a large number of domain names that can be used as rendezvous points with their command and control servers. </p>
<p><a class="link" target="_blank" rel="noopener" href="https://github.com/pchaigno/dga-collection/tree/master/dgacollection">Example<i class="fas fa-external-link-alt"></i></a></p>
<ul>
<li><a class="link" target="_blank" rel="noopener" href="https://seclab.cs.ucsb.edu/media/uploads/papers/torpig.pdf">Torpig<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link" target="_blank" rel="noopener" href="http://vrt-blog.snort.org/2014/03/decoding-domain-generation-algorithms.html">ZeusBot<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link" target="_blank" rel="noopener" href="https://blog.fortinet.com/post/a-closer-look-at-cryptolocker-s-dga">Cryptolocker<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link" target="_blank" rel="noopener" href="http://www.johannesbader.ch/2015/02/the-dgas-of-necurs/">Necurs<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link" target="_blank" rel="noopener" href="http://www.johannesbader.ch/2015/01/the-dga-of-symmi/">Symmi<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link" target="_blank" rel="noopener" href="http://www.johannesbader.ch/2015/05/the-dga-of-ranbyus/">Ranbyus<i class="fas fa-external-link-alt"></i></a></li>
</ul>
<h4 id="0x02-Random-Forest"><a href="#0x02-Random-Forest" class="headerlink" title="0x02 Random Forest"></a>0x02 Random Forest</h4><p>random forest = bagging + decision trees</p>
<h4 id="0x03-code"><a href="#0x03-code" class="headerlink" title="0x03 code"></a>0x03 code</h4><ul>
<li><p>Random Forest</p>
</li>
<li><p>MultinomialNB</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os, sys</span><br><span class="line"><span class="keyword">import</span> traceback</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> optparse</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.feature_extraction</span><br><span class="line"><span class="keyword">import</span> sklearn.ensemble</span><br><span class="line"><span class="keyword">import</span> sklearn.metrics</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tldextract</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pylab</span><br><span class="line"><span class="keyword">from</span> pylab <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure>

<p>收集数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">alexa_dataframe = pd.read_csv(<span class="string">&#x27;data/alexa_100k.csv&#x27;</span>, names=[<span class="string">&#x27;rank&#x27;</span>,<span class="string">&#x27;uri&#x27;</span>], header=<span class="literal">None</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">alexa_dataframe.info()</span><br><span class="line">alexa_dataframe.head()</span><br></pre></td></tr></table></figure>

<pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 100000 entries, 0 to 99999
Data columns (total 2 columns):
rank    100000 non-null int64
uri     100000 non-null object
dtypes: int64(1), object(1)
memory usage: 1.5+ MB
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>rank</th>
      <th>uri</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>facebook.com</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>google.com</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>youtube.com</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>yahoo.com</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>baidu.com</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dga_dataframe = pd.read_csv(<span class="string">&#x27;data/dga_domains.txt&#x27;</span>, names=[<span class="string">&#x27;raw_domain&#x27;</span>], header=<span class="literal">None</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">dga_dataframe.info()</span><br><span class="line">dga_dataframe.head()</span><br></pre></td></tr></table></figure>

<pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 2669 entries, 0 to 2668
Data columns (total 1 columns):
raw_domain    2669 non-null object
dtypes: object(1)
memory usage: 20.9+ KB
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>raw_domain</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>04055051be412eea5a61b7da8438be3d.info</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1cb8a5f36f.info</td>
    </tr>
    <tr>
      <th>2</th>
      <td>30acd347397c34fc273e996b22951002.org</td>
    </tr>
    <tr>
      <th>3</th>
      <td>336c986a284e2b3bc0f69f949cb437cb.info</td>
    </tr>
    <tr>
      <th>4</th>
      <td>336c986a284e2b3bc0f69f949cb437cb.org</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">word_dataframe = pd.read_csv(<span class="string">&#x27;data/words.txt&#x27;</span>, names=[<span class="string">&#x27;word&#x27;</span>], header=<span class="literal">None</span>, dtype=&#123;<span class="string">&#x27;word&#x27;</span>: np.<span class="built_in">str</span>&#125;, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">word_dataframe.info()</span><br><span class="line">word_dataframe.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 479623 entries, 0 to 479622
Data columns (total 1 columns):
word    479619 non-null object
dtypes: object(1)
memory usage: 3.7+ MB
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1080</td>
    </tr>
    <tr>
      <th>1</th>
      <td>10-point</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10th</td>
    </tr>
    <tr>
      <th>3</th>
      <td>11-point</td>
    </tr>
    <tr>
      <th>4</th>
      <td>12-point</td>
    </tr>
    <tr>
      <th>5</th>
      <td>16-point</td>
    </tr>
    <tr>
      <th>6</th>
      <td>18-point</td>
    </tr>
    <tr>
      <th>7</th>
      <td>1st</td>
    </tr>
    <tr>
      <th>8</th>
      <td>2</td>
    </tr>
    <tr>
      <th>9</th>
      <td>20-point</td>
    </tr>
  </tbody>
</table>
</div>



<p>准备数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">domain_extract</span>(<span class="params">uri</span>):</span></span><br><span class="line">    ext = tldextract.extract(uri)</span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">not</span> ext.suffix):</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> ext.domain</span><br><span class="line">    </span><br><span class="line">alexa_dataframe[<span class="string">&#x27;domain&#x27;</span>] = [ domain_extract(uri) <span class="keyword">for</span> uri <span class="keyword">in</span> alexa_dataframe[<span class="string">&#x27;uri&#x27;</span>]]</span><br><span class="line"><span class="keyword">del</span> alexa_dataframe[<span class="string">&#x27;rank&#x27;</span>]</span><br><span class="line"><span class="keyword">del</span> alexa_dataframe[<span class="string">&#x27;uri&#x27;</span>]</span><br><span class="line">alexa_dataframe = alexa_dataframe.dropna()</span><br><span class="line">alexa_dataframe = alexa_dataframe.drop_duplicates()</span><br><span class="line">alexa_dataframe.info()</span><br><span class="line">alexa_dataframe.head()</span><br></pre></td></tr></table></figure>

<pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
Int64Index: 91377 entries, 0 to 99999
Data columns (total 1 columns):
domain    91377 non-null object
dtypes: object(1)
memory usage: 1.4+ MB
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>facebook</td>
    </tr>
    <tr>
      <th>1</th>
      <td>google</td>
    </tr>
    <tr>
      <th>2</th>
      <td>youtube</td>
    </tr>
    <tr>
      <th>3</th>
      <td>yahoo</td>
    </tr>
    <tr>
      <th>4</th>
      <td>baidu</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">alexa_dataframe[<span class="string">&#x27;class&#x27;</span>] = <span class="string">&#x27;legit&#x27;</span></span><br><span class="line"><span class="comment">#对正常数据打标legit</span></span><br><span class="line">alexa_dataframe.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>facebook</td>
      <td>legit</td>
    </tr>
    <tr>
      <th>1</th>
      <td>google</td>
      <td>legit</td>
    </tr>
    <tr>
      <th>2</th>
      <td>youtube</td>
      <td>legit</td>
    </tr>
    <tr>
      <th>3</th>
      <td>yahoo</td>
      <td>legit</td>
    </tr>
    <tr>
      <th>4</th>
      <td>baidu</td>
      <td>legit</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Shuffle the data (important for training/testing)</span></span><br><span class="line">alexa_dataframe = alexa_dataframe.reindex(np.random.permutation(alexa_dataframe.index))</span><br><span class="line"><span class="comment">#打乱循序，重新索引</span></span><br><span class="line"><span class="comment">#Randomly permute a sequence, or return a permuted range</span></span><br><span class="line">alexa_total = alexa_dataframe.shape[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Total Alexa domains %d&#x27;</span> % alexa_total)</span><br></pre></td></tr></table></figure>

<pre><code>Total Alexa domains 91377
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dga_dataframe[<span class="string">&#x27;domain&#x27;</span>] = dga_dataframe.applymap(<span class="keyword">lambda</span> x: x.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>].strip().lower())</span><br><span class="line"><span class="comment">#This method applies a function that accepts and returns a scalar to every element of a DataFrame.</span></span><br><span class="line"><span class="keyword">del</span> dga_dataframe[<span class="string">&#x27;raw_domain&#x27;</span>]</span><br></pre></td></tr></table></figure>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dga_dataframe = dga_dataframe.dropna()</span><br><span class="line">dga_dataframe = dga_dataframe.drop_duplicates()</span><br><span class="line">dga_total = dga_dataframe.shape[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Total DGA domains %d&#x27;</span> % dga_total)</span><br></pre></td></tr></table></figure>

<pre><code>Total DGA domains 2664
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dga_dataframe[<span class="string">&#x27;class&#x27;</span>] = <span class="string">&#x27;dga&#x27;</span></span><br><span class="line">dga_dataframe.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>04055051be412eea5a61b7da8438be3d</td>
      <td>dga</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1cb8a5f36f</td>
      <td>dga</td>
    </tr>
    <tr>
      <th>2</th>
      <td>30acd347397c34fc273e996b22951002</td>
      <td>dga</td>
    </tr>
    <tr>
      <th>3</th>
      <td>336c986a284e2b3bc0f69f949cb437cb</td>
      <td>dga</td>
    </tr>
    <tr>
      <th>5</th>
      <td>40a43e61e56a5c218cf6c22aca27f7ee</td>
      <td>dga</td>
    </tr>
  </tbody>
</table>
</div>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">entropy</span>(<span class="params">s</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    熵计算</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    p, lns = collections.Counter(s), <span class="built_in">float</span>(<span class="built_in">len</span>(s))</span><br><span class="line">    <span class="keyword">return</span> -<span class="built_in">sum</span>( count/lns * math.log(count/lns, <span class="number">2</span>) <span class="keyword">for</span> count <span class="keyword">in</span> p.values())</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">all_domains = pd.concat([alexa_dataframe, dga_dataframe], ignore_index=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#将数据根据不同的轴作简单的融合</span></span><br><span class="line"><span class="comment">#如果两个表的index都没有实际含义，使用ignore_index=True</span></span><br><span class="line">all_domains[<span class="string">&#x27;length&#x27;</span>] = [<span class="built_in">len</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> all_domains[<span class="string">&#x27;domain&#x27;</span>]]</span><br><span class="line">all_domains = all_domains[all_domains[<span class="string">&#x27;length&#x27;</span>] &gt; <span class="number">6</span>]</span><br><span class="line"><span class="comment">#排除短domain的干扰</span></span><br><span class="line">all_domains[<span class="string">&#x27;entropy&#x27;</span>] = [entropy(x) <span class="keyword">for</span> x <span class="keyword">in</span> all_domains[<span class="string">&#x27;domain&#x27;</span>]]</span><br><span class="line">all_domains.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>class</th>
      <th>length</th>
      <th>entropy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>facebook</td>
      <td>legit</td>
      <td>8</td>
      <td>2.750000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>youtube</td>
      <td>legit</td>
      <td>7</td>
      <td>2.521641</td>
    </tr>
    <tr>
      <th>5</th>
      <td>wikipedia</td>
      <td>legit</td>
      <td>9</td>
      <td>2.641604</td>
    </tr>
    <tr>
      <th>10</th>
      <td>blogspot</td>
      <td>legit</td>
      <td>8</td>
      <td>2.750000</td>
    </tr>
    <tr>
      <th>11</th>
      <td>twitter</td>
      <td>legit</td>
      <td>7</td>
      <td>2.128085</td>
    </tr>
    <tr>
      <th>12</th>
      <td>linkedin</td>
      <td>legit</td>
      <td>8</td>
      <td>2.500000</td>
    </tr>
    <tr>
      <th>19</th>
      <td>wordpress</td>
      <td>legit</td>
      <td>9</td>
      <td>2.725481</td>
    </tr>
    <tr>
      <th>23</th>
      <td>microsoft</td>
      <td>legit</td>
      <td>9</td>
      <td>2.947703</td>
    </tr>
    <tr>
      <th>27</th>
      <td>xvideos</td>
      <td>legit</td>
      <td>7</td>
      <td>2.807355</td>
    </tr>
    <tr>
      <th>28</th>
      <td>googleusercontent</td>
      <td>legit</td>
      <td>17</td>
      <td>3.175123</td>
    </tr>
  </tbody>
</table>
</div>



<p>分析数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#箱线图</span></span><br><span class="line">all_domains.boxplot(<span class="string">&#x27;length&#x27;</span>,<span class="string">&#x27;class&#x27;</span>)</span><br><span class="line">pylab.ylabel(<span class="string">&#x27;Domain Length&#x27;</span>)</span><br><span class="line">all_domains.boxplot(<span class="string">&#x27;entropy&#x27;</span>,<span class="string">&#x27;class&#x27;</span>)</span><br><span class="line">pylab.ylabel(<span class="string">&#x27;Domain Entropy&#x27;</span>)</span><br></pre></td></tr></table></figure>




<pre><code>Text(0,0.5,&#39;Domain Entropy&#39;)
</code></pre>
<p><img lazyload src="/images/loading.svg" data-src="output_13_1.png" alt="output_13_1"></p>
<p><img lazyload src="/images/loading.svg" data-src="output_13_2.png" alt="output_13_2"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cond = all_domains[<span class="string">&#x27;class&#x27;</span>] == <span class="string">&#x27;dga&#x27;</span></span><br><span class="line">dga = all_domains[cond]</span><br><span class="line">alexa = all_domains[~cond]</span><br><span class="line">plt.scatter(alexa[<span class="string">&#x27;length&#x27;</span>], alexa[<span class="string">&#x27;entropy&#x27;</span>], s=<span class="number">140</span>, c=<span class="string">&#x27;#aaaaff&#x27;</span>, label=<span class="string">&#x27;Alexa&#x27;</span>, alpha=<span class="number">.2</span>)</span><br><span class="line">plt.scatter(dga[<span class="string">&#x27;length&#x27;</span>], dga[<span class="string">&#x27;entropy&#x27;</span>], s=<span class="number">40</span>, c=<span class="string">&#x27;r&#x27;</span>, label=<span class="string">&#x27;DGA&#x27;</span>, alpha=<span class="number">.3</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"><span class="comment">#放置图例</span></span><br><span class="line">pylab.xlabel(<span class="string">&#x27;Domain Length&#x27;</span>)</span><br><span class="line">pylab.ylabel(<span class="string">&#x27;Domain Entropy&#x27;</span>)</span><br></pre></td></tr></table></figure>




<pre><code>Text(0,0.5,&#39;Domain Entropy&#39;)
</code></pre>
<p><img lazyload src="/images/loading.svg" data-src="output_14_1.png" alt="output_14_1"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">all_domains.tail(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>class</th>
      <th>length</th>
      <th>entropy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>94031</th>
      <td>xcfwwghb</td>
      <td>dga</td>
      <td>8</td>
      <td>2.750000</td>
    </tr>
    <tr>
      <th>94032</th>
      <td>xcgqdfyrkgihlrmfmfib</td>
      <td>dga</td>
      <td>20</td>
      <td>3.684184</td>
    </tr>
    <tr>
      <th>94033</th>
      <td>xclqwzcfcx</td>
      <td>dga</td>
      <td>10</td>
      <td>2.646439</td>
    </tr>
    <tr>
      <th>94034</th>
      <td>xcpfxzuf</td>
      <td>dga</td>
      <td>8</td>
      <td>2.500000</td>
    </tr>
    <tr>
      <th>94035</th>
      <td>xcvxhxze</td>
      <td>dga</td>
      <td>8</td>
      <td>2.405639</td>
    </tr>
    <tr>
      <th>94036</th>
      <td>xdbrbsbm</td>
      <td>dga</td>
      <td>8</td>
      <td>2.405639</td>
    </tr>
    <tr>
      <th>94037</th>
      <td>xdfjryydcfwvkvui</td>
      <td>dga</td>
      <td>16</td>
      <td>3.500000</td>
    </tr>
    <tr>
      <th>94038</th>
      <td>xdjlvcgw</td>
      <td>dga</td>
      <td>8</td>
      <td>3.000000</td>
    </tr>
    <tr>
      <th>94039</th>
      <td>xdrmjeu</td>
      <td>dga</td>
      <td>7</td>
      <td>2.807355</td>
    </tr>
    <tr>
      <th>94040</th>
      <td>xflrjyyjswoatsoq</td>
      <td>dga</td>
      <td>16</td>
      <td>3.500000</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">legit = all_domains[(all_domains[<span class="string">&#x27;class&#x27;</span>]==<span class="string">&#x27;legit&#x27;</span>)]</span><br><span class="line">max_grams = np.maximum(legit[<span class="string">&#x27;alexa_grams&#x27;</span>],legit[<span class="string">&#x27;word_grams&#x27;</span>])</span><br><span class="line">ax = max_grams.hist(bins=<span class="number">80</span>)</span><br><span class="line">ax.figure.suptitle(<span class="string">&#x27;Histogram of the Max NGram Score for Domains&#x27;</span>)</span><br><span class="line">pylab.xlabel(<span class="string">&#x27;Number of Domains&#x27;</span>)</span><br><span class="line">pylab.ylabel(<span class="string">&#x27;Maximum NGram Score&#x27;</span>)</span><br></pre></td></tr></table></figure>




<pre><code>Text(0,0.5,&#39;Maximum NGram Score&#39;)
</code></pre>
<p><img lazyload src="/images/loading.svg" data-src="output_16_1.png" alt="output_16_1"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">word_dataframe = word_dataframe[word_dataframe[<span class="string">&#x27;word&#x27;</span>].<span class="built_in">map</span>(<span class="keyword">lambda</span> x: <span class="built_in">str</span>(x).isalpha())]</span><br><span class="line">word_dataframe = word_dataframe.applymap(<span class="keyword">lambda</span> x: <span class="built_in">str</span>(x).strip().lower())</span><br><span class="line">word_dataframe = word_dataframe.dropna()</span><br><span class="line">word_dataframe = word_dataframe.drop_duplicates()</span><br><span class="line">word_dataframe.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>37</th>
      <td>a</td>
    </tr>
    <tr>
      <th>48</th>
      <td>aa</td>
    </tr>
    <tr>
      <th>51</th>
      <td>aaa</td>
    </tr>
    <tr>
      <th>53</th>
      <td>aaaa</td>
    </tr>
    <tr>
      <th>54</th>
      <td>aaaaaa</td>
    </tr>
    <tr>
      <th>55</th>
      <td>aaal</td>
    </tr>
    <tr>
      <th>56</th>
      <td>aaas</td>
    </tr>
    <tr>
      <th>57</th>
      <td>aaberg</td>
    </tr>
    <tr>
      <th>58</th>
      <td>aachen</td>
    </tr>
    <tr>
      <th>59</th>
      <td>aae</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">alexa_vc = sklearn.feature_extraction.text.CountVectorizer(analyzer=<span class="string">&#x27;char&#x27;</span>, ngram_range=(<span class="number">3</span>,<span class="number">5</span>), min_df=<span class="number">1e-4</span>, max_df=<span class="number">1.0</span>)</span><br><span class="line"><span class="comment">#词袋模型统计词频</span></span><br><span class="line"><span class="comment">#ngram_range：词组切分的长度范围</span></span><br><span class="line"><span class="comment">#如果一个词的频率小于min_df或者大于max_df，将不会被作为关键词</span></span><br><span class="line">counts_matrix = alexa_vc.fit_transform(alexa_dataframe[<span class="string">&#x27;domain&#x27;</span>])</span><br><span class="line"><span class="comment">#生成词频向量</span></span><br><span class="line"><span class="comment">#fit_transform 计算各个词语出现的次数</span></span><br><span class="line">alexa_counts = np.log10(counts_matrix.<span class="built_in">sum</span>(axis=<span class="number">0</span>).getA1())</span><br><span class="line"><span class="comment">#数据归一化</span></span><br><span class="line"><span class="built_in">print</span>(alexa_counts[:<span class="number">10</span>])</span><br><span class="line">ngrams_list = alexa_vc.get_feature_names()</span><br><span class="line"><span class="comment">#从包含文本和图片的数据集中提取特征，转换成机器学习中可用的数值型特征</span></span><br><span class="line"><span class="built_in">print</span>(ngrams_list[:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">_sorted_ngrams = <span class="built_in">sorted</span>(<span class="built_in">zip</span>(ngrams_list, alexa_counts), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#zip()将两个序列合并，返回zip对象，可强制转换为列表或字典</span></span><br><span class="line"><span class="comment"># sorted()对序列进行排序，返回一个排序后的新列表，原数据不改变</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Alexa NGrams: %d&#x27;</span> % <span class="built_in">len</span>(_sorted_ngrams))</span><br><span class="line"><span class="keyword">for</span> ngram, count <span class="keyword">in</span> _sorted_ngrams[:<span class="number">10</span>]:</span><br><span class="line">    <span class="built_in">print</span>(ngram, count)</span><br></pre></td></tr></table></figure>

<pre><code>[1.         1.         1.17609126 1.64345268 1.11394335 1.14612804
 1.         1.17609126 1.07918125 1.54406804]
[&#39;-20&#39;, &#39;-a-&#39;, &#39;-ac&#39;, &#39;-ad&#39;, &#39;-ads&#39;, &#39;-af&#39;, &#39;-ag&#39;, &#39;-ai&#39;, &#39;-air&#39;, &#39;-al&#39;]
Alexa NGrams: 23613
ing 3.443888546777372
lin 3.4271614029259654
ine 3.399673721481038
tor 3.26528962586083
ter 3.2631624649622166
ion 3.2467447097238415
ent 3.228913405994688
por 3.2013971243204513
the 3.2005769267548483
ree 3.16345955176999
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#提取词的数值型特征</span></span><br><span class="line">dict_vc = sklearn.feature_extraction.text.CountVectorizer(analyzer=<span class="string">&#x27;char&#x27;</span>, ngram_range=(<span class="number">3</span>,<span class="number">5</span>), min_df=<span class="number">1e-5</span>, max_df=<span class="number">1.0</span>)</span><br><span class="line">counts_matrix = dict_vc.fit_transform(word_dataframe[<span class="string">&#x27;word&#x27;</span>])</span><br><span class="line">dict_counts = np.log10(counts_matrix.<span class="built_in">sum</span>(axis=<span class="number">0</span>).getA1())</span><br><span class="line">ngrams_list = dict_vc.get_feature_names()</span><br><span class="line"><span class="built_in">print</span>(ngrams_list[:<span class="number">10</span>])</span><br></pre></td></tr></table></figure>

<pre><code>[&#39;aaa&#39;, &#39;aab&#39;, &#39;aac&#39;, &#39;aad&#39;, &#39;aaf&#39;, &#39;aag&#39;, &#39;aah&#39;, &#39;aai&#39;, &#39;aak&#39;, &#39;aal&#39;]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">_sorted_ngrams = <span class="built_in">sorted</span>(<span class="built_in">zip</span>(ngrams_list, dict_counts), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Word NGrams: %d&#x27;</span> % <span class="built_in">len</span>(_sorted_ngrams))</span><br><span class="line"><span class="keyword">for</span> ngram, count <span class="keyword">in</span> _sorted_ngrams[:<span class="number">10</span>]:</span><br><span class="line">    <span class="built_in">print</span>(ngram, count)</span><br></pre></td></tr></table></figure>

<pre><code>Word NGrams: 123061
ing 4.387300822448285
ess 4.204879333760662
ati 4.1933472563864616
ion 4.165036479994566
ter 4.162415036106447
nes 4.112504458767161
tio 4.076822423342773
ate 4.0723602039634885
ent 4.069631102620343
tion 4.0496056125949735
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ngram_count</span>(<span class="params">domain</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    domain中包含的ngrams数</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    alexa_match = alexa_counts * alexa_vc.transform([domain]).T  </span><br><span class="line">    dict_match = dict_counts * dict_vc.transform([domain]).T</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;%s Alexa match:%d Dict match: %d&#x27;</span> % (domain, alexa_match, dict_match))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ngram_count(<span class="string">&#x27;google&#x27;</span>)</span><br><span class="line">ngram_count(<span class="string">&#x27;facebook&#x27;</span>)</span><br><span class="line">ngram_count(<span class="string">&#x27;1cb8a5f36f&#x27;</span>)</span><br><span class="line">ngram_count(<span class="string">&#x27;pterodactylfarts&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>google Alexa match:17 Dict match: 14
facebook Alexa match:31 Dict match: 27
1cb8a5f36f Alexa match:0 Dict match: 0
pterodactylfarts Alexa match:35 Dict match: 76
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Compute NGram matches for all the domains and add to our dataframe</span></span><br><span class="line">all_domains[<span class="string">&#x27;alexa_grams&#x27;</span>]= alexa_counts * alexa_vc.transform(all_domains[<span class="string">&#x27;domain&#x27;</span>]).T</span><br><span class="line">all_domains[<span class="string">&#x27;word_grams&#x27;</span>]= dict_counts * dict_vc.transform(all_domains[<span class="string">&#x27;domain&#x27;</span>]).T</span><br><span class="line">all_domains.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>class</th>
      <th>length</th>
      <th>entropy</th>
      <th>alexa_grams</th>
      <th>word_grams</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>facebook</td>
      <td>legit</td>
      <td>8</td>
      <td>2.750000</td>
      <td>31.302278</td>
      <td>27.872426</td>
    </tr>
    <tr>
      <th>2</th>
      <td>youtube</td>
      <td>legit</td>
      <td>7</td>
      <td>2.521641</td>
      <td>25.855170</td>
      <td>18.287142</td>
    </tr>
    <tr>
      <th>5</th>
      <td>wikipedia</td>
      <td>legit</td>
      <td>9</td>
      <td>2.641604</td>
      <td>24.571024</td>
      <td>29.175635</td>
    </tr>
    <tr>
      <th>10</th>
      <td>blogspot</td>
      <td>legit</td>
      <td>8</td>
      <td>2.750000</td>
      <td>24.435141</td>
      <td>19.274501</td>
    </tr>
    <tr>
      <th>11</th>
      <td>twitter</td>
      <td>legit</td>
      <td>7</td>
      <td>2.128085</td>
      <td>23.244500</td>
      <td>31.130820</td>
    </tr>
    <tr>
      <th>12</th>
      <td>linkedin</td>
      <td>legit</td>
      <td>8</td>
      <td>2.500000</td>
      <td>24.774916</td>
      <td>32.904408</td>
    </tr>
    <tr>
      <th>19</th>
      <td>wordpress</td>
      <td>legit</td>
      <td>9</td>
      <td>2.725481</td>
      <td>38.369509</td>
      <td>33.806635</td>
    </tr>
    <tr>
      <th>23</th>
      <td>microsoft</td>
      <td>legit</td>
      <td>9</td>
      <td>2.947703</td>
      <td>32.133033</td>
      <td>39.530125</td>
    </tr>
    <tr>
      <th>27</th>
      <td>xvideos</td>
      <td>legit</td>
      <td>7</td>
      <td>2.807355</td>
      <td>28.906360</td>
      <td>18.846834</td>
    </tr>
    <tr>
      <th>28</th>
      <td>googleusercontent</td>
      <td>legit</td>
      <td>17</td>
      <td>3.175123</td>
      <td>67.315750</td>
      <td>86.104683</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Use the vectorized operations of the dataframe to investigate differences</span></span><br><span class="line">all_domains[<span class="string">&#x27;diff&#x27;</span>] = all_domains[<span class="string">&#x27;alexa_grams&#x27;</span>] - all_domains[<span class="string">&#x27;word_grams&#x27;</span>]</span><br><span class="line">all_domains.sort_values([<span class="string">&#x27;diff&#x27;</span>], ascending=<span class="literal">True</span>).head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>class</th>
      <th>length</th>
      <th>entropy</th>
      <th>alexa_grams</th>
      <th>word_grams</th>
      <th>diff</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>79366</th>
      <td>bipolardisorderdepressionanxiety</td>
      <td>legit</td>
      <td>32</td>
      <td>3.616729</td>
      <td>117.312465</td>
      <td>190.833856</td>
      <td>-73.521391</td>
    </tr>
    <tr>
      <th>72512</th>
      <td>channel4embarrassingillnesses</td>
      <td>legit</td>
      <td>29</td>
      <td>3.440070</td>
      <td>95.786979</td>
      <td>169.119440</td>
      <td>-73.332460</td>
    </tr>
    <tr>
      <th>10961</th>
      <td>stirringtroubleinternationally</td>
      <td>legit</td>
      <td>30</td>
      <td>3.481728</td>
      <td>134.049367</td>
      <td>207.204729</td>
      <td>-73.155362</td>
    </tr>
    <tr>
      <th>85031</th>
      <td>americansforresponsiblesolutions</td>
      <td>legit</td>
      <td>32</td>
      <td>3.667838</td>
      <td>148.143049</td>
      <td>218.363956</td>
      <td>-70.220908</td>
    </tr>
    <tr>
      <th>20459</th>
      <td>pragmatismopolitico</td>
      <td>legit</td>
      <td>19</td>
      <td>3.326360</td>
      <td>61.244630</td>
      <td>121.536223</td>
      <td>-60.291593</td>
    </tr>
    <tr>
      <th>13702</th>
      <td>egaliteetreconciliation</td>
      <td>legit</td>
      <td>23</td>
      <td>3.186393</td>
      <td>91.938518</td>
      <td>152.125325</td>
      <td>-60.186808</td>
    </tr>
    <tr>
      <th>4706</th>
      <td>interoperabilitybridges</td>
      <td>legit</td>
      <td>23</td>
      <td>3.588354</td>
      <td>95.037285</td>
      <td>153.626312</td>
      <td>-58.589028</td>
    </tr>
    <tr>
      <th>85161</th>
      <td>foreclosurephilippines</td>
      <td>legit</td>
      <td>22</td>
      <td>3.447402</td>
      <td>74.506548</td>
      <td>132.514638</td>
      <td>-58.008090</td>
    </tr>
    <tr>
      <th>45636</th>
      <td>annamalicesissyselfhypnosis</td>
      <td>legit</td>
      <td>27</td>
      <td>3.429908</td>
      <td>68.680068</td>
      <td>126.667692</td>
      <td>-57.987623</td>
    </tr>
    <tr>
      <th>70351</th>
      <td>corazonindomablecapitulos</td>
      <td>legit</td>
      <td>25</td>
      <td>3.813661</td>
      <td>75.535473</td>
      <td>133.160690</td>
      <td>-57.625217</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">all_domains.sort_values([<span class="string">&#x27;diff&#x27;</span>], ascending=<span class="literal">False</span>).head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>class</th>
      <th>length</th>
      <th>entropy</th>
      <th>alexa_grams</th>
      <th>word_grams</th>
      <th>diff</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>54228</th>
      <td>gay-sex-pics-porn-pictures-gay-sex-porn-gay-se...</td>
      <td>legit</td>
      <td>56</td>
      <td>3.661056</td>
      <td>159.642301</td>
      <td>85.124184</td>
      <td>74.518116</td>
    </tr>
    <tr>
      <th>85091</th>
      <td>article-directory-free-submission-free-content</td>
      <td>legit</td>
      <td>46</td>
      <td>3.786816</td>
      <td>235.233896</td>
      <td>188.230453</td>
      <td>47.003443</td>
    </tr>
    <tr>
      <th>16893</th>
      <td>stream-free-movies-online</td>
      <td>legit</td>
      <td>25</td>
      <td>3.509275</td>
      <td>120.250616</td>
      <td>74.496915</td>
      <td>45.753701</td>
    </tr>
    <tr>
      <th>63380</th>
      <td>watch-free-movie-online</td>
      <td>legit</td>
      <td>23</td>
      <td>3.708132</td>
      <td>103.029245</td>
      <td>58.943451</td>
      <td>44.085794</td>
    </tr>
    <tr>
      <th>44253</th>
      <td>best-online-shopping-site</td>
      <td>legit</td>
      <td>25</td>
      <td>3.452879</td>
      <td>123.377240</td>
      <td>79.596640</td>
      <td>43.780601</td>
    </tr>
    <tr>
      <th>22524</th>
      <td>social-bookmarking-sites-list</td>
      <td>legit</td>
      <td>29</td>
      <td>3.702472</td>
      <td>145.755266</td>
      <td>102.261826</td>
      <td>43.493440</td>
    </tr>
    <tr>
      <th>66335</th>
      <td>free-online-directory</td>
      <td>legit</td>
      <td>21</td>
      <td>3.403989</td>
      <td>123.379738</td>
      <td>80.735030</td>
      <td>42.644708</td>
    </tr>
    <tr>
      <th>46553</th>
      <td>free-links-articles-directory</td>
      <td>legit</td>
      <td>29</td>
      <td>3.702472</td>
      <td>153.239055</td>
      <td>110.955361</td>
      <td>42.283694</td>
    </tr>
    <tr>
      <th>59873</th>
      <td>online-web-directory</td>
      <td>legit</td>
      <td>20</td>
      <td>3.584184</td>
      <td>116.310717</td>
      <td>74.082948</td>
      <td>42.227769</td>
    </tr>
    <tr>
      <th>58016</th>
      <td>web-directory-online</td>
      <td>legit</td>
      <td>20</td>
      <td>3.584184</td>
      <td>114.402671</td>
      <td>74.082948</td>
      <td>40.319723</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#gram count低的词</span></span><br><span class="line">weird_cond = (all_domains[<span class="string">&#x27;class&#x27;</span>]==<span class="string">&#x27;legit&#x27;</span>) &amp; (all_domains[<span class="string">&#x27;word_grams&#x27;</span>]&lt;<span class="number">3</span>) &amp; (all_domains[<span class="string">&#x27;alexa_grams&#x27;</span>]&lt;<span class="number">2</span>)</span><br><span class="line">weird = all_domains[weird_cond]</span><br><span class="line"><span class="built_in">print</span>(weird.shape[<span class="number">0</span>])</span><br><span class="line">weird.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<pre><code>91
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>class</th>
      <th>length</th>
      <th>entropy</th>
      <th>alexa_grams</th>
      <th>word_grams</th>
      <th>diff</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1246</th>
      <td>twcczhu</td>
      <td>legit</td>
      <td>7</td>
      <td>2.521641</td>
      <td>1.748188</td>
      <td>0.0</td>
      <td>1.748188</td>
    </tr>
    <tr>
      <th>2009</th>
      <td>ggmm777</td>
      <td>legit</td>
      <td>7</td>
      <td>1.556657</td>
      <td>1.518514</td>
      <td>0.0</td>
      <td>1.518514</td>
    </tr>
    <tr>
      <th>2760</th>
      <td>qq66699</td>
      <td>legit</td>
      <td>7</td>
      <td>1.556657</td>
      <td>1.342423</td>
      <td>0.0</td>
      <td>1.342423</td>
    </tr>
    <tr>
      <th>17347</th>
      <td>crx7601</td>
      <td>legit</td>
      <td>7</td>
      <td>2.807355</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>18682</th>
      <td>hzsxzhyy</td>
      <td>legit</td>
      <td>8</td>
      <td>2.250000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>19418</th>
      <td>02022222222</td>
      <td>legit</td>
      <td>11</td>
      <td>0.684038</td>
      <td>1.041393</td>
      <td>0.0</td>
      <td>1.041393</td>
    </tr>
    <tr>
      <th>19887</th>
      <td>3181302</td>
      <td>legit</td>
      <td>7</td>
      <td>2.235926</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>21172</th>
      <td>hljdns4</td>
      <td>legit</td>
      <td>7</td>
      <td>2.807355</td>
      <td>1.755875</td>
      <td>0.0</td>
      <td>1.755875</td>
    </tr>
    <tr>
      <th>26441</th>
      <td>05tz2e9</td>
      <td>legit</td>
      <td>7</td>
      <td>2.807355</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>26557</th>
      <td>fzysqmy</td>
      <td>legit</td>
      <td>7</td>
      <td>2.521641</td>
      <td>1.176091</td>
      <td>0.0</td>
      <td>1.176091</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#对于这些正常但是gram count低的domain标记为weird</span></span><br><span class="line">all_domains.loc[weird_cond, <span class="string">&#x27;class&#x27;</span>] = <span class="string">&#x27;weird&#x27;</span></span><br><span class="line">all_domains[<span class="string">&#x27;class&#x27;</span>].value_counts()</span><br></pre></td></tr></table></figure>




<pre><code>legit    67221
dga       2664
weird       91
Name: class, dtype: int64
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">all_domains[all_domains[<span class="string">&#x27;class&#x27;</span>] == <span class="string">&#x27;weird&#x27;</span>].head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>class</th>
      <th>length</th>
      <th>entropy</th>
      <th>alexa_grams</th>
      <th>word_grams</th>
      <th>diff</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1246</th>
      <td>twcczhu</td>
      <td>weird</td>
      <td>7</td>
      <td>2.521641</td>
      <td>1.748188</td>
      <td>0.0</td>
      <td>1.748188</td>
    </tr>
    <tr>
      <th>2009</th>
      <td>ggmm777</td>
      <td>weird</td>
      <td>7</td>
      <td>1.556657</td>
      <td>1.518514</td>
      <td>0.0</td>
      <td>1.518514</td>
    </tr>
    <tr>
      <th>2760</th>
      <td>qq66699</td>
      <td>weird</td>
      <td>7</td>
      <td>1.556657</td>
      <td>1.342423</td>
      <td>0.0</td>
      <td>1.342423</td>
    </tr>
    <tr>
      <th>17347</th>
      <td>crx7601</td>
      <td>weird</td>
      <td>7</td>
      <td>2.807355</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>18682</th>
      <td>hzsxzhyy</td>
      <td>weird</td>
      <td>8</td>
      <td>2.250000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cond = all_domains[<span class="string">&#x27;class&#x27;</span>] == <span class="string">&#x27;dga&#x27;</span></span><br><span class="line">dga = all_domains[cond]</span><br><span class="line">alexa = all_domains[~cond]</span><br><span class="line">plt.scatter(alexa[<span class="string">&#x27;word_grams&#x27;</span>], alexa[<span class="string">&#x27;entropy&#x27;</span>], s=<span class="number">140</span>, c=<span class="string">&#x27;#aaaaff&#x27;</span>, label=<span class="string">&#x27;Alexa&#x27;</span>, alpha=<span class="number">.2</span>)</span><br><span class="line">plt.scatter(dga[<span class="string">&#x27;word_grams&#x27;</span>], dga[<span class="string">&#x27;entropy&#x27;</span>], s=<span class="number">40</span>, c=<span class="string">&#x27;r&#x27;</span>, label=<span class="string">&#x27;DGA&#x27;</span>, alpha=<span class="number">.3</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"><span class="comment">#放置图例</span></span><br><span class="line">pylab.xlabel(<span class="string">&#x27;Domain word_grams&#x27;</span>)</span><br><span class="line">pylab.ylabel(<span class="string">&#x27;Domain Entropy&#x27;</span>)</span><br></pre></td></tr></table></figure>




<pre><code>Text(0,0.5,&#39;Domain Entropy&#39;)
</code></pre>
<p><img lazyload src="/images/loading.svg" data-src="output_29_1.png" alt="output_29_1"></p>
<p>训练算法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">not_weird = all_domains[all_domains[<span class="string">&#x27;class&#x27;</span>] != <span class="string">&#x27;weird&#x27;</span>]</span><br><span class="line">X = not_weird.as_matrix([<span class="string">&#x27;length&#x27;</span>, <span class="string">&#x27;entropy&#x27;</span>, <span class="string">&#x27;alexa_grams&#x27;</span>, <span class="string">&#x27;word_grams&#x27;</span>])</span><br><span class="line"><span class="comment">#将frame转换为Numpy-array表示</span></span><br><span class="line">y = np.array(not_weird[<span class="string">&#x27;class&#x27;</span>].tolist())</span><br><span class="line"><span class="comment">#将array转换为list</span></span><br><span class="line">clf = sklearn.ensemble.RandomForestClassifier(n_estimators=<span class="number">20</span>)</span><br><span class="line"><span class="comment">#A random forest classifier</span></span><br><span class="line"><span class="comment">#The number of trees in the forest</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>)</span><br><span class="line"><span class="comment">#随机划分训练集和测试集</span></span><br><span class="line"><span class="comment">#样本占比0.2</span></span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line"><span class="comment">#用训练数据拟合分类器模型</span></span><br><span class="line">y_pred = clf.predict(X_test)</span><br><span class="line"><span class="comment">#用训练好的分类器去预测测试数据</span></span><br></pre></td></tr></table></figure>

<pre><code>/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.
  
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_cm</span>(<span class="params">cm, labels</span>):</span></span><br><span class="line">    <span class="comment">#计算百分比</span></span><br><span class="line">    percent = (cm*<span class="number">100.0</span>)/np.array(np.matrix(cm.<span class="built_in">sum</span>(axis=<span class="number">1</span>)).T)  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Confusion Matrix Stats&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i, label_i <span class="keyword">in</span> <span class="built_in">enumerate</span>(labels):</span><br><span class="line">        <span class="keyword">for</span> j, label_j <span class="keyword">in</span> <span class="built_in">enumerate</span>(labels):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;%s/%s: %.2f%% (%d/%d)&quot;</span> % (label_i, label_j, (percent[i][j]), cm[i][j], cm[i].<span class="built_in">sum</span>()))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">labels = [<span class="string">&#x27;legit&#x27;</span>, <span class="string">&#x27;dga&#x27;</span>]</span><br><span class="line">cm = sklearn.metrics.confusion_matrix(y_test, y_pred, labels)</span><br><span class="line"><span class="comment">#混淆矩阵被用于在分类问题上对准确率的一种评估形式</span></span><br><span class="line">show_cm(cm, labels)</span><br></pre></td></tr></table></figure>

<pre><code>Confusion Matrix Stats
legit/legit: 99.57% (13369/13427)
legit/dga: 0.43% (58/13427)
dga/legit: 15.45% (85/550)
dga/dga: 84.55% (465/550)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">importances = <span class="built_in">zip</span>([<span class="string">&#x27;length&#x27;</span>, <span class="string">&#x27;entropy&#x27;</span>, <span class="string">&#x27;alexa_grams&#x27;</span>, <span class="string">&#x27;word_grams&#x27;</span>], clf.feature_importances_)</span><br><span class="line"><span class="comment">#了解每个特征的重要性</span></span><br><span class="line"><span class="built_in">list</span>(importances)</span><br></pre></td></tr></table></figure>




<pre><code>[(&#39;length&#39;, 0.16033779891739047),
 (&#39;entropy&#39;, 0.12175502861193326),
 (&#39;alexa_grams&#39;, 0.5087685303664589),
 (&#39;word_grams&#39;, 0.20913864210421748)]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clf.fit(X, y)</span><br></pre></td></tr></table></figure>




<pre><code>RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;,
            max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=1,
            oob_score=False, random_state=None, verbose=0,
            warm_start=False)
</code></pre>
<p>测试算法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_it</span>(<span class="params">domain</span>):</span></span><br><span class="line">    _alexa_match = alexa_counts * alexa_vc.transform([domain]).T  </span><br><span class="line">    _dict_match = dict_counts * dict_vc.transform([domain]).T</span><br><span class="line">    _X = [[<span class="built_in">len</span>(domain), entropy(domain), _alexa_match, _dict_match]]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;%s : %s&#x27;</span> % (domain, clf.predict(_X)[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">test_it(<span class="string">&#x27;google&#x27;</span>)</span><br><span class="line">test_it(<span class="string">&#x27;google8sdflkajssjgjksdh&#x27;</span>)</span><br><span class="line">test_it(<span class="string">&#x27;faceboosadfadfafdk&#x27;</span>)</span><br><span class="line">test_it(<span class="string">&#x27;1cb8a5f36f&#x27;</span>)</span><br><span class="line">test_it(<span class="string">&#x27;pterodactyladfasdfasdffarts&#x27;</span>)</span><br><span class="line">test_it(<span class="string">&#x27;ptes9dro-dwacty2lfa5rrts&#x27;</span>)</span><br><span class="line">test_it(<span class="string">&#x27;beyonce&#x27;</span>)</span><br><span class="line">test_it(<span class="string">&#x27;bey666on4ce&#x27;</span>)</span><br><span class="line">test_it(<span class="string">&#x27;supersexy&#x27;</span>)</span><br><span class="line">test_it(<span class="string">&#x27;yourmomissohotinthesummertime&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>google : legit
google8sdflkajssjgjksdh : dga
faceboosadfadfafdk : legit
1cb8a5f36f : dga
pterodactyladfasdfasdffarts : legit
ptes9dro-dwacty2lfa5rrts : dga
beyonce : legit
bey666on4ce : dga
supersexy : legit
yourmomissohotinthesummertime : legit
</code></pre>
<p>使用算法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_model_to_disk</span>(<span class="params">name, model, model_dir=<span class="string">&#x27;models&#x27;</span></span>):</span></span><br><span class="line">    serialized_model = pickle.dumps(model, protocol=pickle.HIGHEST_PROTOCOL)</span><br><span class="line">    model_path = os.path.join(model_dir, name+<span class="string">&#x27;.model&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Storing Serialized Model to Disk (%s:%.2fMeg)&#x27;</span> % (name, <span class="built_in">len</span>(serialized_model)/<span class="number">1024.0</span>/<span class="number">1024.0</span>))</span><br><span class="line">    <span class="built_in">open</span>(model_path,<span class="string">&#x27;wb&#x27;</span>).write(serialized_model)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">save_model_to_disk(<span class="string">&#x27;dga_model_random_forest&#x27;</span>, clf)</span><br><span class="line">save_model_to_disk(<span class="string">&#x27;dga_model_alexa_vectorizor&#x27;</span>, alexa_vc)</span><br><span class="line">save_model_to_disk(<span class="string">&#x27;dga_model_alexa_counts&#x27;</span>, alexa_counts)</span><br><span class="line">save_model_to_disk(<span class="string">&#x27;dga_model_dict_vectorizor&#x27;</span>, dict_vc)</span><br><span class="line">save_model_to_disk(<span class="string">&#x27;dga_model_dict_counts&#x27;</span>, dict_counts)</span><br></pre></td></tr></table></figure>

<pre><code>Storing Serialized Model to Disk (dga_model_random_forest:1.80Meg)
Storing Serialized Model to Disk (dga_model_alexa_vectorizor:2.93Meg)
Storing Serialized Model to Disk (dga_model_alexa_counts:0.18Meg)
Storing Serialized Model to Disk (dga_model_dict_vectorizor:5.39Meg)
Storing Serialized Model to Disk (dga_model_dict_counts:0.94Meg)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_model_from_disk</span>(<span class="params">name, model_dir=<span class="string">&#x27;models&#x27;</span></span>):</span></span><br><span class="line">    model_path = os.path.join(model_dir, name+<span class="string">&#x27;.model&#x27;</span>)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        model = pickle.loads(<span class="built_in">open</span>(model_path,<span class="string">&#x27;rb&#x27;</span>).read())</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;success&#x27;</span>)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Could not load model: %s from directory %s!&#x27;</span> % (name, model_path))</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">clf = load_model_from_disk(<span class="string">&#x27;dga_model_random_forest&#x27;</span>)</span><br><span class="line">alexa_vc = load_model_from_disk(<span class="string">&#x27;dga_model_alexa_vectorizor&#x27;</span>)</span><br><span class="line">alexa_counts = load_model_from_disk(<span class="string">&#x27;dga_model_alexa_counts&#x27;</span>)</span><br><span class="line">dict_vc = load_model_from_disk(<span class="string">&#x27;dga_model_dict_vectorizor&#x27;</span>)</span><br><span class="line">dict_counts = load_model_from_disk(<span class="string">&#x27;dga_model_dict_counts&#x27;</span>)</span><br><span class="line">model = &#123;<span class="string">&#x27;clf&#x27;</span>:clf, <span class="string">&#x27;alexa_vc&#x27;</span>:alexa_vc, <span class="string">&#x27;alexa_counts&#x27;</span>:alexa_counts,</span><br><span class="line">                 <span class="string">&#x27;dict_vc&#x27;</span>:dict_vc, <span class="string">&#x27;dict_counts&#x27;</span>:dict_counts&#125;</span><br></pre></td></tr></table></figure>

<pre><code>success
success
success
success
success
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_url</span>(<span class="params">model, url</span>):</span></span><br><span class="line">    domain = domain_extract(url)</span><br><span class="line">    alexa_match = model[<span class="string">&#x27;alexa_counts&#x27;</span>] * model[<span class="string">&#x27;alexa_vc&#x27;</span>].transform([url]).T</span><br><span class="line">    dict_match = model[<span class="string">&#x27;dict_counts&#x27;</span>] * model[<span class="string">&#x27;dict_vc&#x27;</span>].transform([url]).T</span><br><span class="line">    </span><br><span class="line">    X = [[<span class="built_in">len</span>(domain), entropy(domain), alexa_match, dict_match]]</span><br><span class="line">    y_pred = model[<span class="string">&#x27;clf&#x27;</span>].predict(X)[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;%s : %s&#x27;</span> % (domain, y_pred))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">evaluate_url(model, <span class="string">&#x27;adfhalksfhjashfk.com&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>adfhalksfhjashfk : dga
</code></pre>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mtnb = MultinomialNB()</span><br><span class="line">mtnb.fit(X_train,y_train)</span><br></pre></td></tr></table></figure>




<pre><code>MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nb_y_pred=mtnb.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(classification_report(y_test, nb_y_pred))</span><br><span class="line">cm = sklearn.metrics.confusion_matrix(y_test, nb_y_pred)</span><br><span class="line">show_cm(cm, labels)</span><br></pre></td></tr></table></figure>

<pre><code>             precision    recall  f1-score   support

        dga       0.71      0.87      0.78       550
      legit       0.99      0.99      0.99     13427

avg / total       0.98      0.98      0.98     13977

Confusion Matrix Stats
legit/legit: 86.73% (477/550)
legit/dga: 13.27% (73/550)
dga/legit: 1.44% (194/13427)
dga/dga: 98.56% (13233/13427)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> tldextract</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential, load_model</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> sequence</span><br><span class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Dense, Dropout, Activation</span><br><span class="line"><span class="keyword">from</span> keras.layers.embeddings <span class="keyword">import</span> Embedding</span><br><span class="line"><span class="keyword">from</span> keras.layers.recurrent <span class="keyword">import</span> LSTM</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> feature_extraction</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">from</span> zipfile <span class="keyword">import</span> ZipFile</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">alexa_dataframe = pd.read_csv(<span class="string">&#x27;data/top-1m.csv&#x27;</span>, names=[<span class="string">&#x27;rank&#x27;</span>,<span class="string">&#x27;uri&#x27;</span>], header=<span class="literal">None</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">alexa_dataframe.info()</span><br><span class="line">alexa_dataframe.head()</span><br></pre></td></tr></table></figure>

<pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 1000000 entries, 0 to 999999
Data columns (total 2 columns):
rank    1000000 non-null int64
uri     1000000 non-null object
dtypes: int64(1), object(1)
memory usage: 15.3+ MB
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>rank</th>
      <th>uri</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>google.com</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>youtube.com</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>facebook.com</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>baidu.com</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>wikipedia.org</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_set</span>(<span class="params">filename</span>):</span></span><br><span class="line">    fw = <span class="built_in">open</span>(<span class="string">&#x27;data/dga_domain.txt&#x27;</span>, <span class="string">&#x27;w+&#x27;</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(filename, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            lineArr = line.strip().split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">            fw.write(lineArr[<span class="number">1</span>] + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    fw.close()</span><br><span class="line">load_data_set(<span class="string">&#x27;data/dga.txt&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dga_dataframe = pd.read_csv(<span class="string">&#x27;data/dga_domain.txt&#x27;</span>, names=[<span class="string">&#x27;raw_domain&#x27;</span>], header=<span class="literal">None</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">dga_dataframe.info()</span><br><span class="line">dga_dataframe.head()</span><br></pre></td></tr></table></figure>

<pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 1158695 entries, 0 to 1158694
Data columns (total 1 columns):
raw_domain    1158695 non-null object
dtypes: object(1)
memory usage: 8.8+ MB
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>raw_domain</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>ogxbnjopz.biz</td>
    </tr>
    <tr>
      <th>1</th>
      <td>zyejwiist.net</td>
    </tr>
    <tr>
      <th>2</th>
      <td>buuqogz.com</td>
    </tr>
    <tr>
      <th>3</th>
      <td>vpjmomduqll.org</td>
    </tr>
    <tr>
      <th>4</th>
      <td>uakwifutnpn.biz</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">domain_extract</span>(<span class="params">uri</span>):</span></span><br><span class="line">    ext = tldextract.extract(uri)</span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">not</span> ext.suffix):</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> ext.domain</span><br><span class="line">    </span><br><span class="line">alexa_dataframe[<span class="string">&#x27;domain&#x27;</span>] = [ domain_extract(uri) <span class="keyword">for</span> uri <span class="keyword">in</span> alexa_dataframe[<span class="string">&#x27;uri&#x27;</span>]]</span><br><span class="line"><span class="keyword">del</span> alexa_dataframe[<span class="string">&#x27;rank&#x27;</span>]</span><br><span class="line"><span class="keyword">del</span> alexa_dataframe[<span class="string">&#x27;uri&#x27;</span>]</span><br><span class="line">alexa_dataframe = alexa_dataframe.dropna()</span><br><span class="line">alexa_dataframe = alexa_dataframe.drop_duplicates()</span><br><span class="line">alexa_dataframe[<span class="string">&#x27;length&#x27;</span>] = [<span class="built_in">len</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> alexa_dataframe[<span class="string">&#x27;domain&#x27;</span>]]</span><br><span class="line">alexa_dataframe = alexa_dataframe[alexa_dataframe[<span class="string">&#x27;length&#x27;</span>] &gt; <span class="number">6</span>]</span><br><span class="line">alexa_dataframe.info()</span><br><span class="line">alexa_dataframe.head()</span><br></pre></td></tr></table></figure>

<pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
Int64Index: 718018 entries, 1 to 999999
Data columns (total 2 columns):
domain    718018 non-null object
length    718018 non-null int64
dtypes: int64(1), object(1)
memory usage: 16.4+ MB
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>length</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>youtube</td>
      <td>7</td>
    </tr>
    <tr>
      <th>2</th>
      <td>facebook</td>
      <td>8</td>
    </tr>
    <tr>
      <th>4</th>
      <td>wikipedia</td>
      <td>9</td>
    </tr>
    <tr>
      <th>11</th>
      <td>instagram</td>
      <td>9</td>
    </tr>
    <tr>
      <th>13</th>
      <td>twitter</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">alexa_dataframe[<span class="string">&#x27;class&#x27;</span>] = <span class="string">&#x27;legit&#x27;</span></span><br><span class="line"><span class="comment">#对正常数据打标legit</span></span><br><span class="line">alexa_dataframe.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>length</th>
      <th>class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>youtube</td>
      <td>7</td>
      <td>legit</td>
    </tr>
    <tr>
      <th>2</th>
      <td>facebook</td>
      <td>8</td>
      <td>legit</td>
    </tr>
    <tr>
      <th>4</th>
      <td>wikipedia</td>
      <td>9</td>
      <td>legit</td>
    </tr>
    <tr>
      <th>11</th>
      <td>instagram</td>
      <td>9</td>
      <td>legit</td>
    </tr>
    <tr>
      <th>13</th>
      <td>twitter</td>
      <td>7</td>
      <td>legit</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Shuffle the data (important for training/testing)</span></span><br><span class="line">alexa_dataframe = alexa_dataframe.reindex(np.random.permutation(alexa_dataframe.index))</span><br><span class="line"><span class="comment">#打乱循序，重新索引</span></span><br><span class="line"><span class="comment">#Randomly permute a sequence, or return a permuted range</span></span><br><span class="line">alexa_total = alexa_dataframe.shape[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Total Alexa domains %d&#x27;</span> % alexa_total)</span><br></pre></td></tr></table></figure>

<pre><code>Total Alexa domains 718018
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dga_dataframe[<span class="string">&#x27;domain&#x27;</span>] = dga_dataframe.applymap(<span class="keyword">lambda</span> x: x.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>].strip().lower())</span><br><span class="line"><span class="comment">#This method applies a function that accepts and returns a scalar to every element of a DataFrame.</span></span><br><span class="line"><span class="keyword">del</span> dga_dataframe[<span class="string">&#x27;raw_domain&#x27;</span>]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dga_dataframe = dga_dataframe.dropna()</span><br><span class="line">dga_dataframe = dga_dataframe.drop_duplicates()</span><br><span class="line">dga_dataframe[<span class="string">&#x27;length&#x27;</span>] = [<span class="built_in">len</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> dga_dataframe[<span class="string">&#x27;domain&#x27;</span>]]</span><br><span class="line">dga_dataframe = dga_dataframe[dga_dataframe[<span class="string">&#x27;length&#x27;</span>] &gt; <span class="number">6</span>]</span><br><span class="line">dga_total = dga_dataframe.shape[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Total DGA domains %d&#x27;</span> % dga_total)</span><br></pre></td></tr></table></figure>

<pre><code>Total DGA domains 1082010
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dga_dataframe[<span class="string">&#x27;class&#x27;</span>] = <span class="string">&#x27;dga&#x27;</span></span><br><span class="line">dga_dataframe.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>length</th>
      <th>class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>ogxbnjopz</td>
      <td>9</td>
      <td>dga</td>
    </tr>
    <tr>
      <th>1</th>
      <td>zyejwiist</td>
      <td>9</td>
      <td>dga</td>
    </tr>
    <tr>
      <th>2</th>
      <td>buuqogz</td>
      <td>7</td>
      <td>dga</td>
    </tr>
    <tr>
      <th>3</th>
      <td>vpjmomduqll</td>
      <td>11</td>
      <td>dga</td>
    </tr>
    <tr>
      <th>4</th>
      <td>uakwifutnpn</td>
      <td>11</td>
      <td>dga</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">all_domains = pd.concat([alexa_dataframe[:<span class="number">5000</span>], dga_dataframe[:<span class="number">5000</span>]], ignore_index=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#</span></span><br><span class="line">all_domains.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>length</th>
      <th>class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>youtube</td>
      <td>7</td>
      <td>legit</td>
    </tr>
    <tr>
      <th>1</th>
      <td>facebook</td>
      <td>8</td>
      <td>legit</td>
    </tr>
    <tr>
      <th>2</th>
      <td>wikipedia</td>
      <td>9</td>
      <td>legit</td>
    </tr>
    <tr>
      <th>3</th>
      <td>instagram</td>
      <td>9</td>
      <td>legit</td>
    </tr>
    <tr>
      <th>4</th>
      <td>twitter</td>
      <td>7</td>
      <td>legit</td>
    </tr>
    <tr>
      <th>5</th>
      <td>blogspot</td>
      <td>8</td>
      <td>legit</td>
    </tr>
    <tr>
      <th>6</th>
      <td>netflix</td>
      <td>7</td>
      <td>legit</td>
    </tr>
    <tr>
      <th>7</th>
      <td>pornhub</td>
      <td>7</td>
      <td>legit</td>
    </tr>
    <tr>
      <th>8</th>
      <td>xvideos</td>
      <td>7</td>
      <td>legit</td>
    </tr>
    <tr>
      <th>9</th>
      <td>livejasmin</td>
      <td>10</td>
      <td>legit</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">all_domains.tail(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>length</th>
      <th>class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>9990</th>
      <td>mxepwpxki</td>
      <td>9</td>
      <td>dga</td>
    </tr>
    <tr>
      <th>9991</th>
      <td>xnvqgaddhivrqowtbs</td>
      <td>18</td>
      <td>dga</td>
    </tr>
    <tr>
      <th>9992</th>
      <td>btgjyoydcwoeigdldngr</td>
      <td>20</td>
      <td>dga</td>
    </tr>
    <tr>
      <th>9993</th>
      <td>mnnridfyhxkyk</td>
      <td>13</td>
      <td>dga</td>
    </tr>
    <tr>
      <th>9994</th>
      <td>jmcctiodbdemfejo</td>
      <td>16</td>
      <td>dga</td>
    </tr>
    <tr>
      <th>9995</th>
      <td>mepoiwtmeffy</td>
      <td>12</td>
      <td>dga</td>
    </tr>
    <tr>
      <th>9996</th>
      <td>iwpikrmppfqeere</td>
      <td>15</td>
      <td>dga</td>
    </tr>
    <tr>
      <th>9997</th>
      <td>gcibdmrs</td>
      <td>8</td>
      <td>dga</td>
    </tr>
    <tr>
      <th>9998</th>
      <td>tusdspujigdyntbxusuah</td>
      <td>21</td>
      <td>dga</td>
    </tr>
    <tr>
      <th>9999</th>
      <td>wvsiuqhblxfijnoefjnao</td>
      <td>21</td>
      <td>dga</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = all_domains[<span class="string">&#x27;domain&#x27;</span>]</span><br><span class="line">labels = all_domains[<span class="string">&#x27;class&#x27;</span>]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ngram_vectorizer = feature_extraction.text.CountVectorizer(analyzer=<span class="string">&#x27;char&#x27;</span>, ngram_range=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">count_vec = ngram_vectorizer.fit_transform(X)</span><br><span class="line">max_features = count_vec.shape[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = [<span class="number">0</span> <span class="keyword">if</span> x == <span class="string">&#x27;legit&#x27;</span> <span class="keyword">else</span> <span class="number">1</span> <span class="keyword">for</span> x <span class="keyword">in</span> labels]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">final_data = []</span><br></pre></td></tr></table></figure>

<h5 id="多层感知机（MLP）"><a href="#多层感知机（MLP）" class="headerlink" title="多层感知机（MLP）"></a>多层感知机（MLP）</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_model</span>(<span class="params">max_features</span>):</span></span><br><span class="line">    model = Sequential()</span><br><span class="line">    model.add(Dense(<span class="number">1</span>, input_dim=max_features, activation=<span class="string">&#x27;sigmoid&#x27;</span>))</span><br><span class="line">    <span class="comment">#添加一个全连接层，激活函数使用sigmoid，输出维度max_features</span></span><br><span class="line">    model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>,optimizer=<span class="string">&#x27;adam&#x27;</span>)</span><br><span class="line">    <span class="comment">#编译模型，损失函数采用对数损失函数，优化器选用adam</span></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">max_epoch = <span class="number">50</span></span><br><span class="line">nfolds = <span class="number">10</span></span><br><span class="line"><span class="comment">#10轮训练</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> fold <span class="keyword">in</span> <span class="built_in">range</span>(nfolds):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;fold %u/%u&quot;</span> % (fold+<span class="number">1</span>, nfolds))</span><br><span class="line">    X_train, X_test, y_train, y_test, _, label_test = train_test_split(count_vec, y, labels, test_size=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Build model...&#x27;</span>)</span><br><span class="line">    model = build_model(max_features)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Train...&quot;</span>)</span><br><span class="line">    X_train, X_holdout, y_train, y_holdout = train_test_split(X_train, y_train, test_size=<span class="number">0.05</span>)</span><br><span class="line">    best_iter = -<span class="number">1</span></span><br><span class="line">    best_auc = <span class="number">0.0</span></span><br><span class="line">    out_data = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> ep <span class="keyword">in</span> <span class="built_in">range</span>(max_epoch):</span><br><span class="line">        model.fit(X_train.todense(), y_train, batch_size=batch_size, nb_epoch=<span class="number">1</span>)</span><br><span class="line">        t_probs = model.predict_proba(X_holdout.todense())</span><br><span class="line">        t_auc = sklearn.metrics.roc_auc_score(y_holdout, t_probs)</span><br><span class="line">        <span class="comment">#计算AUC值</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Epoch %d: auc = %f (best=%f)&#x27;</span> % (ep, t_auc, best_auc))</span><br><span class="line">        <span class="keyword">if</span> t_auc &gt; best_auc:</span><br><span class="line">            best_auc = t_auc</span><br><span class="line">            best_iter = ep</span><br><span class="line"></span><br><span class="line">            probs = model.predict_proba(X_test.todense())</span><br><span class="line">            out_data = &#123;<span class="string">&#x27;y&#x27;</span>:y_test, <span class="string">&#x27;labels&#x27;</span>: label_test, <span class="string">&#x27;probs&#x27;</span>:probs, <span class="string">&#x27;epochs&#x27;</span>: ep,</span><br><span class="line">                            <span class="string">&#x27;confusion_matrix&#x27;</span>: sklearn.metrics.confusion_matrix(y_test, probs &gt; <span class="number">.5</span>)&#125;</span><br><span class="line">            <span class="built_in">print</span>(sklearn.metrics.confusion_matrix(y_test, probs &gt; <span class="number">.5</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> (ep-best_iter) &gt; <span class="number">5</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    final_data.append(out_data)</span><br><span class="line">    model.save(<span class="string">&#x27;model.h5&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>fold 1/10
Build model...
Train...


/usr/lib/python3/dist-packages/ipykernel_launcher.py:15: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  from ipykernel import kernelapp as app


Epoch 1/1
7600/7600 [==============================] - 1s 86us/step - loss: 0.6297
Epoch 0: auc = 0.950239 (best=0.000000)
[[915  86]
 [108 891]]
Epoch 1/1
7600/7600 [==============================] - 0s 26us/step - loss: 0.5243
Epoch 1: auc = 0.980196 (best=0.950239)
[[952  49]
 [ 83 916]]
Epoch 1/1
7600/7600 [==============================] - 0s 31us/step - loss: 0.4502
Epoch 2: auc = 0.984872 (best=0.980196)
[[965  36]
 [ 78 921]]
Epoch 1/1
7600/7600 

Epoch 32: auc = 0.994192 (best=0.994192)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = load_model(<span class="string">&#x27;model.h5&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(final_data)</span><br></pre></td></tr></table></figure>

<pre><code>[&#123;&#39;y&#39;: [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1], &#39;labels&#39;: 2403    legit
2789    legit
450     legit
4521    legit
2841    legit
8645      dga
6999      dga
7831      dga
6291      dga
3746    legit
6226      dga
4111    legit
8487      dga
678     legit
90      legit
6151      dga
8300      dga
4004    legit
2489    legit
4836    legit
8291      dga
8198      dga
8911      dga
7585      dga
260     legit
5905      dga
5646      dga
970     legit
8718      dga
275     legit
        ...  
8589      dga
6620      dga
7470      dga
5230      dga
4827    legit
5677      dga
3417    legit
8539      dga
7147      dga
3699    legit
4751    legit
3043    legit
5475      dga
3736    legit
3887    legit
6349      dga
4996    legit
7379      dga
3530    legit
1942    legit
7914      dga
9752      dga
6717      dga
5363      dga
7622      dga
961     legit
1641    legit
4607    legit
8649      dga
6087      dga
Name: class, Length: 2000, dtype: object, &#39;probs&#39;: array([[0.14488636],
       [0.00496732],
       [0.00896166],
       ...,
       [0.00593334],
       [0.95598286],
       [0.9867235 ]], dtype=float32), &#39;epochs&#39;: 43, &#39;confusion_matrix&#39;: array([[972,  29],
       [ 62, 937]])&#125;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">z_test = np.array([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,  <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line">model.predict(z_test)</span><br></pre></td></tr></table></figure>




<pre><code>array([[1.]], dtype=float32)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(sklearn.metrics.classification_report(final_data[<span class="number">0</span>][<span class="string">&#x27;y&#x27;</span>], final_data[<span class="number">0</span>][<span class="string">&#x27;probs&#x27;</span>] &gt; <span class="number">.5</span>))</span><br></pre></td></tr></table></figure>

<pre><code>              precision    recall  f1-score   support

           0       0.95      0.97      0.96       970
           1       0.97      0.95      0.96      1030

   micro avg       0.96      0.96      0.96      2000
   macro avg       0.96      0.96      0.96      2000
weighted avg       0.96      0.96      0.96      2000
</code></pre>
<h5 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_model_lstm</span>(<span class="params">max_features, maxlen</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Build LSTM model&quot;&quot;&quot;</span></span><br><span class="line">    model = Sequential()</span><br><span class="line">    model.add(Embedding(max_features, <span class="number">128</span>, input_length=maxlen))</span><br><span class="line">    <span class="comment">#添加一个嵌入层，嵌入层是将正整数（下标）转换为具有固定大小的向量</span></span><br><span class="line">    model.add(LSTM(<span class="number">128</span>))</span><br><span class="line">    <span class="comment">#添加长短期记忆网络LSTM，从样本中学习特征，这个是核心层</span></span><br><span class="line">    model.add(Dropout(<span class="number">0.5</span>))</span><br><span class="line">    <span class="comment">#添加Dropout层防止过拟合</span></span><br><span class="line">    model.add(Dense(<span class="number">1</span>))</span><br><span class="line">    model.add(Activation(<span class="string">&#x27;sigmoid&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, optimizer=<span class="string">&#x27;rmsprop&#x27;</span>)</span><br><span class="line">    <span class="comment">#编译模型，损失函数采用对数损失函数，优化器选用rmsprop</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">X = all_domains[<span class="string">&#x27;domain&#x27;</span>]</span><br><span class="line">labels = all_domains[<span class="string">&#x27;class&#x27;</span>]</span><br><span class="line"></span><br><span class="line">valid_chars = &#123;x:idx+<span class="number">1</span> <span class="keyword">for</span> idx, x <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">set</span>(<span class="string">&#x27;&#x27;</span>.join(X)))&#125;</span><br><span class="line">max_features = <span class="built_in">len</span>(valid_chars) + <span class="number">1</span></span><br><span class="line"><span class="comment">#计算特征字符长度</span></span><br><span class="line">maxlen = np.<span class="built_in">max</span>([<span class="built_in">len</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> X])</span><br><span class="line"><span class="comment">#记录最长的域名长度</span></span><br><span class="line">X = [[valid_chars[y] <span class="keyword">for</span> y <span class="keyword">in</span> x] <span class="keyword">for</span> x <span class="keyword">in</span> X]</span><br><span class="line"><span class="comment">#转换为下标数组</span></span><br><span class="line">X = sequence.pad_sequences(X, maxlen=maxlen)</span><br><span class="line"><span class="comment">#进行长度填充</span></span><br><span class="line">y = [<span class="number">0</span> <span class="keyword">if</span> x == <span class="string">&#x27;legit&#x27;</span> <span class="keyword">else</span> <span class="number">1</span> <span class="keyword">for</span> x <span class="keyword">in</span> labels]</span><br><span class="line">final_data = []</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> fold <span class="keyword">in</span> <span class="built_in">range</span>(nfolds):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;fold %u/%u&quot;</span> % (fold+<span class="number">1</span>, nfolds))</span><br><span class="line">    X_train, X_test, y_train, y_test, _, label_test = train_test_split(X, y, labels, </span><br><span class="line">                                                                           test_size=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Build model...&#x27;</span>)</span><br><span class="line">    model = build_model_lstm(max_features, maxlen)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Train...&quot;</span>)</span><br><span class="line">    X_train, X_holdout, y_train, y_holdout = train_test_split(X_train, y_train, test_size=<span class="number">0.05</span>)</span><br><span class="line">    best_iter = -<span class="number">1</span></span><br><span class="line">    best_auc = <span class="number">0.0</span></span><br><span class="line">    out_data = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> ep <span class="keyword">in</span> <span class="built_in">range</span>(max_epoch):</span><br><span class="line">        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        t_probs = model.predict_proba(X_holdout)</span><br><span class="line">        t_auc = sklearn.metrics.roc_auc_score(y_holdout, t_probs)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Epoch %d: auc = %f (best=%f)&#x27;</span> % (ep, t_auc, best_auc))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> t_auc &gt; best_auc:</span><br><span class="line">            best_auc = t_auc</span><br><span class="line">            best_iter = ep</span><br><span class="line"></span><br><span class="line">            probs = model.predict_proba(X_test)</span><br><span class="line"></span><br><span class="line">            out_data = &#123;<span class="string">&#x27;y&#x27;</span>:y_test, <span class="string">&#x27;labels&#x27;</span>: label_test, <span class="string">&#x27;probs&#x27;</span>:probs, <span class="string">&#x27;epochs&#x27;</span>: ep, <span class="string">&#x27;confusion_matrix&#x27;</span>: sklearn.metrics.confusion_matrix(y_test, probs &gt; <span class="number">.5</span>)&#125;</span><br><span class="line"></span><br><span class="line">            <span class="built_in">print</span>(sklearn.metrics.confusion_matrix(y_test, probs &gt; <span class="number">.5</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> (ep-best_iter) &gt; <span class="number">2</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    final_data.append(out_data)</span><br></pre></td></tr></table></figure>

<pre><code>fold 1/10
Build model...
Train...

Epoch 1/1
7600/7600 [==============================] - 24s 3ms/step - loss: 0.3562
Epoch 0: auc = 0.979725 (best=0.000000)
[[893 113]
 [ 42 952]]
Epoch 1/1
7600/7600 [==============================] - 23s 3ms/step - loss: 0.1643
Epoch 7: auc = 0.980221 (best=0.981659)
Epoch 1/1
7600/7600 [==============================] - 21s 3ms/step - loss: 0.1603
Epoch 8: auc = 0.979843 (best=0.981659)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(sklearn.metrics.classification_report(final_data[<span class="number">0</span>][<span class="string">&#x27;y&#x27;</span>], final_data[<span class="number">0</span>][<span class="string">&#x27;probs&#x27;</span>] &gt; <span class="number">.5</span>))</span><br></pre></td></tr></table></figure>

<pre><code>              precision    recall  f1-score   support

           0       0.95      0.96      0.96      1006
           1       0.96      0.95      0.95       994

   micro avg       0.96      0.96      0.96      2000
   macro avg       0.96      0.96      0.96      2000
weighted avg       0.96      0.96      0.96      2000
</code></pre>

                </div>

                <div class="home-article-meta-info-container">
    <div class="home-article-meta-info">
        <span><i class="fas fa-history"></i>&nbsp;<span class="home-article-date" data-date="Sat Aug 24 2019 21:00:00 GMT+0800">2019-08-24</span></span>
        
            <span class="home-article-category"><i class="fas fa-folder"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/categories/%E5%AE%89%E5%85%A8%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/">安全数据分析</a>&nbsp;
                        </li>
                    
                    
                </ul>
            </span>
        
        
            <span class="home-article-tag">
                <i class="fas fa-tags"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/tags/ML/">ML</a>&nbsp;
                        </li>
                    
                </ul>
            </span>
        
    </div>

    <a href="/DGA%20Domain%20Detection/">Read more&nbsp;<i class="fas fa-angle-right"></i></a>
</div>

            </li>
        
            <li class="home-article-item">

                

                <h3 class="home-article-title">
                    <a href="/SOAR(%E5%AE%89%E5%85%A8%E7%BC%96%E6%8E%92%E8%87%AA%E5%8A%A8%E5%8C%96%E4%B8%8E%E5%93%8D%E5%BA%94)%E4%B8%AD%E6%B1%82%E8%A7%A3%E5%AE%89%E5%85%A8%E8%BF%90%E8%90%A5%E4%B9%8B%E6%B3%95/">
                        从SOAR(安全编排自动化与响应)中求解安全运营之法
                    </a>
                </h3>

                <!-- <div class="home-article-content markdown-body">
                    
                        

凡学问者，皆有术法道三大层次。法者，于术精通而升华成理，复以理指导术之提高，学问之提高层次。达于法者，达中乘也。



0x00 个人理解的企业应用安全建设参与企业应用安全建设两年有余，在公司的应用安全建设比较早期的时候参与进来，最近一年又有幸深度参与了多家中小型公司的应用安全建设，无论是基于云安全平台还是基于自研平台的企业安全建设都有了些许思考。也渐渐构建起了自己的安全观，
“企业安全建设是一个动态博弈需要持续投入的过程。安全是业务的一个重要属性，是业务的核心竞争力之一，应用安全的本质是运营。安全建设更重要的是看待安全问题的思路、角度和高度。攻防之道，相辅相成。”
什么是运营，一切围绕着网站产品进行的人工干预都叫运营，那什么又是强运营呢，直白点就是需要大量人工参与、与其它角色大量沟通的运营。从笔者的角度来看，企业安全建设就是一个强运营的工作，尤其是在安全建设后期，平台、工具、制度相对完善之后。大量的运营使人痛苦，尤其对技术安全运营来说尤甚，所以在很长的时间里思考这个困境的解法，有了些许思路，通过本文中将自己对企业安全中应用安全建设的思考和大家分享下。
当企业开始应用安全建设时，一般会经历这几个阶段：采购阶段、自研阶段、产品闭环，以期实现高效运营的目标。
0x01 采购阶段这个阶段相信很多经历过“一个人的安全部”的都会深有感触，简单调研之后，大概率会发现这样一个事实————“一穷二白”，好一点的可能运维或IT同学已经做了一部分，例如系统漏洞、高危端口等，但很明显远远不够的。这时候就要开始采购安全产品了，“管它好不好用，先止血”。对各个乙方的安全解决方案进行调研，在各个开源社区寻找各类开源安全工具、平台，在耗费了大量精力和经历安全预算申请的绝望后，终于部署了防火墙、IDS等安全产品，从开源社区找到了SOC平台、扫描器、风控平台等开源安全产品。枪有了，能不能打道猎物还是要看人。同样，安全产品怎么来用才能发挥最大价值是个值得思考的事情，需要大量的内部调研，尝试与已有的流程、机制、产品进行配合，以及如何得到高层的帮助，这就需要大家各显神通了。
在对接时，会发现在不经过二次开发的情况下很难实现有效的配合，更多的是在各自为战。与此同时，需要对这些产品进行运营，处理报警日志、漏洞扫描、漏洞的推动修复、应用上线审核、活动风险控制、各类安全应急等等，现阶段如果想要全部一手抓，难度有点大。笔者认为，在甲方做企业安全建设，最终还是要对结果负责，对于安全效果，有两个指标是最关键的核心指标，一个是漏洞/事件数，一个是安全产品覆盖面。
所以在初期阶段没法全覆盖的情况下，最有效的办法是找到业务最痛最关心的点，重点保障，得到认可。通过短期快速止血和长期安全机制建设相结合的方法迭代改进来度过这一阶段。为什么要找到业务最痛最关心的点呢？企业的安全是100%服务保障业务的，业务永远是第一位的，有业务才有安全。(当然如果公司的业务都是基于云产品部署的，那可以直接跳过这一阶段了，云平台提供的一整套安全产品对于基本的安全保障还是很有效的。)
0x02 自研阶段经历了采购阶段安全建设后，有了一定的安全水位，安全团队的配置也得到相应的提高，在基于开源或采购的安全产品进行运营时，被大量平台之间的协作搞得焦头烂额，迫切的需要开始安全平台的部分自研。
在这提一嘴安全团队的建设，笔者认为的安全团队组成主要有攻防、运营、开发三部分组成，其中开发又分为安全工具开发和安全平台开发，其中的区别在于安全工具开发需要专业的攻防能力，而安全平台开发则更侧重于开发本身，专业的人做专业的事，让一个安全同学去开发一个安全运营平台与现有的代码构建等平台进行对接是一件很困难的事情，所以就需要专业的开发来做这部分工作。
没有哪套安全解决方案可以应用在所有企业上，这就需要安全团队针对当前的业务模式、系统架构、发布流程等针对性的开发一些工具或平台来使安全解决方案更契合当前企业的技术栈。
例如SDL中的应用发布流程，其中最重要的莫过于发布卡点，卡点又要依赖代码安全扫描，而每个公司使用的开发框架往往不同，甚至在某些公司会对开发框架进行大量的修改，这种情况下通用的代码扫描就不可靠了，就需要对代码安全扫描器进行改造或自研，然后扫描出的漏洞需要通过漏洞运营平台来管理，如何修复对于开发来说也是个棘手的问题，要解决快速修复的问题就需要完整的代码级解决方案，好多公司都有安全包的组件供开发使用。例子只是其中的一个点，这一阶段往往是漫长的，平台和工具会经过一次次的迭代，最终和业务达到和平共处的状态。
为什么需要专业的开发，一个很重要的原因是需要工程化的能力，这里引用《赵彦的CISO闪电战：两年甲方安全修炼之路》中的一句话，“工程化能力体现在能把自研的安全产品、安全系统落地于海量分布式环境，落地于规模庞大的组织的IT系统上，且不降低性能，不影响可用性，不出故障和运营事故”。
因为安全产品导致的大型故障发生过很多起，安全产品有时候就是一个双刃剑，例如WAF，既能挡住恶意攻击，也有可能会把正常用户拒之门外。如果安全自身把业务给搞瘫痪了，那要安全还有何用，在很多情况下，稳定性往往是高于安全性的，凸显出工程化体系化的重要性。
0x03 产品闭环每个企业的安全思路都是不完全相同的，经过了自研阶段后，会形成自己企业特有的安全建设解决方案。漫长的自研阶段度过后，可能会有同学认为“纵深防御体系”（笔者理解的纵深防御，从系统、中间件、网络、应用、业务等各个环节布控，一道道防线由外到内共同组成整个防御体系）已经建设完成。从产品上来说，或许是的。但是从安全运营的角度来看，当前每个产品都还是孤立的点，产品之间的联动更多的是靠人工运营。
拿漏洞的运营举例，一个漏洞的生命周期通常是漏洞产生（SRC、内部发现、工具发现、威胁情报）、漏洞确认（是否误报、定级）、漏洞分配（对应的开发修复）、修复审核（是否修复以及修复方案的健壮性）、漏洞关闭，这其中就需要SRC、工具、TIP、SOC、开发中台、SIEM等平台的联动，来实现漏洞生命周期的闭环。类似闭环还有很多，但是工具类的产品闭环往往不是那么容易，这时可以寻找突破点，做产品的小闭环。
0x04 运营的痛点安全人员短缺、报警数量多、处置速度无法保证、处置经验有效沉淀少、威胁态势愈加危险和复杂等等
往往在安全产品闭环阶段后，技术安全团队的大小会稳定下来甚至会缩小，应用安全运营人员也会越来越少，在笔者看来这是一个正常的进化过程。但是业务的扩张并没有停止，应用也是一刻不停发布上线，随着企业规模越来越大，暴露的攻击面也越来越广，各类报警、漏洞大量增加，在有限的人力下，处置速度和经验沉淀很难有保障，更不用说现在大环境下安全形势了。
求变之心愈加强烈。
0x05 SOAR是否是一剂良药？思考这个问题很久了，应用安全建设强运营的困境该如何去突围？ 从最初的鼓吹AI到回归现实，终于从SOAR(安全编排自动化与响应)中看到些许希望。
简单介绍一下SOAR，SOAR是Gartner 2018年在安全领域定义的最新前沿技术，与UEBA、EDR等侧重于威胁识别发现的技术不同，SOAR集中在识别后的威胁处理，强调用户可以通过事件编排中心通过编码实现任意的威胁处理逻辑。
SOAR 是一系列技术的合集，它能够帮助企业和组织收集安全运维团队监控到的各种信息（包括各种安全系统产生的告警），并对这些信息进行事件分析和告警分诊。然后在标准工作流程的指引下，利用人机结合的方式帮助安全运维人员定义、排序和驱动标准化的事件响应活动。SOAR 工具使得企业和组织能够对事件分析与响应流程进行形式化的描述。
SOAR相关的安全产品在国外国内都已经有安全公司进入到这个领域，但是在本文中，不去讨论具体的实现和产品，而是将其视作一个方法论，领会它的思路，尝试将其融入到产品自研和产品闭环中去。
看一下SOAR的组成，编排、自动化、响应，其实从名字中已经给了我们答案，笔者认为，最核心的思想在于Orchestration和Automation，先将事件处理流程或其它的流程通过编排的方式形成闭环，然后对其中大量重复工作的部分进行自动化。 至于响应，也是同理，具体的以后详谈。
有一点需要明确，目前通过应用SOAR来实现全自动组织和缓解的情况非常罕见，安全没有银弹，大多数缓解和阻断仍然需要安全人员的参与。但是SOAR的思想非常值得借鉴，尤其是在经历采购阶段、产品自研、产品闭环这几个阶段后，思考能不能通过SOAR的方法论来减轻工作量。
例如漏洞扫描的流程，发现、上报、分配确认、修复确认，其中发现、上报、修复确认均可以实现自动化，再比如各类安全警报的处理也可以应用这套方法论。
让专业的人来处理专业的事，用自动化来处理重复工作，或许是突围应用安全强运营困境的一个解法。SOAR目前还处于成长期，保持期待和不断探索。
Modern Security Operations Center = SOAR + SIEM + UEBA + OTHER

                    
                </div> -->
                <div class="article-content markdown-body">
                    <blockquote>
<blockquote>
<p>凡学问者，皆有术法道三大层次。法者，于术精通而升华成理，复以理指导术之提高，学问之提高层次。达于法者，达中乘也。</p>
</blockquote>
</blockquote>
<p><img lazyload src="/images/loading.svg" data-src="15982653176360.jpg"></p>
<h2 id="0x00-个人理解的企业应用安全建设"><a href="#0x00-个人理解的企业应用安全建设" class="headerlink" title="0x00 个人理解的企业应用安全建设"></a>0x00 个人理解的企业应用安全建设</h2><p>参与企业应用安全建设两年有余，在公司的应用安全建设比较早期的时候参与进来，最近一年又有幸深度参与了多家中小型公司的应用安全建设，无论是基于云安全平台还是基于自研平台的企业安全建设都有了些许思考。也渐渐构建起了自己的安全观，</p>
<p>“企业安全建设是一个动态博弈需要持续投入的过程。安全是业务的一个重要属性，是业务的核心竞争力之一，应用安全的本质是运营。安全建设更重要的是看待安全问题的思路、角度和高度。攻防之道，相辅相成。”</p>
<p>什么是运营，一切围绕着网站产品进行的人工干预都叫运营，那什么又是强运营呢，直白点就是需要大量人工参与、与其它角色大量沟通的运营。从笔者的角度来看，企业安全建设就是一个强运营的工作，尤其是在安全建设后期，平台、工具、制度相对完善之后。大量的运营使人痛苦，尤其对技术安全运营来说尤甚，所以在很长的时间里思考这个困境的解法，有了些许思路，通过本文中将自己对企业安全中应用安全建设的思考和大家分享下。</p>
<p>当企业开始应用安全建设时，一般会经历这几个阶段：采购阶段、自研阶段、产品闭环，以期实现高效运营的目标。</p>
<h2 id="0x01-采购阶段"><a href="#0x01-采购阶段" class="headerlink" title="0x01 采购阶段"></a>0x01 采购阶段</h2><p>这个阶段相信很多经历过“一个人的安全部”的都会深有感触，简单调研之后，大概率会发现这样一个事实————“一穷二白”，好一点的可能运维或IT同学已经做了一部分，例如系统漏洞、高危端口等，但很明显远远不够的。这时候就要开始采购安全产品了，“管它好不好用，先止血”。对各个乙方的安全解决方案进行调研，在各个开源社区寻找各类开源安全工具、平台，在耗费了大量精力和经历安全预算申请的绝望后，终于部署了防火墙、IDS等安全产品，从开源社区找到了SOC平台、扫描器、风控平台等开源安全产品。枪有了，能不能打道猎物还是要看人。同样，安全产品怎么来用才能发挥最大价值是个值得思考的事情，需要大量的内部调研，尝试与已有的流程、机制、产品进行配合，以及如何得到高层的帮助，这就需要大家各显神通了。</p>
<p>在对接时，会发现在不经过二次开发的情况下很难实现有效的配合，更多的是在各自为战。与此同时，需要对这些产品进行运营，处理报警日志、漏洞扫描、漏洞的推动修复、应用上线审核、活动风险控制、各类安全应急等等，现阶段如果想要全部一手抓，难度有点大。笔者认为，<strong>在甲方做企业安全建设，最终还是要对结果负责，对于安全效果，有两个指标是最关键的核心指标，一个是漏洞/事件数，一个是安全产品覆盖面。</strong></p>
<p>所以在初期阶段没法全覆盖的情况下，<strong>最有效的办法是找到业务最痛最关心的点，重点保障，得到认可。通过短期快速止血和长期安全机制建设相结合的方法迭代改进来度过这一阶段</strong>。为什么要找到业务最痛最关心的点呢？企业的安全是100%服务保障业务的，业务永远是第一位的，有业务才有安全。(当然如果公司的业务都是基于云产品部署的，那可以直接跳过这一阶段了，云平台提供的一整套安全产品对于基本的安全保障还是很有效的。)</p>
<h2 id="0x02-自研阶段"><a href="#0x02-自研阶段" class="headerlink" title="0x02 自研阶段"></a>0x02 自研阶段</h2><p>经历了采购阶段安全建设后，有了一定的安全水位，安全团队的配置也得到相应的提高，在基于开源或采购的安全产品进行运营时，被大量平台之间的协作搞得焦头烂额，迫切的需要开始安全平台的部分自研。</p>
<p>在这提一嘴安全团队的建设，笔者认为的安全团队组成主要有攻防、运营、开发三部分组成，其中开发又分为安全工具开发和安全平台开发，其中的区别在于安全工具开发需要专业的攻防能力，而安全平台开发则更侧重于开发本身，<strong>专业的人做专业的事</strong>，让一个安全同学去开发一个安全运营平台与现有的代码构建等平台进行对接是一件很困难的事情，所以就需要专业的开发来做这部分工作。</p>
<p>没有哪套安全解决方案可以应用在所有企业上，这就需要安全团队针对当前的业务模式、系统架构、发布流程等针对性的开发一些工具或平台来使安全解决方案更契合当前企业的技术栈。</p>
<p>例如SDL中的应用发布流程，其中最重要的莫过于发布卡点，卡点又要依赖代码安全扫描，而每个公司使用的开发框架往往不同，甚至在某些公司会对开发框架进行大量的修改，这种情况下通用的代码扫描就不可靠了，就需要对代码安全扫描器进行改造或自研，然后扫描出的漏洞需要通过漏洞运营平台来管理，如何修复对于开发来说也是个棘手的问题，要解决快速修复的问题就需要完整的代码级解决方案，好多公司都有安全包的组件供开发使用。例子只是其中的一个点，这一阶段往往是漫长的，平台和工具会经过一次次的迭代，最终和业务达到和平共处的状态。</p>
<p>为什么需要专业的开发，一个很重要的原因是需要工程化的能力，这里引用《赵彦的CISO闪电战：两年甲方安全修炼之路》中的一句话，<strong>“工程化能力体现在能把自研的安全产品、安全系统落地于海量分布式环境，落地于规模庞大的组织的IT系统上，且不降低性能，不影响可用性，不出故障和运营事故”</strong>。</p>
<p>因为安全产品导致的大型故障发生过很多起，安全产品有时候就是一个双刃剑，例如WAF，既能挡住恶意攻击，也有可能会把正常用户拒之门外。如果安全自身把业务给搞瘫痪了，那要安全还有何用，在很多情况下，稳定性往往是高于安全性的，凸显出工程化体系化的重要性。</p>
<h2 id="0x03-产品闭环"><a href="#0x03-产品闭环" class="headerlink" title="0x03 产品闭环"></a>0x03 产品闭环</h2><p>每个企业的安全思路都是不完全相同的，经过了自研阶段后，会形成自己企业特有的安全建设解决方案。漫长的自研阶段度过后，可能会有同学认为“纵深防御体系”（<strong>笔者理解的纵深防御，从系统、中间件、网络、应用、业务等各个环节布控，一道道防线由外到内共同组成整个防御体系</strong>）已经建设完成。从产品上来说，或许是的。但是从安全运营的角度来看，当前每个产品都还是孤立的点，产品之间的联动更多的是靠人工运营。</p>
<p>拿漏洞的运营举例，一个漏洞的生命周期通常是漏洞产生（SRC、内部发现、工具发现、威胁情报）、漏洞确认（是否误报、定级）、漏洞分配（对应的开发修复）、修复审核（是否修复以及修复方案的健壮性）、漏洞关闭，这其中就需要SRC、工具、TIP、SOC、开发中台、SIEM等平台的联动，来实现漏洞生命周期的闭环。类似闭环还有很多，但是工具类的产品闭环往往不是那么容易，这时可以寻找突破点，做产品的小闭环。</p>
<h2 id="0x04-运营的痛点"><a href="#0x04-运营的痛点" class="headerlink" title="0x04 运营的痛点"></a>0x04 运营的痛点</h2><p>安全人员短缺、报警数量多、处置速度无法保证、处置经验有效沉淀少、威胁态势愈加危险和复杂等等</p>
<p>往往在安全产品闭环阶段后，技术安全团队的大小会稳定下来甚至会缩小，应用安全运营人员也会越来越少，在笔者看来这是一个正常的进化过程。但是业务的扩张并没有停止，应用也是一刻不停发布上线，随着企业规模越来越大，暴露的攻击面也越来越广，各类报警、漏洞大量增加，在有限的人力下，处置速度和经验沉淀很难有保障，更不用说现在大环境下安全形势了。</p>
<p>求变之心愈加强烈。</p>
<h2 id="0x05-SOAR是否是一剂良药？"><a href="#0x05-SOAR是否是一剂良药？" class="headerlink" title="0x05 SOAR是否是一剂良药？"></a>0x05 SOAR是否是一剂良药？</h2><p>思考这个问题很久了，<strong>应用安全建设强运营的困境该如何去突围？</strong> 从最初的鼓吹AI到回归现实，终于从SOAR(安全编排自动化与响应)中看到些许希望。</p>
<p>简单介绍一下SOAR，SOAR是Gartner 2018年在安全领域定义的最新前沿技术，与UEBA、EDR等侧重于威胁识别发现的技术不同，SOAR集中在识别后的威胁处理，强调用户可以通过事件编排中心通过编码实现任意的威胁处理逻辑。</p>
<p>SOAR 是一系列技术的合集，它能够帮助企业和组织收集安全运维团队监控到的各种信息（包括各种安全系统产生的告警），并对这些信息进行事件分析和告警分诊。然后在标准工作流程的指引下，利用人机结合的方式帮助安全运维人员定义、排序和驱动标准化的事件响应活动。SOAR 工具使得企业和组织能够对事件分析与响应流程进行形式化的描述。</p>
<p>SOAR相关的安全产品在国外国内都已经有安全公司进入到这个领域，但是在本文中，不去讨论具体的实现和产品，而是将其视作一个<strong>方法论</strong>，领会它的思路，尝试将其融入到产品自研和产品闭环中去。</p>
<p>看一下SOAR的组成，编排、自动化、响应，其实从名字中已经给了我们答案，笔者认为，最核心的思想在于Orchestration和Automation，<strong>先将事件处理流程或其它的流程通过编排的方式形成闭环，然后对其中大量重复工作的部分进行自动化。</strong> 至于响应，也是同理，具体的以后详谈。</p>
<p>有一点需要明确，<strong>目前通过应用SOAR来实现全自动组织和缓解的情况非常罕见，安全没有银弹，大多数缓解和阻断仍然需要安全人员的参与</strong>。但是SOAR的思想非常值得借鉴，尤其是在经历采购阶段、产品自研、产品闭环这几个阶段后，思考能不能通过SOAR的方法论来减轻工作量。</p>
<p>例如漏洞扫描的流程，发现、上报、分配确认、修复确认，其中发现、上报、修复确认均可以实现自动化，再比如各类安全警报的处理也可以应用这套方法论。</p>
<p>让专业的人来处理专业的事，用自动化来处理重复工作，或许是突围应用安全强运营困境的一个解法。SOAR目前还处于成长期，保持期待和不断探索。</p>
<p>Modern Security Operations Center = SOAR + SIEM + UEBA + OTHER</p>

                </div>

                <div class="home-article-meta-info-container">
    <div class="home-article-meta-info">
        <span><i class="fas fa-history"></i>&nbsp;<span class="home-article-date" data-date="Thu Aug 08 2019 21:00:00 GMT+0800">2019-08-08</span></span>
        
            <span class="home-article-category"><i class="fas fa-folder"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/categories/%E5%BA%94%E7%94%A8%E5%AE%89%E5%85%A8/">应用安全</a>&nbsp;
                        </li>
                    
                    
                </ul>
            </span>
        
        
            <span class="home-article-tag">
                <i class="fas fa-tags"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/tags/THINK/">THINK</a>&nbsp;
                        </li>
                    
                </ul>
            </span>
        
    </div>

    <a href="/SOAR(%E5%AE%89%E5%85%A8%E7%BC%96%E6%8E%92%E8%87%AA%E5%8A%A8%E5%8C%96%E4%B8%8E%E5%93%8D%E5%BA%94)%E4%B8%AD%E6%B1%82%E8%A7%A3%E5%AE%89%E5%85%A8%E8%BF%90%E8%90%A5%E4%B9%8B%E6%B3%95/">Read more&nbsp;<i class="fas fa-angle-right"></i></a>
</div>

            </li>
        
            <li class="home-article-item">

                

                <h3 class="home-article-title">
                    <a href="/K%20Nearest%20Neighbor/">
                        K Nearest Neighbor
                    </a>
                </h3>

                <!-- <div class="home-article-content markdown-body">
                    
                        <h4 id="0x01-KNN"><a href="#0x01-KNN" class="headerlink" title="0x01 KNN"></a>0x01 KNN</h4><p>采用测量不同特征值之间的距离进行分类</p>
                    
                </div> -->
                <div class="article-content markdown-body">
                    <h4 id="0x01-KNN"><a href="#0x01-KNN" class="headerlink" title="0x01 KNN"></a>0x01 KNN</h4><p>采用测量不同特征值之间的距离进行分类</p>
<span id="more"></span>

<p>优点：</p>
<ul>
<li>精度高</li>
<li>对异常值不敏感</li>
<li>无数据输入假定</li>
</ul>
<p>缺点：</p>
<ul>
<li>计算复杂度高</li>
<li>空间复杂度高</li>
</ul>
<p>适用数据范围：</p>
<ul>
<li>数值型</li>
<li>标称型</li>
</ul>
<h4 id="0x02-算法实现"><a href="#0x02-算法实现" class="headerlink" title="0x02 算法实现"></a>0x02 算法实现</h4><p><strong>算法描述</strong></p>
<ul>
<li><p>1.计算测试数据与各个训练数据之间的距离</p>
</li>
<li><p>2.按照距离的递增关系进行排序</p>
</li>
<li><p>3.选取距离最小的K个点</p>
</li>
<li><p>4.确定前K个点所在类别的出现频率</p>
</li>
<li><p>5.返回前K个点中出现频率最高的类别作为测试数据的预测分类</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">def classify0(inX, dataSet, labels, k):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line"></span><br><span class="line">    :param inX: 输入向量</span><br><span class="line">    :param dataSet: 训练数据集</span><br><span class="line">    :param labels: 标签</span><br><span class="line">    :param k: k</span><br><span class="line">    :return:</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    #距离计算</span><br><span class="line">    dataSetSize = dataSet.shape[0] #读取矩阵第一维度的长度</span><br><span class="line">    diffMat = tile(inX, (dataSetSize, 1)) - dataSet #tile把inX复制dataSetSize维度</span><br><span class="line">    sqDiffMat = diffMat**2</span><br><span class="line">    sqDistances = sqDiffMat.sum(axis=1)</span><br><span class="line">    distances = sqDistances**0.5</span><br><span class="line">    #选择距离最小的k个点</span><br><span class="line">    sortedDistIndicies = distances.argsort()</span><br><span class="line">    print(sortedDistIndicies)</span><br><span class="line">    classCount=&#123;&#125;</span><br><span class="line">    for i in range(k):</span><br><span class="line">        voteIlabel = labels[sortedDistIndicies[i]]</span><br><span class="line">        classCount[voteIlabel] = classCount.get(voteIlabel, 0) + 1</span><br><span class="line">    #排序</span><br><span class="line">    sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True)</span><br><span class="line">    return sortedClassCount[0][0]</span><br></pre></td></tr></table></figure>

<p><img lazyload src="/images/loading.svg" data-src="15366780712525.jpg"></p>
<h4 id="0x03-实例1"><a href="#0x03-实例1" class="headerlink" title="0x03 实例1"></a>0x03 实例1</h4><ul>
<li>收集数据</li>
</ul>
<p><img lazyload src="/images/loading.svg" data-src="15366784508012.jpg"></p>
<ul>
<li>准备数据</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def file2matrix(filename):</span><br><span class="line">    with open(filename, &quot;r&quot;) as fr:</span><br><span class="line">        frreadlines = fr.readlines()</span><br><span class="line">        numberOfLines = len(frreadlines)</span><br><span class="line">        returnMat = zeros((numberOfLines, 3))</span><br><span class="line">        classLabelVector = []</span><br><span class="line">        index = 0</span><br><span class="line">        for line in frreadlines:</span><br><span class="line">            line = line.strip()</span><br><span class="line">            listFromLine = line.split(&#x27;\t&#x27;)</span><br><span class="line">            returnMat[index, :] = listFromLine[0:3]</span><br><span class="line">            labels = &#123;&#x27;didntLike&#x27;: 1, &#x27;smallDoses&#x27;: 2, &#x27;largeDoses&#x27;: 3&#125;</span><br><span class="line">            classLabelVector.append(labels[listFromLine[-1]])</span><br><span class="line">            index += 1</span><br><span class="line">        return returnMat, classLabelVector</span><br></pre></td></tr></table></figure>

<p><img lazyload src="/images/loading.svg" data-src="15366798028789.jpg"></p>
<ul>
<li>分析数据</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def DataMat(data, labels):</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(111)</span><br><span class="line">    ax.scatter(data[:, 0], data[:, 1], 15.0*array(labels), 15.0*array(labels))</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p><img lazyload src="/images/loading.svg" data-src="15366804644367.jpg"></p>
<ul>
<li>处理数据</li>
</ul>
<p>归一化数值，转化到0~1之间</p>
<p><code>newV = (oldV-min)/(max-min)</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def autoNorm(dataSet):</span><br><span class="line">    minVals = dataSet.min(0)</span><br><span class="line">    maxVals = dataSet.max(0)</span><br><span class="line">    ranges = maxVals - minVals</span><br><span class="line">    m = dataSet.shape[0]</span><br><span class="line">    normDataSet = dataSet - tile(minVals, (m, 1))</span><br><span class="line">    normDataSet = normDataSet/tile(ranges, (m, 1))</span><br><span class="line">    return normDataSet, ranges, minVals</span><br></pre></td></tr></table></figure>

<p><img lazyload src="/images/loading.svg" data-src="15366812840237.jpg"></p>
<ul>
<li>测试算法</li>
</ul>
<p>对hoRatio和k进行参数调整，寻找最佳值</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def testData(data_mat, data_label):</span><br><span class="line">    hoRatio = 0.80 #内变量</span><br><span class="line">    normDataSet, ranges, minVals = autoNorm(data_mat)</span><br><span class="line">    m = normDataSet.shape[0]</span><br><span class="line">    numTestVecs = int(m * hoRatio)</span><br><span class="line">    trueCount = 0.0</span><br><span class="line">    for i in range(numTestVecs):</span><br><span class="line">        classifierResult = classify0(normDataSet[i, :], normDataSet[numTestVecs:m, :], data_label[numTestVecs:m], 5)</span><br><span class="line">        if (classifierResult == data_label[i]):</span><br><span class="line">            trueCount += 1.0</span><br><span class="line">    print(&quot;the total true rate is: %f&quot; % (trueCount/float(numTestVecs)*100) + &quot;%&quot;)</span><br><span class="line">    print(trueCount)</span><br></pre></td></tr></table></figure>
<p><img lazyload src="/images/loading.svg" data-src="15366824971011.jpg"></p>
<ul>
<li>使用算法</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def usemode(a, b, c):</span><br><span class="line">    file_path = &quot;datingTestSet.txt&quot;</span><br><span class="line">    data_mat, data_label = file2matrix(file_path)</span><br><span class="line">    normDataSet, ranges, minVals = autoNorm(data_mat)</span><br><span class="line">    inarr = array([a, b, c])</span><br><span class="line">    classifierResult = classify0((inarr-minVals)/ranges, normDataSet, data_label, 5)</span><br><span class="line">    return classifierResult</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">result = usemode(40920, 8.326976, 0.953952)</span><br></pre></td></tr></table></figure>

<p><code>(inarr-minVals)/ranges</code>是传入参数归一化后的结果，代入classify0模型，求出与历史数据中的临近值，即结果</p>
<h4 id="0x04-实例2"><a href="#0x04-实例2" class="headerlink" title="0x04 实例2"></a>0x04 实例2</h4><p>手写数字识别</p>
<p><img lazyload src="/images/loading.svg" data-src="15367314254091.jpg"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">def img2vector(filename):</span><br><span class="line">    returnVect = zeros((1,1024))</span><br><span class="line">    fr = open(filename)</span><br><span class="line">    for i in range(32):</span><br><span class="line">        lineStr = fr.readline()</span><br><span class="line">        for j in range(32):</span><br><span class="line">            returnVect[0, 32*i+j] = int(lineStr[j])</span><br><span class="line">    return returnVect</span><br><span class="line"></span><br><span class="line">def handwritingClassTest():</span><br><span class="line">    hwLabels = []</span><br><span class="line">    trainingFileList = listdir(&#x27;digits/trainingDigits&#x27;)</span><br><span class="line">    m = len(trainingFileList)</span><br><span class="line">    trainingMat = zeros((m, 1024))</span><br><span class="line">    for i in range(m):</span><br><span class="line">        #从文件名上解析当前文件中的正确值，存入label</span><br><span class="line">        fileNameStr = trainingFileList[i]</span><br><span class="line">        fileStr = fileNameStr.split(&#x27;.&#x27;)[0]</span><br><span class="line">        classNumStr = int(fileStr.split(&#x27;_&#x27;)[0])</span><br><span class="line">        hwLabels.append(classNumStr)</span><br><span class="line">        trainingMat[i, :] = img2vector(&#x27;digits/trainingDigits/%s&#x27; % fileNameStr)</span><br><span class="line">    testFileList = listdir(&#x27;digits/testDigits&#x27;)</span><br><span class="line">    trueCount = 0.0</span><br><span class="line">    mTest = len(testFileList)</span><br><span class="line">    for i in range(mTest):</span><br><span class="line">        fileNameStr = testFileList[i]</span><br><span class="line">        fileStr = fileNameStr.split(&#x27;.&#x27;)[0]</span><br><span class="line">        classNumStr = int(fileStr.split(&#x27;_&#x27;)[0])</span><br><span class="line">        vectorUnderTest = img2vector(&#x27;digits/testDigits/%s&#x27; % fileNameStr)</span><br><span class="line">        classifierResult = classify0(vectorUnderTest, trainingMat, hwLabels, 3)</span><br><span class="line">        if (classifierResult == classNumStr): trueCount += 1.0</span><br><span class="line">    print(&quot;\nthe total true rate is: %f&quot; % (trueCount/float(mTest)))</span><br></pre></td></tr></table></figure>

<p><img lazyload src="/images/loading.svg" data-src="15367319690071.jpg"></p>
<h4 id="0x05-安全应用"><a href="#0x05-安全应用" class="headerlink" title="0x05 安全应用"></a>0x05 安全应用</h4><p>从数学角度来看，异常行为检测也是对被检测的未知行为进行分类的过程，未知行为与已知的正常行为相似，则该行为是正常行为，否则是入侵行为[1]</p>
<p><img lazyload src="/images/loading.svg" data-src="15367429422892.jpg"></p>
<p>还有像<code>恶意软件检测</code>等安全领域应用</p>
<h4 id="0x06-其他应用"><a href="#0x06-其他应用" class="headerlink" title="0x06 其他应用"></a>0x06 其他应用</h4><ul>
<li>文字识别</li>
<li>人脸识别</li>
<li>医用图像处理</li>
</ul>
<p>参考：[1]<a class="link" target="_blank" rel="noopener" href="http://read.pudn.com/downloads116/ebook/489656/KNN.pdf">基于 kNN 算法的异常行为检测方法研究<i class="fas fa-external-link-alt"></i></a></p>

                </div>

                <div class="home-article-meta-info-container">
    <div class="home-article-meta-info">
        <span><i class="fas fa-history"></i>&nbsp;<span class="home-article-date" data-date="Sat Aug 03 2019 21:00:00 GMT+0800">2019-08-03</span></span>
        
            <span class="home-article-category"><i class="fas fa-folder"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/categories/%E5%AE%89%E5%85%A8%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/">安全数据分析</a>&nbsp;
                        </li>
                    
                    
                </ul>
            </span>
        
        
            <span class="home-article-tag">
                <i class="fas fa-tags"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/tags/ML/">ML</a>&nbsp;
                        </li>
                    
                </ul>
            </span>
        
    </div>

    <a href="/K%20Nearest%20Neighbor/">Read more&nbsp;<i class="fas fa-angle-right"></i></a>
</div>

            </li>
        
            <li class="home-article-item">

                

                <h3 class="home-article-title">
                    <a href="/Decision%20Tree/">
                        Decision Tree
                    </a>
                </h3>

                <!-- <div class="home-article-content markdown-body">
                    
                        0x01 DT分类算法
优点

计算复杂度不高
输出结果易于理解
中间值缺失不敏感
可处理不相关特征

缺点

可能会产生过度匹配问题

适用数据类型：

数值型
标称型

0x02 准备数据算法描述
1.根节点开始，测试待分类项中相应的特征属性

2.按照其值选择输出分支，直到到达叶子节点

3.将叶子节点存放的类别作为决策结果


划分数据集

将无序的数据变得更加有序
信息增益：划分数据集之后信息发生的变化熵：信息的期望值

熵计算公式


123456789101112131415def calcShannonEnt(dataSet):    numEntries = len(dataSet) #计算数据集中实例总数    labelCounts = &#123;&#125;    #统计每个键值的数量，dict    for featVec in dataSet:        currentLabel = featVec[-1]        if currentLabel not in labelCounts.keys():            labelCounts[currentLabel] = 0        labelCounts[currentLabel] += 1    shannonEnt = 0.0    #计算香农熵    for key in labelCounts:        prob = float(labelCounts[key])/numEntries        shannonEnt -= prob * log(prob, 2)    return shannonEnt



划分数据集

按照给定特征划分数据集
123456789101112131415def splitDataSet(dataSet, axis, value):    &#x27;&#x27;&#x27;        :param dataSet: 待划分数据集    :param axis: 特征    :param value: 特征值    :return: 符合条件的值列表    &#x27;&#x27;&#x27;    retDataSet = []    for featVec in dataSet:        if featVec[axis] == value:            reducedFeatVec = featVec[:axis]                 reducedFeatVec.extend(featVec[axis+1:]) #把特征列除去            retDataSet.append(reducedFeatVec)    return retDataSet

选择最好的数据集划分方式
熵越高，则混合的数据就越多
12345678910111213141516171819202122def chooseBestFeatureToSplit(dataSet):    &#x27;&#x27;&#x27;    :param dataSet: 数据集    :return:    &#x27;&#x27;&#x27;    numFeatures = len(dataSet[0]) - 1      #特征列的长度，-1为label    baseEntropy = calcShannonEnt(dataSet)  #计算数据集的香农熵    bestInfoGain = 0.0    bestFeature = -1    for i in range(numFeatures):        featList = [example[i] for example in dataSet] #创建一个list包含所有数据的第i个feature        uniqueVals = set(featList)       #转变为set格式        newEntropy = 0.0        for value in uniqueVals:            subDataSet = splitDataSet(dataSet, i, value) #遍历featList中的所有feature，对每个feture划分一次数据集            prob = len(subDataSet)/float(len(dataSet))            newEntropy += prob * calcShannonEnt(subDataSet)  #计算当前feature的香农熵        infoGain = baseEntropy - newEntropy     #计算熵差，信息增益        if (infoGain &gt; bestInfoGain): #计算最大信息增益            bestInfoGain = infoGain            bestFeature = i    return bestFeature                      #返回最好的feature

递归构建决策树
1.得到数据集2.最好feature划分3.递归划分
当处理了所有feature后，类标签仍然不唯一时，采用多数表决方式决定子节点分类
12345678def majorityCnt(classList):    classCount=&#123;&#125;    for vote in classList:        if vote not in classCount.keys():            classCount[vote] = 0        classCount[vote] += 1    sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True)    return sortedClassCount[0][0]

利用递归构建tree
12345678910111213141516def createTree(dataSet,labels):    classList = [example[-1] for example in dataSet] #数据集的所有类标签    if classList.count(classList[0]) == len(classList):         return classList[0] #当类标签完全相同返回该类标签    if len(dataSet[0]) == 1: #当所有属性都处理完，label仍然不唯一时，采用表决方式        return majorityCnt(classList)    bestFeat = chooseBestFeatureToSplit(dataSet)    bestFeatLabel = labels[bestFeat] #当前数据集选取的最好特征变量    myTree = &#123;bestFeatLabel: &#123;&#125;&#125;    del(labels[bestFeat]) #删除用过的feature    featValues = [example[bestFeat] for example in dataSet]    uniqueVals = set(featValues)    for value in uniqueVals:         subLabels = labels[:]        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels) #利用递归构建tree    return myTree  


绘制树形图

利用Matplotlib annotations实现绘制树形图
实现效果如下图

0x03 测试和储存分类器
将标签字符串转换为索引

123456789101112131415161718def classify(inputTree,featLabels,testVec):    &#x27;&#x27;&#x27;    :param inputTree: tree dict    :param featLabels: labels    :param testVec: 位置,eg.[1, 0]    :return:    &#x27;&#x27;&#x27;    firstStr = list(inputTree.keys())[0]    secondDict = inputTree[firstStr]    featIndex = featLabels.index(firstStr)    key = testVec[featIndex]    valueOfFeat = secondDict[key]    if isinstance(valueOfFeat, dict):         classLabel = classify(valueOfFeat, featLabels, testVec)    else:        classLabel = valueOfFeat    return classLabel



存储决策树

使用pickle持久化对象
pickle.dump(obj, file[, protocol])
12345678910def storeTree(inputTree, filename):    import pickle    fw = open(filename, &#x27;wb&#x27;)    pickle.dump(inputTree, fw)    fw.close()    def grabTree(filename):    import pickle    fr = open(filename, &#x27;rb&#x27;)    return pickle.load(fr)

0x04 使用决策树预测隐形眼镜类型
收集数据

lenses

准备数据

解析通过’\t’分隔的数据



分析数据&amp;训练模型

12labels = [&#x27;age&#x27;, &#x27;prescript&#x27;, &#x27;astigmatic&#x27;, &#x27;tearRate&#x27;]lenses_tree = createTree(lenses, labels)



测试模型



0x05 其它模型
ID3（分类树）
  每次根据“最大信息熵增益”选取当前最佳的特征来分割数据，并按照该特征的所有取值来切分

C4.5（分类树）
  ID3的升级版，采用信息增益比率，通过引入一个被称作分裂信息(Split information)的项来惩罚取值较多的Feature  弥补了ID3中不能处理特征属性值连续的问题

CART（分类回归树）
  CART是一棵二叉树，采用二元切分法，每次把数据切成两份，分别进入左子树、右子树。而且每个非叶子节点都有两个孩子，所以CART的叶子节点比非叶子多1


0x05 安全领域
分析恶意网络攻击和入侵
口令爆破检测
僵尸流量检测


                    
                </div> -->
                <div class="article-content markdown-body">
                    <h4 id="0x01-DT"><a href="#0x01-DT" class="headerlink" title="0x01 DT"></a>0x01 DT</h4><p>分类算法</p>
<p>优点</p>
<ul>
<li>计算复杂度不高</li>
<li>输出结果易于理解</li>
<li>中间值缺失不敏感</li>
<li>可处理不相关特征</li>
</ul>
<p>缺点</p>
<ul>
<li>可能会产生过度匹配问题</li>
</ul>
<p>适用数据类型：</p>
<ul>
<li>数值型</li>
<li>标称型</li>
</ul>
<h4 id="0x02-准备数据"><a href="#0x02-准备数据" class="headerlink" title="0x02 准备数据"></a>0x02 准备数据</h4><p><strong>算法描述</strong></p>
<pre><code>1.根节点开始，测试待分类项中相应的特征属性

2.按照其值选择输出分支，直到到达叶子节点

3.将叶子节点存放的类别作为决策结果
</code></pre>
<ul>
<li>划分数据集</li>
</ul>
<p>将无序的数据变得更加有序</p>
<p><code>信息增益</code>：划分数据集之后信息发生的变化<br><code>熵</code>：信息的期望值</p>
<ul>
<li>熵计算公式</li>
</ul>
<p><img lazyload src="/images/loading.svg" data-src="15367568995606.jpg"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def calcShannonEnt(dataSet):</span><br><span class="line">    numEntries = len(dataSet) #计算数据集中实例总数</span><br><span class="line">    labelCounts = &#123;&#125;</span><br><span class="line">    #统计每个键值的数量，dict</span><br><span class="line">    for featVec in dataSet:</span><br><span class="line">        currentLabel = featVec[-1]</span><br><span class="line">        if currentLabel not in labelCounts.keys():</span><br><span class="line">            labelCounts[currentLabel] = 0</span><br><span class="line">        labelCounts[currentLabel] += 1</span><br><span class="line">    shannonEnt = 0.0</span><br><span class="line">    #计算香农熵</span><br><span class="line">    for key in labelCounts:</span><br><span class="line">        prob = float(labelCounts[key])/numEntries</span><br><span class="line">        shannonEnt -= prob * log(prob, 2)</span><br><span class="line">    return shannonEnt</span><br></pre></td></tr></table></figure>

<p><img lazyload src="/images/loading.svg" data-src="15367570540472.jpg"></p>
<ul>
<li>划分数据集</li>
</ul>
<p><strong>按照给定特征划分数据集</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def splitDataSet(dataSet, axis, value):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    </span><br><span class="line">    :param dataSet: 待划分数据集</span><br><span class="line">    :param axis: 特征</span><br><span class="line">    :param value: 特征值</span><br><span class="line">    :return: 符合条件的值列表</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    retDataSet = []</span><br><span class="line">    for featVec in dataSet:</span><br><span class="line">        if featVec[axis] == value:</span><br><span class="line">            reducedFeatVec = featVec[:axis]     </span><br><span class="line">            reducedFeatVec.extend(featVec[axis+1:]) #把特征列除去</span><br><span class="line">            retDataSet.append(reducedFeatVec)</span><br><span class="line">    return retDataSet</span><br></pre></td></tr></table></figure>

<p><strong>选择最好的数据集划分方式</strong></p>
<p>熵越高，则混合的数据就越多</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">def chooseBestFeatureToSplit(dataSet):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    :param dataSet: 数据集</span><br><span class="line">    :return:</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    numFeatures = len(dataSet[0]) - 1      #特征列的长度，-1为label</span><br><span class="line">    baseEntropy = calcShannonEnt(dataSet)  #计算数据集的香农熵</span><br><span class="line">    bestInfoGain = 0.0</span><br><span class="line">    bestFeature = -1</span><br><span class="line">    for i in range(numFeatures):</span><br><span class="line">        featList = [example[i] for example in dataSet] #创建一个list包含所有数据的第i个feature</span><br><span class="line">        uniqueVals = set(featList)       #转变为set格式</span><br><span class="line">        newEntropy = 0.0</span><br><span class="line">        for value in uniqueVals:</span><br><span class="line">            subDataSet = splitDataSet(dataSet, i, value) #遍历featList中的所有feature，对每个feture划分一次数据集</span><br><span class="line">            prob = len(subDataSet)/float(len(dataSet))</span><br><span class="line">            newEntropy += prob * calcShannonEnt(subDataSet)  #计算当前feature的香农熵</span><br><span class="line">        infoGain = baseEntropy - newEntropy     #计算熵差，信息增益</span><br><span class="line">        if (infoGain &gt; bestInfoGain): #计算最大信息增益</span><br><span class="line">            bestInfoGain = infoGain</span><br><span class="line">            bestFeature = i</span><br><span class="line">    return bestFeature                      #返回最好的feature</span><br></pre></td></tr></table></figure>

<p><strong>递归构建决策树</strong></p>
<p>1.得到数据集<br>2.最好feature划分<br>3.递归划分</p>
<p>当处理了所有feature后，类标签仍然不唯一时，采用多数表决方式决定子节点分类</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def majorityCnt(classList):</span><br><span class="line">    classCount=&#123;&#125;</span><br><span class="line">    for vote in classList:</span><br><span class="line">        if vote not in classCount.keys():</span><br><span class="line">            classCount[vote] = 0</span><br><span class="line">        classCount[vote] += 1</span><br><span class="line">    sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True)</span><br><span class="line">    return sortedClassCount[0][0]</span><br></pre></td></tr></table></figure>

<p>利用递归构建tree</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">def createTree(dataSet,labels):</span><br><span class="line">    classList = [example[-1] for example in dataSet] #数据集的所有类标签</span><br><span class="line">    if classList.count(classList[0]) == len(classList): </span><br><span class="line">        return classList[0] #当类标签完全相同返回该类标签</span><br><span class="line">    if len(dataSet[0]) == 1: #当所有属性都处理完，label仍然不唯一时，采用表决方式</span><br><span class="line">        return majorityCnt(classList)</span><br><span class="line">    bestFeat = chooseBestFeatureToSplit(dataSet)</span><br><span class="line">    bestFeatLabel = labels[bestFeat] #当前数据集选取的最好特征变量</span><br><span class="line">    myTree = &#123;bestFeatLabel: &#123;&#125;&#125;</span><br><span class="line">    del(labels[bestFeat]) #删除用过的feature</span><br><span class="line">    featValues = [example[bestFeat] for example in dataSet]</span><br><span class="line">    uniqueVals = set(featValues)</span><br><span class="line">    for value in uniqueVals: </span><br><span class="line">        subLabels = labels[:]</span><br><span class="line">        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels) #利用递归构建tree</span><br><span class="line">    return myTree  </span><br></pre></td></tr></table></figure>

<ul>
<li>绘制树形图</li>
</ul>
<p>利用Matplotlib annotations实现绘制树形图</p>
<p>实现效果如下图</p>
<p><img lazyload src="/images/loading.svg" data-src="15370834199165.jpg"></p>
<h4 id="0x03-测试和储存分类器"><a href="#0x03-测试和储存分类器" class="headerlink" title="0x03 测试和储存分类器"></a>0x03 测试和储存分类器</h4><ul>
<li>将标签字符串转换为索引</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def classify(inputTree,featLabels,testVec):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line"></span><br><span class="line">    :param inputTree: tree dict</span><br><span class="line">    :param featLabels: labels</span><br><span class="line">    :param testVec: 位置,eg.[1, 0]</span><br><span class="line">    :return:</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    firstStr = list(inputTree.keys())[0]</span><br><span class="line">    secondDict = inputTree[firstStr]</span><br><span class="line">    featIndex = featLabels.index(firstStr)</span><br><span class="line">    key = testVec[featIndex]</span><br><span class="line">    valueOfFeat = secondDict[key]</span><br><span class="line">    if isinstance(valueOfFeat, dict): </span><br><span class="line">        classLabel = classify(valueOfFeat, featLabels, testVec)</span><br><span class="line">    else:</span><br><span class="line">        classLabel = valueOfFeat</span><br><span class="line">    return classLabel</span><br></pre></td></tr></table></figure>


<ul>
<li>存储决策树</li>
</ul>
<p>使用pickle持久化对象</p>
<p><code>pickle.dump(obj, file[, protocol])</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def storeTree(inputTree, filename):</span><br><span class="line">    import pickle</span><br><span class="line">    fw = open(filename, &#x27;wb&#x27;)</span><br><span class="line">    pickle.dump(inputTree, fw)</span><br><span class="line">    fw.close()</span><br><span class="line">    </span><br><span class="line">def grabTree(filename):</span><br><span class="line">    import pickle</span><br><span class="line">    fr = open(filename, &#x27;rb&#x27;)</span><br><span class="line">    return pickle.load(fr)</span><br></pre></td></tr></table></figure>

<h4 id="0x04-使用决策树预测隐形眼镜类型"><a href="#0x04-使用决策树预测隐形眼镜类型" class="headerlink" title="0x04 使用决策树预测隐形眼镜类型"></a>0x04 使用决策树预测隐形眼镜类型</h4><ul>
<li>收集数据</li>
</ul>
<p><a class="link" target="_blank" rel="noopener" href="http://archive.ics.uci.edu/ml/machine-learning-databases/lenses/">lenses<i class="fas fa-external-link-alt"></i></a></p>
<ul>
<li>准备数据</li>
</ul>
<p>解析通过’\t’分隔的数据</p>
<p><img lazyload src="/images/loading.svg" data-src="15370863087583.jpg"></p>
<p><img lazyload src="/images/loading.svg" data-src="15370863188621.jpg"></p>
<ul>
<li>分析数据&amp;训练模型</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">labels = [&#x27;age&#x27;, &#x27;prescript&#x27;, &#x27;astigmatic&#x27;, &#x27;tearRate&#x27;]</span><br><span class="line">lenses_tree = createTree(lenses, labels)</span><br></pre></td></tr></table></figure>
<p><img lazyload src="/images/loading.svg" data-src="15370866231411.jpg"></p>
<p><img lazyload src="/images/loading.svg" data-src="15370866007025.jpg"></p>
<ul>
<li>测试模型</li>
</ul>
<p><img lazyload src="/images/loading.svg" data-src="15370873991863.jpg"></p>
<p><img lazyload src="/images/loading.svg" data-src="15370875259893.jpg"></p>
<h4 id="0x05-其它模型"><a href="#0x05-其它模型" class="headerlink" title="0x05 其它模型"></a>0x05 其它模型</h4><ul>
<li><p>ID3（分类树）</p>
<p>  每次根据“最大信息熵增益”选取当前最佳的特征来分割数据，并按照该特征的所有取值来切分</p>
</li>
<li><p>C4.5（分类树）</p>
<p>  ID3的升级版，采用信息增益比率，通过引入一个被称作分裂信息(Split information)的项来惩罚取值较多的Feature<br>  弥补了ID3中不能处理特征属性值连续的问题</p>
</li>
<li><p>CART（分类回归树）</p>
<p>  CART是一棵二叉树，采用二元切分法，每次把数据切成两份，分别进入左子树、右子树。而且每个非叶子节点都有两个孩子，所以CART的叶子节点比非叶子多1</p>
</li>
</ul>
<h4 id="0x05-安全领域"><a href="#0x05-安全领域" class="headerlink" title="0x05 安全领域"></a>0x05 安全领域</h4><ul>
<li>分析恶意网络攻击和入侵</li>
<li>口令爆破检测</li>
<li>僵尸流量检测</li>
</ul>

                </div>

                <div class="home-article-meta-info-container">
    <div class="home-article-meta-info">
        <span><i class="fas fa-history"></i>&nbsp;<span class="home-article-date" data-date="Thu Aug 01 2019 21:00:00 GMT+0800">2019-08-01</span></span>
        
            <span class="home-article-category"><i class="fas fa-folder"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/categories/%E5%AE%89%E5%85%A8%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/">安全数据分析</a>&nbsp;
                        </li>
                    
                    
                </ul>
            </span>
        
        
            <span class="home-article-tag">
                <i class="fas fa-tags"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/tags/ML/">ML</a>&nbsp;
                        </li>
                    
                </ul>
            </span>
        
    </div>

    <a href="/Decision%20Tree/">Read more&nbsp;<i class="fas fa-angle-right"></i></a>
</div>

            </li>
        
            <li class="home-article-item">

                

                <h3 class="home-article-title">
                    <a href="/Naive%20Bayes%EF%BC%88NB%EF%BC%89/">
                        Naive Bayes（NB）
                    </a>
                </h3>

                <!-- <div class="home-article-content markdown-body">
                    
                        0x01 NB朴素：整个形式化的过程只做最原始、最简单的假设
优点

数据较少情况下仍然有效
可以处理多类别问题

缺点

对于输入数据的处理方式比较敏感

适用数据类型

标称型

0x02 贝叶斯决策理论计算数据点属于每个类别的概率，并进行比较，选择具有最高概率的决策
条件概率
推导过程





0x03 构建文档分类器两个假设

特征之间相互独立（统计意义上的独立）
每个特征同等重要

word2vec
1234567891011121314151617181920212223242526272829303132333435363738def loadDataSet():    &#x27;&#x27;&#x27;    测试数据    :return:     &#x27;&#x27;&#x27;    postingList = [[&#x27;my&#x27;, &#x27;dog&#x27;, &#x27;has&#x27;, &#x27;flea&#x27;, &#x27;problems&#x27;, &#x27;help&#x27;, &#x27;please&#x27;],                 [&#x27;maybe&#x27;, &#x27;not&#x27;, &#x27;take&#x27;, &#x27;him&#x27;, &#x27;to&#x27;, &#x27;dog&#x27;, &#x27;park&#x27;, &#x27;stupid&#x27;],                 [&#x27;my&#x27;, &#x27;dalmation&#x27;, &#x27;is&#x27;, &#x27;so&#x27;, &#x27;cute&#x27;, &#x27;I&#x27;, &#x27;love&#x27;, &#x27;him&#x27;],                 [&#x27;stop&#x27;, &#x27;posting&#x27;, &#x27;stupid&#x27;, &#x27;worthless&#x27;, &#x27;garbage&#x27;],                 [&#x27;mr&#x27;, &#x27;licks&#x27;, &#x27;ate&#x27;, &#x27;my&#x27;, &#x27;steak&#x27;, &#x27;how&#x27;, &#x27;to&#x27;, &#x27;stop&#x27;, &#x27;him&#x27;],                 [&#x27;quit&#x27;, &#x27;buying&#x27;, &#x27;worthless&#x27;, &#x27;dog&#x27;, &#x27;food&#x27;, &#x27;stupid&#x27;]]    classVec = [0, 1, 0, 1, 0, 1]  #是否包含侮辱性词语，为1    return postingList, classVec                 def createVocabList(dataSet):    &#x27;&#x27;&#x27;    创建dataSet的不重复词列表    :param dataSet:     :return:     &#x27;&#x27;&#x27;    vocabSet = set([])    for document in dataSet:        vocabSet = vocabSet | set(document)    return list(vocabSet)def setOfWords2Vec(vocabList, inputSet):    &#x27;&#x27;&#x27;    :param vocabList: 不重复词列表    :param inputSet: 某文档    :return: 文档向量    &#x27;&#x27;&#x27;    returnVec = [0]*len(vocabList) #创建一个长度和vocabList相等的全部为0的向量    for word in inputSet:        if word in vocabList:            returnVec[vocabList.index(word)] = 1        else:            print(&quot;the word: %s is not in my Vocabulary!&quot; % word)    return returnVec #[0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]

训练算法

从词向量计算概率

12for postdoc in postingList:    trainmat.append(setOfWords2Vec(vocablist, postdoc))

通过setOfWords2Vec方法对文档进行处理，返回文档向量
123456789101112131415161718192021def trainNB0(trainMatrix,trainCategory):    numTrainDocs = len(trainMatrix) #6 文档矩阵的行数    numWords = len(trainMatrix[0]) #32 矩阵的长度    pAbusive = sum(trainCategory)/float(numTrainDocs) #3/6  文档属于侮辱类型的概率    p0Num = ones(numWords) #ones函数可以创建任意维度和元素个数的数组，其元素值均为1    p1Num = ones(numWords)    p0Denom = 0.0    p1Denom = 0.0    for i in range(numTrainDocs):        if trainCategory[i] == 1:            p1Num += trainMatrix[i] #如果标签为侮辱性的，则两个列表相加            p1Denom += sum(trainMatrix[i]) #侮辱性文档的词数相加        else:            p0Num += trainMatrix[i]            p0Denom += sum(trainMatrix[i])    #p1num：[2. 2. 1. 1. 1. 1. 2. 1. 1. 2. 2. 1. 1. 1. 4. 2. 3. 2. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 1. 2. 1. 3.]    #p1Demon：19.0    p1Vect = log(p1Num/p1Denom)          #将单个词的数目除以总词数得到条件概率    p0Vect = log(p0Num/p0Denom)    return p0Vect, p1Vect, pAbusive

概率向量：在给定文档类别条件下词汇表中单词的出现概率p0Vect:正常文档的概率向量p1Vect:侮辱性文档概率向量pAbusive:侮辱文档的概率

概率值为0问题

利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率，即计算p(w0|1)p(w1|1)p(w2|1)。如果其中一个概率值为0，那么最后的乘积也为0。为降低 这种影响，可以将所有词的出现数初始化为1，并将分母初始化为2
12p0Denom = 2.0p1Denom = 2.0


下溢出问题

相乘许多很小的数，最后四舍五入后会得到0
p(w0|ci)*p(w1|ci)*...*p(w0|ci) 取对数，得到ln(p(w0|ci))+ln(p(w1|ci))+...+ln(p(w0|ci))
12p1Vect = log(p1Num/p1Denom)         p0Vect = log(p0Num/p0Denom)

测试算法

的含义为给定w向量的基础上来自类别ci的概率是多少
p(ci)的概率为pAbusive接下来需要计算p(w|ci)，假设所有词都互相独立，即p(w0,w1,w2..wN|ci)=p(w0|ci)p(w1|ci)p(w2|ci)...p(wN|ci)
因为P(w)P(ci)两者是一样的，可以忽略
因为log(p(w|c)p(c)) = log(p(w|c)) + log(p(c))，所以在classifyNB方法中求和
123456789101112131415def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):    &#x27;&#x27;&#x27;    元素相乘    :param vec2Classify:要分类的向量    :param p0Vec:正常文档概率向量    :param p1Vec:侮辱文档概率向量    :param pClass1:侮辱文档的概率    :return:1 or 0    &#x27;&#x27;&#x27;    p1 = sum(vec2Classify * p1Vec) + log(pClass1)    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)    if p1 &gt; p0:        return 1    else:         return 0

便利函数
12345678910111213def testingNB():    listOPosts,listClasses = loadDataSet()    myVocabList = createVocabList(listOPosts)    trainMat=[]    for postinDoc in listOPosts:        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))    p0V,p1V,pAb = trainNB0(array(trainMat), array(listClasses))    testEntry = [&#x27;love&#x27;, &#x27;my&#x27;, &#x27;dalmation&#x27;]    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))    print(testEntry, &#x27;classified as: &#x27;, classifyNB(thisDoc, p0V, p1V, pAb))    testEntry = [&#x27;stupid&#x27;, &#x27;garbage&#x27;]    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))    print(testEntry, &#x27;classified as: &#x27;, classifyNB(thisDoc, p0V, p1V, pAb))



词袋模型

在词袋中，每个单词可以出现 多次，而在词集中，每个词只能出现一次
每当遇到一个单词时，词向量中的对应值会+1
123456def bagOfWords2VecMN(vocabList, inputSet):    returnVec = [0]*len(vocabList)    for word in inputSet:        if word in vocabList:            returnVec[vocabList.index(word)] += 1    return returnVec


0x04 Action 1垃圾邮件判断
123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051def textParse(bigString):    &#x27;&#x27;&#x27;    简单分词处理    :param bigString:     :return:     &#x27;&#x27;&#x27;    import re    listOfTokens = re.split(&#x27;\W*&#x27;, bigString)    return [tok.lower() for tok in listOfTokens if len(tok) &gt; 2] #取长度大于3，转化为小写    def spamTest():    &#x27;&#x27;&#x27;    数据输入    处理    分割    训练    测试    :return:     &#x27;&#x27;&#x27;    docList=[]    classList = []    fullText = []    for i in range(1, 26):        wordList = textParse(open(&#x27;email/spam/%d.txt&#x27; % i, &#x27;rb&#x27;).read().decode(&#x27;GBK&#x27;, &#x27;ignore&#x27;))        docList.append(wordList)        fullText.extend(wordList)        classList.append(1)        wordList = textParse(open(&#x27;email/ham/%d.txt&#x27; % i, &#x27;rb&#x27;).read().decode(&#x27;GBK&#x27;, &#x27;ignore&#x27;))        docList.append(wordList)        fullText.extend(wordList)        classList.append(0)    vocabList = createVocabList(docList) #创建不重复词表    trainingSet = list(range(50)) #[0, 1, 2, 3, 4, 5, 6, 7, 8...44, 45, 46, 47, 48, 49]    testSet=[]    for i in range(10): #随机选择10条数据作为测试集        randIndex = int(random.uniform(0, len(trainingSet)))        testSet.append(trainingSet[randIndex])        del(trainingSet[randIndex])      trainMat = []    trainClasses = []    for docIndex in trainingSet: # 训练集        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex])) #词袋模型，构建词向量        trainClasses.append(classList[docIndex])    p0V, p1V, pSpam = trainNB0(array(trainMat), array(trainClasses))    errorCount = 0    for docIndex in testSet: # 测试集        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])        if classifyNB(array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:            errorCount += 1            print(&quot;classification error&quot;, docList[docIndex])    print(&#x27;the error rate is: &#x27;, float(errorCount)/len(testSet))





                    
                </div> -->
                <div class="article-content markdown-body">
                    <h4 id="0x01-NB"><a href="#0x01-NB" class="headerlink" title="0x01 NB"></a>0x01 NB</h4><p>朴素：整个形式化的过程只做最原始、最简单的假设</p>
<p>优点</p>
<ul>
<li>数据较少情况下仍然有效</li>
<li>可以处理多类别问题</li>
</ul>
<p>缺点</p>
<ul>
<li>对于输入数据的处理方式比较敏感</li>
</ul>
<p>适用数据类型</p>
<ul>
<li>标称型</li>
</ul>
<h4 id="0x02-贝叶斯决策理论"><a href="#0x02-贝叶斯决策理论" class="headerlink" title="0x02 贝叶斯决策理论"></a>0x02 贝叶斯决策理论</h4><p>计算数据点属于每个类别的概率，并进行比较，选择具有最高概率的决策</p>
<p><strong>条件概率</strong></p>
<p>推导过程</p>
<p><img lazyload src="/images/loading.svg" data-src="15371937340058.jpg"></p>
<p><img lazyload src="/images/loading.svg" data-src="15371937397573.jpg"></p>
<p><img lazyload src="/images/loading.svg" data-src="15371937656472.jpg"></p>
<p><img lazyload src="/images/loading.svg" data-src="15371937826699.jpg"></p>
<p><img lazyload src="/images/loading.svg" data-src="15371937881588.jpg"></p>
<h4 id="0x03-构建文档分类器"><a href="#0x03-构建文档分类器" class="headerlink" title="0x03 构建文档分类器"></a>0x03 构建文档分类器</h4><p>两个假设</p>
<ul>
<li>特征之间相互独立（统计意义上的独立）</li>
<li>每个特征同等重要</li>
</ul>
<p><strong>word2vec</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">def loadDataSet():</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    测试数据</span><br><span class="line">    :return: </span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    postingList = [[&#x27;my&#x27;, &#x27;dog&#x27;, &#x27;has&#x27;, &#x27;flea&#x27;, &#x27;problems&#x27;, &#x27;help&#x27;, &#x27;please&#x27;],</span><br><span class="line">                 [&#x27;maybe&#x27;, &#x27;not&#x27;, &#x27;take&#x27;, &#x27;him&#x27;, &#x27;to&#x27;, &#x27;dog&#x27;, &#x27;park&#x27;, &#x27;stupid&#x27;],</span><br><span class="line">                 [&#x27;my&#x27;, &#x27;dalmation&#x27;, &#x27;is&#x27;, &#x27;so&#x27;, &#x27;cute&#x27;, &#x27;I&#x27;, &#x27;love&#x27;, &#x27;him&#x27;],</span><br><span class="line">                 [&#x27;stop&#x27;, &#x27;posting&#x27;, &#x27;stupid&#x27;, &#x27;worthless&#x27;, &#x27;garbage&#x27;],</span><br><span class="line">                 [&#x27;mr&#x27;, &#x27;licks&#x27;, &#x27;ate&#x27;, &#x27;my&#x27;, &#x27;steak&#x27;, &#x27;how&#x27;, &#x27;to&#x27;, &#x27;stop&#x27;, &#x27;him&#x27;],</span><br><span class="line">                 [&#x27;quit&#x27;, &#x27;buying&#x27;, &#x27;worthless&#x27;, &#x27;dog&#x27;, &#x27;food&#x27;, &#x27;stupid&#x27;]]</span><br><span class="line">    classVec = [0, 1, 0, 1, 0, 1]  #是否包含侮辱性词语，为1</span><br><span class="line">    return postingList, classVec</span><br><span class="line">                 </span><br><span class="line">def createVocabList(dataSet):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    创建dataSet的不重复词列表</span><br><span class="line">    :param dataSet: </span><br><span class="line">    :return: </span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    vocabSet = set([])</span><br><span class="line">    for document in dataSet:</span><br><span class="line">        vocabSet = vocabSet | set(document)</span><br><span class="line">    return list(vocabSet)</span><br><span class="line"></span><br><span class="line">def setOfWords2Vec(vocabList, inputSet):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    :param vocabList: 不重复词列表</span><br><span class="line">    :param inputSet: 某文档</span><br><span class="line">    :return: 文档向量</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    returnVec = [0]*len(vocabList) #创建一个长度和vocabList相等的全部为0的向量</span><br><span class="line">    for word in inputSet:</span><br><span class="line">        if word in vocabList:</span><br><span class="line">            returnVec[vocabList.index(word)] = 1</span><br><span class="line">        else:</span><br><span class="line">            print(&quot;the word: %s is not in my Vocabulary!&quot; % word)</span><br><span class="line">    return returnVec #[0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</span><br></pre></td></tr></table></figure>

<p><strong>训练算法</strong></p>
<ul>
<li>从词向量计算概率</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for postdoc in postingList:</span><br><span class="line">    trainmat.append(setOfWords2Vec(vocablist, postdoc))</span><br></pre></td></tr></table></figure>
<p><img lazyload src="/images/loading.svg" data-src="15372608870667.jpg"></p>
<p>通过setOfWords2Vec方法对文档进行处理，返回文档向量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def trainNB0(trainMatrix,trainCategory):</span><br><span class="line">    numTrainDocs = len(trainMatrix) #6 文档矩阵的行数</span><br><span class="line">    numWords = len(trainMatrix[0]) #32 矩阵的长度</span><br><span class="line">    pAbusive = sum(trainCategory)/float(numTrainDocs) #3/6  文档属于侮辱类型的概率</span><br><span class="line">    p0Num = ones(numWords) #ones函数可以创建任意维度和元素个数的数组，其元素值均为1</span><br><span class="line">    p1Num = ones(numWords)</span><br><span class="line">    p0Denom = 0.0</span><br><span class="line">    p1Denom = 0.0</span><br><span class="line">    for i in range(numTrainDocs):</span><br><span class="line">        if trainCategory[i] == 1:</span><br><span class="line">            p1Num += trainMatrix[i] #如果标签为侮辱性的，则两个列表相加</span><br><span class="line">            p1Denom += sum(trainMatrix[i]) #侮辱性文档的词数相加</span><br><span class="line">        else:</span><br><span class="line">            p0Num += trainMatrix[i]</span><br><span class="line">            p0Denom += sum(trainMatrix[i])</span><br><span class="line">    #p1num：[2. 2. 1. 1. 1. 1. 2. 1. 1. 2. 2. 1. 1. 1. 4. 2. 3. 2. 1. 1. 1. 1. 2. 2.</span><br><span class="line"> 2. 2. 1. 1. 1. 2. 1. 3.]</span><br><span class="line">    #p1Demon：19.0</span><br><span class="line">    p1Vect = log(p1Num/p1Denom)          #将单个词的数目除以总词数得到条件概率</span><br><span class="line">    p0Vect = log(p0Num/p0Denom)</span><br><span class="line">    return p0Vect, p1Vect, pAbusive</span><br></pre></td></tr></table></figure>
<p><img lazyload src="/images/loading.svg" data-src="15372610382692.jpg"></p>
<p><code>概率向量</code>：在给定文档类别条件下词汇表中单词的出现概率<br><code>p0Vect</code>:正常文档的概率向量<br><code>p1Vect</code>:侮辱性文档概率向量<br><code>pAbusive</code>:侮辱文档的概率</p>
<ul>
<li>概率值为0问题</li>
</ul>
<p>利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率，即计算p(w0|1)p(w1|1)p(w2|1)。如果其中一个概率值为0，那么最后的乘积也为0。为降低 这种影响，可以将所有词的出现数初始化为1，并将分母初始化为2</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p0Denom = 2.0</span><br><span class="line">p1Denom = 2.0</span><br></pre></td></tr></table></figure>

<ul>
<li>下溢出问题</li>
</ul>
<p>相乘许多很小的数，最后四舍五入后会得到0</p>
<p><code>p(w0|ci)*p(w1|ci)*...*p(w0|ci)</code> 取对数，得到<code>ln(p(w0|ci))+ln(p(w1|ci))+...+ln(p(w0|ci))</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p1Vect = log(p1Num/p1Denom)         </span><br><span class="line">p0Vect = log(p0Num/p0Denom)</span><br></pre></td></tr></table></figure>

<p><strong>测试算法</strong></p>
<p><img lazyload src="/images/loading.svg" data-src="15372632355434.jpg"></p>
<p><img lazyload src="/images/loading.svg" data-src="15372632623577.jpg"><br>的含义为给定w向量的基础上来自类别ci的概率是多少</p>
<p>p(ci)的概率为<code>pAbusive</code><br>接下来需要计算p(w|ci)，假设所有词都互相独立，即<br><code>p(w0,w1,w2..wN|ci)=p(w0|ci)p(w1|ci)p(w2|ci)...p(wN|ci)</code></p>
<p>因为P(w)P(ci)两者是一样的，可以忽略</p>
<p>因为<code>log(p(w|c)p(c)) = log(p(w|c)) + log(p(c))</code>，所以在classifyNB方法中求和</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    元素相乘</span><br><span class="line">    :param vec2Classify:要分类的向量</span><br><span class="line">    :param p0Vec:正常文档概率向量</span><br><span class="line">    :param p1Vec:侮辱文档概率向量</span><br><span class="line">    :param pClass1:侮辱文档的概率</span><br><span class="line">    :return:1 or 0</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    p1 = sum(vec2Classify * p1Vec) + log(pClass1)</span><br><span class="line">    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)</span><br><span class="line">    if p1 &gt; p0:</span><br><span class="line">        return 1</span><br><span class="line">    else: </span><br><span class="line">        return 0</span><br></pre></td></tr></table></figure>

<p><strong>便利函数</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def testingNB():</span><br><span class="line">    listOPosts,listClasses = loadDataSet()</span><br><span class="line">    myVocabList = createVocabList(listOPosts)</span><br><span class="line">    trainMat=[]</span><br><span class="line">    for postinDoc in listOPosts:</span><br><span class="line">        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))</span><br><span class="line">    p0V,p1V,pAb = trainNB0(array(trainMat), array(listClasses))</span><br><span class="line">    testEntry = [&#x27;love&#x27;, &#x27;my&#x27;, &#x27;dalmation&#x27;]</span><br><span class="line">    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))</span><br><span class="line">    print(testEntry, &#x27;classified as: &#x27;, classifyNB(thisDoc, p0V, p1V, pAb))</span><br><span class="line">    testEntry = [&#x27;stupid&#x27;, &#x27;garbage&#x27;]</span><br><span class="line">    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))</span><br><span class="line">    print(testEntry, &#x27;classified as: &#x27;, classifyNB(thisDoc, p0V, p1V, pAb))</span><br></pre></td></tr></table></figure>

<p><img lazyload src="/images/loading.svg" data-src="15372709062992.jpg"></p>
<ul>
<li>词袋模型</li>
</ul>
<p>在词袋中，每个单词可以出现 多次，而在词集中，每个词只能出现一次</p>
<p>每当遇到一个单词时，词向量中的对应值会+1</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def bagOfWords2VecMN(vocabList, inputSet):</span><br><span class="line">    returnVec = [0]*len(vocabList)</span><br><span class="line">    for word in inputSet:</span><br><span class="line">        if word in vocabList:</span><br><span class="line">            returnVec[vocabList.index(word)] += 1</span><br><span class="line">    return returnVec</span><br></pre></td></tr></table></figure>


<h4 id="0x04-Action-1"><a href="#0x04-Action-1" class="headerlink" title="0x04 Action 1"></a>0x04 Action 1</h4><p>垃圾邮件判断</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">def textParse(bigString):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    简单分词处理</span><br><span class="line">    :param bigString: </span><br><span class="line">    :return: </span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    import re</span><br><span class="line">    listOfTokens = re.split(&#x27;\W*&#x27;, bigString)</span><br><span class="line">    return [tok.lower() for tok in listOfTokens if len(tok) &gt; 2] #取长度大于3，转化为小写</span><br><span class="line">    </span><br><span class="line">def spamTest():</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    数据输入</span><br><span class="line">    处理</span><br><span class="line">    分割</span><br><span class="line">    训练</span><br><span class="line">    测试</span><br><span class="line">    :return: </span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    docList=[]</span><br><span class="line">    classList = []</span><br><span class="line">    fullText = []</span><br><span class="line">    for i in range(1, 26):</span><br><span class="line">        wordList = textParse(open(&#x27;email/spam/%d.txt&#x27; % i, &#x27;rb&#x27;).read().decode(&#x27;GBK&#x27;, &#x27;ignore&#x27;))</span><br><span class="line">        docList.append(wordList)</span><br><span class="line">        fullText.extend(wordList)</span><br><span class="line">        classList.append(1)</span><br><span class="line">        wordList = textParse(open(&#x27;email/ham/%d.txt&#x27; % i, &#x27;rb&#x27;).read().decode(&#x27;GBK&#x27;, &#x27;ignore&#x27;))</span><br><span class="line">        docList.append(wordList)</span><br><span class="line">        fullText.extend(wordList)</span><br><span class="line">        classList.append(0)</span><br><span class="line">    vocabList = createVocabList(docList) #创建不重复词表</span><br><span class="line">    trainingSet = list(range(50)) #[0, 1, 2, 3, 4, 5, 6, 7, 8...44, 45, 46, 47, 48, 49]</span><br><span class="line">    testSet=[]</span><br><span class="line">    for i in range(10): #随机选择10条数据作为测试集</span><br><span class="line">        randIndex = int(random.uniform(0, len(trainingSet)))</span><br><span class="line">        testSet.append(trainingSet[randIndex])</span><br><span class="line">        del(trainingSet[randIndex])  </span><br><span class="line">    trainMat = []</span><br><span class="line">    trainClasses = []</span><br><span class="line">    for docIndex in trainingSet: # 训练集</span><br><span class="line">        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex])) #词袋模型，构建词向量</span><br><span class="line">        trainClasses.append(classList[docIndex])</span><br><span class="line">    p0V, p1V, pSpam = trainNB0(array(trainMat), array(trainClasses))</span><br><span class="line">    errorCount = 0</span><br><span class="line">    for docIndex in testSet: # 测试集</span><br><span class="line">        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])</span><br><span class="line">        if classifyNB(array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:</span><br><span class="line">            errorCount += 1</span><br><span class="line">            print(&quot;classification error&quot;, docList[docIndex])</span><br><span class="line">    print(&#x27;the error rate is: &#x27;, float(errorCount)/len(testSet))</span><br></pre></td></tr></table></figure>





                </div>

                <div class="home-article-meta-info-container">
    <div class="home-article-meta-info">
        <span><i class="fas fa-history"></i>&nbsp;<span class="home-article-date" data-date="Sun Jul 28 2019 21:00:00 GMT+0800">2019-07-28</span></span>
        
            <span class="home-article-category"><i class="fas fa-folder"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/categories/%E5%AE%89%E5%85%A8%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/">安全数据分析</a>&nbsp;
                        </li>
                    
                    
                </ul>
            </span>
        
        
            <span class="home-article-tag">
                <i class="fas fa-tags"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/tags/ML/">ML</a>&nbsp;
                        </li>
                    
                </ul>
            </span>
        
    </div>

    <a href="/Naive%20Bayes%EF%BC%88NB%EF%BC%89/">Read more&nbsp;<i class="fas fa-angle-right"></i></a>
</div>

            </li>
        
            <li class="home-article-item">

                

                <h3 class="home-article-title">
                    <a href="/Logistic%20Regression/">
                        Logistic Regression
                    </a>
                </h3>

                <!-- <div class="home-article-content markdown-body">
                    
                        0x01 LR根据现有数据对分类边界线建立回归公式，以此进行分类
LR的目的是寻找一个非线性函数Sigmoid的最佳拟合参数，求解过程可以由最优化 算法来完成在最优化算法中，最常用的就是梯度上升算法，而梯度上升算法又可以简化为随机梯度上升算法
优点

计算代价不高
易于理解和实现

缺点

容易欠拟合
分类精度可能不高

适用数据类型

数值型
标称型

类阶跃函数：Sigmoid函数

LR分类器：在每个特征上都乘以一个回归系数，然后把 所有的结果值相加，将这个总和代入Sigmoid函数中，进而得到一个范围在0~1之间的数值。任 何大于0.5的数据被分入1类，小于0.5即被归入0类
0x02 训练算法确定回归系数

梯度上升法

要找到某函数的 最大值，最好的方法是沿着该函数的梯度方向探寻


训练数据


123456789101112131415161718192021222324252627def sigmoid(inX):    &#x27;&#x27;&#x27;    sigmoid函数    :param inX:    :return:    &#x27;&#x27;&#x27;    return 1.0/(1+exp(-inX))def gradAscent(dataMatIn, classLabels):    &#x27;&#x27;&#x27;    梯度上升优化算法    :param dataMatIn:     :param classLabels:     :return:     &#x27;&#x27;&#x27;    dataMatrix = mat(dataMatIn) #转换为numpy矩阵数据类型    labelMat = mat(classLabels).transpose()    m, n = shape(dataMatrix)    alpha = 0.001 #步长    maxCycles = 500 #迭代次数    weights = ones((n, 1))    for k in range(maxCycles):    #计算真实类别与预测类别的差值        h = sigmoid(dataMatrix*weights)     #矩阵相乘        error = (labelMat - h)              #向量相减        weights = weights + alpha * dataMatrix.transpose() * error #矩阵相乘    return weights




分析数据

12345678910111213141516171819202122232425262728293031def plotBestFit(weights):    &#x27;&#x27;&#x27;    import matplotlib.pyplot as plt    画出决策边界    :param weights:     :return:     &#x27;&#x27;&#x27;    dataMat, labelMat=loadDataSet()    dataArr = array(dataMat)    n = shape(dataArr)[0]     xcord1 = []    ycord1 = []    xcord2 = []    ycord2 = []    for i in range(n):        if int(labelMat[i]) == 1:            xcord1.append(dataArr[i, 1])            ycord1.append(dataArr[i, 2])        else:            xcord2.append(dataArr[i, 1])            ycord2.append(dataArr[i, 2])    fig = plt.figure()    ax = fig.add_subplot(111)    ax.scatter(xcord1, ycord1, s=30, c=&#x27;red&#x27;, marker=&#x27;s&#x27;)    ax.scatter(xcord2, ycord2, s=30, c=&#x27;green&#x27;)    x = arange(-3.0, 3.0, 0.1)    y = (-weights[0]-weights[1]*x)/weights[2] #设置sigmiod为0    ax.plot(x, y)    plt.xlabel(&#x27;X1&#x27;)    plt.ylabel(&#x27;X2&#x27;)    plt.show()
1plotBestFit(weights.getA()) #getA()函数与mat()函数的功能相反，将一个numpy矩阵转换为数组



随机梯度上升算法

在线学习算法一次仅用一个样本点来更新回归系数
123456789101112131415def stocGradAscent0(dataMatrix, classLabels):    &#x27;&#x27;&#x27;    随机梯度上升算法    :param dataMatrix:    :param classLabels:    :return:    &#x27;&#x27;&#x27;    m, n = shape(dataMatrix)    alpha = 0.01    weights = ones(n)   #初始化    for i in range(m):        h = sigmoid(sum(dataMatrix[i]*weights))        error = classLabels[i] - h        weights = weights + alpha * error * dataMatrix[i]    return weights



改进的随机梯度上升算法

1234567891011121314151617181920def stocGradAscent1(dataMatrix, classLabels, numIter=150):    &#x27;&#x27;&#x27;    改进的随机梯度上升算法    :param dataMatrix:    :param classLabels:    :param numIter:默认迭代次数    :return:    &#x27;&#x27;&#x27;    m, n = shape(dataMatrix)    weights = ones(n)    for j in range(numIter):        dataIndex = list(range(m))        for i in range(m):            alpha = 4/(1.0+j+i)+0.0001    #每次迭代进行调整，不断减小            randIndex = int(random.uniform(0, len(dataIndex))) #随机选择样本更新回归系数            h = sigmoid(sum(dataMatrix[randIndex]*weights))            error = classLabels[randIndex] - h            weights = weights + alpha * error * dataMatrix[randIndex]            del(dataIndex[randIndex])    return weights


0x03 实例1
准备数据

如何处理数据中的缺失值？
1.使用可用特征的均值来填补缺失值
2.使用特殊值来填补缺失值
3.忽略有缺失值的样本
4.使用相似样本的均值添补缺失值
5.使用另外的机器学习算法预测缺失值


测试算法

12345678910111213141516171819202122232425262728293031323334353637383940414243444546def classifyVector(inX, weights):    &#x27;&#x27;&#x27;    :param inX: 回归系数    :param weights: 特征向量    :return: 0 or 1    &#x27;&#x27;&#x27;    prob = sigmoid(sum(inX*weights))    if prob &gt; 0.5:        return 1.0    else:        return 0.0def colicTest():    frTrain = open(&#x27;horseColicTraining.txt&#x27;)    frTest = open(&#x27;horseColicTest.txt&#x27;)    trainingSet = []    trainingLabels = []    for line in frTrain.readlines():        currLine = line.strip().split(&#x27;\t&#x27;)        lineArr = []        for i in range(21):            lineArr.append(float(currLine[i]))        trainingSet.append(lineArr)        trainingLabels.append(float(currLine[21]))    trainWeights = stocGradAscent1(array(trainingSet), trainingLabels, 1000) #计算回归系数向量，迭代1000次    errorCount = 0    numTestVec = 0.0    for line in frTest.readlines(): #导入测试集计算分类错误率        numTestVec += 1.0        currLine = line.strip().split(&#x27;\t&#x27;)        lineArr = []        for i in range(21):            lineArr.append(float(currLine[i]))        if int(classifyVector(array(lineArr), trainWeights)) != int(currLine[21]):            errorCount += 1    errorRate = (float(errorCount)/numTestVec)    print(&quot;the error rate of this test is: %f&quot; % errorRate)    return errorRatedef multiTest():    numTests = 10    errorSum=0.0    for k in range(numTests): # 计算10次求平均值        errorSum += colicTest()    print(&quot;after %d iterations the average error rate is: %f&quot; % (numTests, errorSum/float(numTests)))



                    
                </div> -->
                <div class="article-content markdown-body">
                    <h4 id="0x01-LR"><a href="#0x01-LR" class="headerlink" title="0x01 LR"></a>0x01 LR</h4><p>根据现有数据对分类边界线建立回归公式，以此进行分类</p>
<p>LR的目的是寻找一个非线性函数Sigmoid的最佳拟合参数，求解过程可以由最优化 算法来完成<br>在最优化算法中，最常用的就是梯度上升算法，而梯度上升算法又可以简化为随机梯度上升算法</p>
<p>优点</p>
<ul>
<li>计算代价不高</li>
<li>易于理解和实现</li>
</ul>
<p>缺点</p>
<ul>
<li>容易欠拟合</li>
<li>分类精度可能不高</li>
</ul>
<p>适用数据类型</p>
<ul>
<li>数值型</li>
<li>标称型</li>
</ul>
<p>类阶跃函数：<code>Sigmoid函数</code></p>
<p><img lazyload src="/images/loading.svg" data-src="15377821130484.jpg"></p>
<p>LR分类器：<code>在每个特征上都乘以一个回归系数，然后把 所有的结果值相加，将这个总和代入Sigmoid函数中，进而得到一个范围在0~1之间的数值。任 何大于0.5的数据被分入1类，小于0.5即被归入0类</code></p>
<h4 id="0x02-训练算法"><a href="#0x02-训练算法" class="headerlink" title="0x02 训练算法"></a>0x02 训练算法</h4><p><strong>确定回归系数</strong></p>
<ul>
<li>梯度上升法</li>
</ul>
<p><code>要找到某函数的 最大值，最好的方法是沿着该函数的梯度方向探寻</code></p>
<p><img lazyload src="/images/loading.svg" data-src="15377952402353.jpg"></p>
<ul>
<li>训练数据</li>
</ul>
<p><img lazyload src="/images/loading.svg" data-src="15377958608774.jpg"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">def sigmoid(inX):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    sigmoid函数</span><br><span class="line">    :param inX:</span><br><span class="line">    :return:</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    return 1.0/(1+exp(-inX))</span><br><span class="line"></span><br><span class="line">def gradAscent(dataMatIn, classLabels):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    梯度上升优化算法</span><br><span class="line">    :param dataMatIn: </span><br><span class="line">    :param classLabels: </span><br><span class="line">    :return: </span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    dataMatrix = mat(dataMatIn) #转换为numpy矩阵数据类型</span><br><span class="line">    labelMat = mat(classLabels).transpose()</span><br><span class="line">    m, n = shape(dataMatrix)</span><br><span class="line">    alpha = 0.001 #步长</span><br><span class="line">    maxCycles = 500 #迭代次数</span><br><span class="line">    weights = ones((n, 1))</span><br><span class="line">    for k in range(maxCycles):</span><br><span class="line">    #计算真实类别与预测类别的差值</span><br><span class="line">        h = sigmoid(dataMatrix*weights)     #矩阵相乘</span><br><span class="line">        error = (labelMat - h)              #向量相减</span><br><span class="line">        weights = weights + alpha * dataMatrix.transpose() * error #矩阵相乘</span><br><span class="line">    return weights</span><br></pre></td></tr></table></figure>


<p><img lazyload src="/images/loading.svg" data-src="15377964122497.jpg"></p>
<ul>
<li>分析数据</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">def plotBestFit(weights):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    import matplotlib.pyplot as plt</span><br><span class="line">    画出决策边界</span><br><span class="line">    :param weights: </span><br><span class="line">    :return: </span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    dataMat, labelMat=loadDataSet()</span><br><span class="line">    dataArr = array(dataMat)</span><br><span class="line">    n = shape(dataArr)[0] </span><br><span class="line">    xcord1 = []</span><br><span class="line">    ycord1 = []</span><br><span class="line">    xcord2 = []</span><br><span class="line">    ycord2 = []</span><br><span class="line">    for i in range(n):</span><br><span class="line">        if int(labelMat[i]) == 1:</span><br><span class="line">            xcord1.append(dataArr[i, 1])</span><br><span class="line">            ycord1.append(dataArr[i, 2])</span><br><span class="line">        else:</span><br><span class="line">            xcord2.append(dataArr[i, 1])</span><br><span class="line">            ycord2.append(dataArr[i, 2])</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(111)</span><br><span class="line">    ax.scatter(xcord1, ycord1, s=30, c=&#x27;red&#x27;, marker=&#x27;s&#x27;)</span><br><span class="line">    ax.scatter(xcord2, ycord2, s=30, c=&#x27;green&#x27;)</span><br><span class="line">    x = arange(-3.0, 3.0, 0.1)</span><br><span class="line">    y = (-weights[0]-weights[1]*x)/weights[2] #设置sigmiod为0</span><br><span class="line">    ax.plot(x, y)</span><br><span class="line">    plt.xlabel(&#x27;X1&#x27;)</span><br><span class="line">    plt.ylabel(&#x27;X2&#x27;)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plotBestFit(weights.getA()) #getA()函数与mat()函数的功能相反，将一个numpy矩阵转换为数组</span><br></pre></td></tr></table></figure>

<p><img lazyload src="/images/loading.svg" data-src="15377969992698.jpg"></p>
<ul>
<li>随机梯度上升算法</li>
</ul>
<p><code>在线学习算法</code><br><code>一次仅用一个样本点来更新回归系数</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def stocGradAscent0(dataMatrix, classLabels):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    随机梯度上升算法</span><br><span class="line">    :param dataMatrix:</span><br><span class="line">    :param classLabels:</span><br><span class="line">    :return:</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    m, n = shape(dataMatrix)</span><br><span class="line">    alpha = 0.01</span><br><span class="line">    weights = ones(n)   #初始化</span><br><span class="line">    for i in range(m):</span><br><span class="line">        h = sigmoid(sum(dataMatrix[i]*weights))</span><br><span class="line">        error = classLabels[i] - h</span><br><span class="line">        weights = weights + alpha * error * dataMatrix[i]</span><br><span class="line">    return weights</span><br></pre></td></tr></table></figure>

<p><img lazyload src="/images/loading.svg" data-src="15377977124914.jpg"></p>
<ul>
<li>改进的随机梯度上升算法</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def stocGradAscent1(dataMatrix, classLabels, numIter=150):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    改进的随机梯度上升算法</span><br><span class="line">    :param dataMatrix:</span><br><span class="line">    :param classLabels:</span><br><span class="line">    :param numIter:默认迭代次数</span><br><span class="line">    :return:</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    m, n = shape(dataMatrix)</span><br><span class="line">    weights = ones(n)</span><br><span class="line">    for j in range(numIter):</span><br><span class="line">        dataIndex = list(range(m))</span><br><span class="line">        for i in range(m):</span><br><span class="line">            alpha = 4/(1.0+j+i)+0.0001    #每次迭代进行调整，不断减小</span><br><span class="line">            randIndex = int(random.uniform(0, len(dataIndex))) #随机选择样本更新回归系数</span><br><span class="line">            h = sigmoid(sum(dataMatrix[randIndex]*weights))</span><br><span class="line">            error = classLabels[randIndex] - h</span><br><span class="line">            weights = weights + alpha * error * dataMatrix[randIndex]</span><br><span class="line">            del(dataIndex[randIndex])</span><br><span class="line">    return weights</span><br></pre></td></tr></table></figure>

<p><img lazyload src="/images/loading.svg" data-src="15377984535574.jpg"></p>
<h4 id="0x03-实例1"><a href="#0x03-实例1" class="headerlink" title="0x03 实例1"></a>0x03 实例1</h4><ul>
<li>准备数据</li>
</ul>
<p>如何处理数据中的缺失值？</p>
<pre><code>1.使用可用特征的均值来填补缺失值
2.使用特殊值来填补缺失值
3.忽略有缺失值的样本
4.使用相似样本的均值添补缺失值
5.使用另外的机器学习算法预测缺失值
</code></pre>
<ul>
<li>测试算法</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">def classifyVector(inX, weights):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line"></span><br><span class="line">    :param inX: 回归系数</span><br><span class="line">    :param weights: 特征向量</span><br><span class="line">    :return: 0 or 1</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    prob = sigmoid(sum(inX*weights))</span><br><span class="line">    if prob &gt; 0.5:</span><br><span class="line">        return 1.0</span><br><span class="line">    else:</span><br><span class="line">        return 0.0</span><br><span class="line"></span><br><span class="line">def colicTest():</span><br><span class="line">    frTrain = open(&#x27;horseColicTraining.txt&#x27;)</span><br><span class="line">    frTest = open(&#x27;horseColicTest.txt&#x27;)</span><br><span class="line">    trainingSet = []</span><br><span class="line">    trainingLabels = []</span><br><span class="line">    for line in frTrain.readlines():</span><br><span class="line">        currLine = line.strip().split(&#x27;\t&#x27;)</span><br><span class="line">        lineArr = []</span><br><span class="line">        for i in range(21):</span><br><span class="line">            lineArr.append(float(currLine[i]))</span><br><span class="line">        trainingSet.append(lineArr)</span><br><span class="line">        trainingLabels.append(float(currLine[21]))</span><br><span class="line">    trainWeights = stocGradAscent1(array(trainingSet), trainingLabels, 1000) #计算回归系数向量，迭代1000次</span><br><span class="line">    errorCount = 0</span><br><span class="line">    numTestVec = 0.0</span><br><span class="line">    for line in frTest.readlines(): #导入测试集计算分类错误率</span><br><span class="line">        numTestVec += 1.0</span><br><span class="line">        currLine = line.strip().split(&#x27;\t&#x27;)</span><br><span class="line">        lineArr = []</span><br><span class="line">        for i in range(21):</span><br><span class="line">            lineArr.append(float(currLine[i]))</span><br><span class="line">        if int(classifyVector(array(lineArr), trainWeights)) != int(currLine[21]):</span><br><span class="line">            errorCount += 1</span><br><span class="line">    errorRate = (float(errorCount)/numTestVec)</span><br><span class="line">    print(&quot;the error rate of this test is: %f&quot; % errorRate)</span><br><span class="line">    return errorRate</span><br><span class="line"></span><br><span class="line">def multiTest():</span><br><span class="line">    numTests = 10</span><br><span class="line">    errorSum=0.0</span><br><span class="line">    for k in range(numTests): # 计算10次求平均值</span><br><span class="line">        errorSum += colicTest()</span><br><span class="line">    print(&quot;after %d iterations the average error rate is: %f&quot; % (numTests, errorSum/float(numTests)))</span><br></pre></td></tr></table></figure>

<p><img lazyload src="/images/loading.svg" data-src="15377994809259.jpg"></p>

                </div>

                <div class="home-article-meta-info-container">
    <div class="home-article-meta-info">
        <span><i class="fas fa-history"></i>&nbsp;<span class="home-article-date" data-date="Sat Jul 27 2019 21:00:00 GMT+0800">2019-07-27</span></span>
        
            <span class="home-article-category"><i class="fas fa-folder"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/categories/%E5%AE%89%E5%85%A8%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/">安全数据分析</a>&nbsp;
                        </li>
                    
                    
                </ul>
            </span>
        
        
            <span class="home-article-tag">
                <i class="fas fa-tags"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/tags/ML/">ML</a>&nbsp;
                        </li>
                    
                </ul>
            </span>
        
    </div>

    <a href="/Logistic%20Regression/">Read more&nbsp;<i class="fas fa-angle-right"></i></a>
</div>

            </li>
        
            <li class="home-article-item">

                

                <h3 class="home-article-title">
                    <a href="/Support%20Vector%20Machines%EF%BC%88SVM%EF%BC%89/">
                        Support Vector Machines（SVM）
                    </a>
                </h3>

                <!-- <div class="home-article-content markdown-body">
                    
                        0x01 SVM实现算法：序列最小优化（SMO）
支持向量：离分割超平面最近的那些点
优点：

范化错误率低
计算开销不大
结果易解释

缺点：

对参数调节和核函数的选择敏感
原始分类器需要修改才能处理多分类问题

适用数据类型：

数值型
标称型

0x02 SMO将大优化问题分解为多个小优化问题来求解
目标是求出一系列alpha和b，一旦求出了这些alpha，就很容易计算出权重向量w并得到分隔超平面
工作原理是:每次循环中选择两个alpha进行优化处理。一旦找到一对合适的alpha，那么就增大其中一个同时减小另一个

简化版实现

12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091def selectJrand(i, m):    &#x27;&#x27;&#x27;    在某个区间范围内随机选择一个整数    :param i: 第i个alpha的下标    :param m: 所有alpha的数目    :return:    &#x27;&#x27;&#x27;    j=i    while (j==i):        j = int(random.uniform(0, m))    return jdef clipAlpha(aj, H, L):    &#x27;&#x27;&#x27;    用于调整大于H或小于L的alpha值    :param aj:    :param H:    :param L:    :return:    &#x27;&#x27;&#x27;    if aj &gt; H:         aj = H    if L &gt; aj:        aj = L    return ajdef smoSimple(dataMatIn, classLabels, C, toler, maxIter):    &#x27;&#x27;&#x27;    :param dataMatIn:数据集    :param classLabels:类别标签    :param C:常数C    :param toler:容错率    :param maxIter:退出前的最大循环次数    :return:    &#x27;&#x27;&#x27;    dataMatrix = mat(dataMatIn)    labelMat = mat(classLabels).transpose()    b = 0    m, n = shape(dataMatrix)    alphas = mat(zeros((m, 1)))    iter = 0 #记录循环次数    while (iter &lt; maxIter):        alphaPairsChanged = 0 #用于记录alpha是否优化        for i in range(m):            fXi = float(multiply(alphas, labelMat).T*(dataMatrix*dataMatrix[i, :].T)) + b #预测的类别            Ei = fXi - float(labelMat[i])#预测和实际的误差            #如果误差过大，则进行优化            if ((labelMat[i]*Ei &lt; -toler) and (alphas[i] &lt; C)) or ((labelMat[i]*Ei &gt; toler) and (alphas[i] &gt; 0)):                j = selectJrand(i, m) #随机选择第二个alpha值                fXj = float(multiply(alphas, labelMat).T*(dataMatrix*dataMatrix[j, :].T)) + b #预测类别                Ej = fXj - float(labelMat[j]) #误差                alphaIold = alphas[i].copy()                alphaJold = alphas[j].copy()                #计算L和H                if (labelMat[i] != labelMat[j]):                    L = max(0, alphas[j] - alphas[i])                    H = min(C, C + alphas[j] - alphas[i])                else:                    L = max(0, alphas[j] + alphas[i] - C)                    H = min(C, alphas[j] + alphas[i])                if L==H:                    print(&quot;L==H&quot;)                    continue                eta = 2.0 * dataMatrix[i, :]*dataMatrix[j, :].T - dataMatrix[i, :]*dataMatrix[i, :].T - dataMatrix[j, :]*dataMatrix[j, :].T #最优修改量                if eta &gt;= 0:                    print(&quot;eta&gt;=0&quot;)                    continue                alphas[j] -= labelMat[j]*(Ei - Ej)/eta                alphas[j] = clipAlpha(alphas[j], H, L)                if (abs(alphas[j] - alphaJold) &lt; 0.00001):                    print(&quot;j not moving enough&quot;)                    continue                alphas[i] += labelMat[j]*labelMat[i]*(alphaJold - alphas[j])#修改i，修改量和j相同，方向相反                #设置常数项                b1 = b - Ei - labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i, :]*dataMatrix[i, :].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[i, :]*dataMatrix[j, :].T                b2 = b - Ej - labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i, :]*dataMatrix[j, :].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[j, :]*dataMatrix[j, :].T                if (0 &lt; alphas[i]) and (C &gt; alphas[i]):                    b = b1                elif (0 &lt; alphas[j]) and (C &gt; alphas[j]):                    b = b2                else:                    b = (b1 + b2)/2.0                alphaPairsChanged += 1                print(&quot;iter: %d i:%d, pairs changed %d&quot; % (iter, i, alphaPairsChanged))        if (alphaPairsChanged == 0):            iter += 1        else:            iter = 0        print(&quot;iteration number: %d&quot; % iter)    return b, alphas


找出哪些点是支持向量
123for i in range(100):    if alphas[i] &gt; 0.0:        print(dateArr[i], labelArr[i])



完整版实现


123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158class optStruct:    def __init__(self, dataMatIn, classLabels, C, toler, kTup):  # Initialize the structure with the parameters        self.X = dataMatIn        self.labelMat = classLabels        self.C = C        self.tol = toler        self.m = shape(dataMatIn)[0]        self.alphas = mat(zeros((self.m, 1)))        self.b = 0        self.eCache = mat(zeros((self.m, 2))) #first column is valid flag        self.K = mat(zeros((self.m, self.m)))        for i in range(self.m):            self.K[:, i] = kernelTrans(self.X, self.X[i, :], kTup)        def calcEk(oS, k):    &#x27;&#x27;&#x27;    计算E值并返回    :param oS:    :param k:    :return:    &#x27;&#x27;&#x27;    fXk = float(multiply(oS.alphas, oS.labelMat).T*oS.K[:, k] + oS.b)    Ek = fXk - float(oS.labelMat[k])    return Ek        def selectJ(i, oS, Ei):    &#x27;&#x27;&#x27;    选择第二个alpha    :param i:    :param oS:    :param Ei:    :return:    &#x27;&#x27;&#x27;    maxK = -1    maxDeltaE = 0    Ej = 0    oS.eCache[i] = [1, Ei]    validEcacheList = nonzero(oS.eCache[:, 0].A)[0]    if (len(validEcacheList)) &gt; 1:        for k in validEcacheList:            if k == i:                continue            Ek = calcEk(oS, k)            deltaE = abs(Ei - Ek)            if (deltaE &gt; maxDeltaE):                maxK = k                maxDeltaE = deltaE                Ej = Ek        return maxK, Ej    else:        j = selectJrand(i, oS.m)        Ej = calcEk(oS, j)    return j, Ejdef updateEk(oS, k):    &#x27;&#x27;&#x27;    计算误差值并存入缓存    :param oS:    :param k:    :return:    &#x27;&#x27;&#x27;    Ek = calcEk(oS, k)    oS.eCache[k] = [1, Ek]        def innerL(i, oS):    &#x27;&#x27;&#x27;    优化过程    :param i:    :param oS:    :return:    &#x27;&#x27;&#x27;    Ei = calcEk(oS, i)    if ((oS.labelMat[i]*Ei &lt; -oS.tol) and (oS.alphas[i] &lt; oS.C)) or ((oS.labelMat[i]*Ei &gt; oS.tol) and (oS.alphas[i] &gt; 0)):        j, Ej = selectJ(i, oS, Ei)        alphaIold = oS.alphas[i].copy(); alphaJold = oS.alphas[j].copy()        if (oS.labelMat[i] != oS.labelMat[j]):            L = max(0, oS.alphas[j] - oS.alphas[i])            H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i])        else:            L = max(0, oS.alphas[j] + oS.alphas[i] - oS.C)            H = min(oS.C, oS.alphas[j] + oS.alphas[i])        if L==H:            print(&quot;L==H&quot;)            return 0        eta = 2.0 * oS.K[i, j] - oS.K[i, i] - oS.K[j, j]        if eta &gt;= 0:            print(&quot;eta&gt;=0&quot;)            return 0        oS.alphas[j] -= oS.labelMat[j]*(Ei - Ej)/eta        oS.alphas[j] = clipAlpha(oS.alphas[j], H, L)        updateEk(oS, j)        if (abs(oS.alphas[j] - alphaJold) &lt; 0.00001):            print(&quot;j not moving enough&quot;)            return 0        oS.alphas[i] += oS.labelMat[j]*oS.labelMat[i]*(alphaJold - oS.alphas[j])        updateEk(oS, i)        b1 = oS.b - Ei - oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.K[i, i] - oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.K[i, j]        b2 = oS.b - Ej - oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.K[i, j] - oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.K[j, j]        if (0 &lt; oS.alphas[i]) and (oS.C &gt; oS.alphas[i]):            oS.b = b1        elif (0 &lt; oS.alphas[j]) and (oS.C &gt; oS.alphas[j]):            oS.b = b2        else:            oS.b = (b1 + b2)/2.0        return 1    else:        return 0def smoP(dataMatIn, classLabels, C, toler, maxIter,kTup=(&#x27;lin&#x27;, 0)):    &#x27;&#x27;&#x27;    完整的SMO外循环    :param dataMatIn:    :param classLabels:    :param C:    :param toler:    :param maxIter:    :param kTup:    :return:    &#x27;&#x27;&#x27;    oS = optStruct(mat(dataMatIn), mat(classLabels).transpose(), C, toler, kTup)    iter = 0    entireSet = True    alphaPairsChanged = 0    while (iter &lt; maxIter) and ((alphaPairsChanged &gt; 0) or (entireSet)):        alphaPairsChanged = 0        if entireSet:            for i in range(oS.m):                        alphaPairsChanged += innerL(i,oS)                print(&quot;fullSet, iter: %d i:%d, pairs changed %d&quot; % (iter, i, alphaPairsChanged))            iter += 1        else:            nonBoundIs = nonzero((oS.alphas.A &gt; 0) * (oS.alphas.A &lt; C))[0]            for i in nonBoundIs:                alphaPairsChanged += innerL(i, oS)                print(&quot;non-bound, iter: %d i:%d, pairs changed %d&quot; % (iter, i, alphaPairsChanged))            iter += 1        if entireSet:            entireSet = False        elif (alphaPairsChanged == 0):            entireSet = True        print(&quot;iteration number: %d&quot; % iter)    return oS.b, oS.alphasdef calcWs(alphas, dataArr, classLabels):    &#x27;&#x27;&#x27;    利用计算出的alpha进行分类    :param alphas:    :param dataArr:    :param classLabels:    :return:    &#x27;&#x27;&#x27;    X = mat(dataArr)    labelMat = mat(classLabels).transpose()    m, n = shape(X)    w = zeros((n, 1))    for i in range(m):        w += multiply(alphas[i]*labelMat[i], X[i, :].T)    return w

1234567for i in range(100):        if alphas[i] &gt; 0.0:            print(dateArr[i], labelArr[i])    ws = calcWs(alphas, dateArr, labelArr)    print(ws)    datmat = mat(dateArr)    print(datmat[0]*mat(ws) + b)


最后一行为测试结果，小于0属于-1类，大于0属于1类，等于0属于-1类
0x03 kernel

将数据映射到高维空间

将数据从一个特征空间转换到另一个特征空间映射会将低维特征空间映射到高维空间

径向基核函数


1234567891011121314151617181920def kernelTrans(X, A, kTup):    &#x27;&#x27;&#x27;    核函数    :param X:    :param A:    :param kTup:包含核函数信息的元组    :return:    &#x27;&#x27;&#x27;    m, n = shape(X)    K = mat(zeros((m, 1)))    if kTup[0]==&#x27;lin&#x27;:        K = X * A.T    elif kTup[0]==&#x27;rbf&#x27;:        for j in range(m):            deltaRow = X[j, :] - A            ...
                    
                </div> -->
                <div class="article-content markdown-body">
                    <h4 id="0x01-SVM"><a href="#0x01-SVM" class="headerlink" title="0x01 SVM"></a>0x01 SVM</h4><p>实现算法：序列最小优化（SMO）</p>
<p>支持向量：离分割超平面最近的那些点</p>
<p>优点：</p>
<ul>
<li>范化错误率低</li>
<li>计算开销不大</li>
<li>结果易解释</li>
</ul>
<p>缺点：</p>
<ul>
<li>对参数调节和核函数的选择敏感</li>
<li>原始分类器需要修改才能处理多分类问题</li>
</ul>
<p>适用数据类型：</p>
<ul>
<li>数值型</li>
<li>标称型</li>
</ul>
<h4 id="0x02-SMO"><a href="#0x02-SMO" class="headerlink" title="0x02 SMO"></a>0x02 SMO</h4><p>将大优化问题分解为多个小优化问题来求解</p>
<p>目标是求出一系列alpha和b，一旦求出了这些alpha，就很容易计算出权重向量w并得到分隔超平面</p>
<p>工作原理是:每次循环中选择两个alpha进行优化处理。一旦找到一对合适的alpha，那么就增大其中一个同时减小另一个</p>
<ul>
<li>简化版实现</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line">def selectJrand(i, m):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    在某个区间范围内随机选择一个整数</span><br><span class="line">    :param i: 第i个alpha的下标</span><br><span class="line">    :param m: 所有alpha的数目</span><br><span class="line">    :return:</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    j=i</span><br><span class="line">    while (j==i):</span><br><span class="line">        j = int(random.uniform(0, m))</span><br><span class="line">    return j</span><br><span class="line"></span><br><span class="line">def clipAlpha(aj, H, L):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    用于调整大于H或小于L的alpha值</span><br><span class="line">    :param aj:</span><br><span class="line">    :param H:</span><br><span class="line">    :param L:</span><br><span class="line">    :return:</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    if aj &gt; H: </span><br><span class="line">        aj = H</span><br><span class="line">    if L &gt; aj:</span><br><span class="line">        aj = L</span><br><span class="line">    return aj</span><br><span class="line"></span><br><span class="line">def smoSimple(dataMatIn, classLabels, C, toler, maxIter):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line"></span><br><span class="line">    :param dataMatIn:数据集</span><br><span class="line">    :param classLabels:类别标签</span><br><span class="line">    :param C:常数C</span><br><span class="line">    :param toler:容错率</span><br><span class="line">    :param maxIter:退出前的最大循环次数</span><br><span class="line">    :return:</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    dataMatrix = mat(dataMatIn)</span><br><span class="line">    labelMat = mat(classLabels).transpose()</span><br><span class="line">    b = 0</span><br><span class="line">    m, n = shape(dataMatrix)</span><br><span class="line">    alphas = mat(zeros((m, 1)))</span><br><span class="line">    iter = 0 #记录循环次数</span><br><span class="line">    while (iter &lt; maxIter):</span><br><span class="line">        alphaPairsChanged = 0 #用于记录alpha是否优化</span><br><span class="line">        for i in range(m):</span><br><span class="line">            fXi = float(multiply(alphas, labelMat).T*(dataMatrix*dataMatrix[i, :].T)) + b #预测的类别</span><br><span class="line">            Ei = fXi - float(labelMat[i])#预测和实际的误差</span><br><span class="line">            #如果误差过大，则进行优化</span><br><span class="line">            if ((labelMat[i]*Ei &lt; -toler) and (alphas[i] &lt; C)) or ((labelMat[i]*Ei &gt; toler) and (alphas[i] &gt; 0)):</span><br><span class="line">                j = selectJrand(i, m) #随机选择第二个alpha值</span><br><span class="line">                fXj = float(multiply(alphas, labelMat).T*(dataMatrix*dataMatrix[j, :].T)) + b #预测类别</span><br><span class="line">                Ej = fXj - float(labelMat[j]) #误差</span><br><span class="line">                alphaIold = alphas[i].copy()</span><br><span class="line">                alphaJold = alphas[j].copy()</span><br><span class="line">                #计算L和H</span><br><span class="line">                if (labelMat[i] != labelMat[j]):</span><br><span class="line">                    L = max(0, alphas[j] - alphas[i])</span><br><span class="line">                    H = min(C, C + alphas[j] - alphas[i])</span><br><span class="line">                else:</span><br><span class="line">                    L = max(0, alphas[j] + alphas[i] - C)</span><br><span class="line">                    H = min(C, alphas[j] + alphas[i])</span><br><span class="line">                if L==H:</span><br><span class="line">                    print(&quot;L==H&quot;)</span><br><span class="line">                    continue</span><br><span class="line">                eta = 2.0 * dataMatrix[i, :]*dataMatrix[j, :].T - dataMatrix[i, :]*dataMatrix[i, :].T - dataMatrix[j, :]*dataMatrix[j, :].T #最优修改量</span><br><span class="line">                if eta &gt;= 0:</span><br><span class="line">                    print(&quot;eta&gt;=0&quot;)</span><br><span class="line">                    continue</span><br><span class="line">                alphas[j] -= labelMat[j]*(Ei - Ej)/eta</span><br><span class="line">                alphas[j] = clipAlpha(alphas[j], H, L)</span><br><span class="line">                if (abs(alphas[j] - alphaJold) &lt; 0.00001):</span><br><span class="line">                    print(&quot;j not moving enough&quot;)</span><br><span class="line">                    continue</span><br><span class="line">                alphas[i] += labelMat[j]*labelMat[i]*(alphaJold - alphas[j])#修改i，修改量和j相同，方向相反</span><br><span class="line">                #设置常数项</span><br><span class="line">                b1 = b - Ei - labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i, :]*dataMatrix[i, :].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[i, :]*dataMatrix[j, :].T</span><br><span class="line">                b2 = b - Ej - labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i, :]*dataMatrix[j, :].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[j, :]*dataMatrix[j, :].T</span><br><span class="line">                if (0 &lt; alphas[i]) and (C &gt; alphas[i]):</span><br><span class="line">                    b = b1</span><br><span class="line">                elif (0 &lt; alphas[j]) and (C &gt; alphas[j]):</span><br><span class="line">                    b = b2</span><br><span class="line">                else:</span><br><span class="line">                    b = (b1 + b2)/2.0</span><br><span class="line">                alphaPairsChanged += 1</span><br><span class="line">                print(&quot;iter: %d i:%d, pairs changed %d&quot; % (iter, i, alphaPairsChanged))</span><br><span class="line">        if (alphaPairsChanged == 0):</span><br><span class="line">            iter += 1</span><br><span class="line">        else:</span><br><span class="line">            iter = 0</span><br><span class="line">        print(&quot;iteration number: %d&quot; % iter)</span><br><span class="line">    return b, alphas</span><br></pre></td></tr></table></figure>

<p><img lazyload src="/images/loading.svg" data-src="15384703252463.jpg"></p>
<p>找出哪些点是支持向量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for i in range(100):</span><br><span class="line">    if alphas[i] &gt; 0.0:</span><br><span class="line">        print(dateArr[i], labelArr[i])</span><br></pre></td></tr></table></figure>

<p><img lazyload src="/images/loading.svg" data-src="15384704640833.jpg"></p>
<ul>
<li>完整版实现</li>
</ul>
<p><img lazyload src="/images/loading.svg" data-src="15384717843165.jpg"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br></pre></td><td class="code"><pre><span class="line">class optStruct:</span><br><span class="line">    def __init__(self, dataMatIn, classLabels, C, toler, kTup):  # Initialize the structure with the parameters</span><br><span class="line">        self.X = dataMatIn</span><br><span class="line">        self.labelMat = classLabels</span><br><span class="line">        self.C = C</span><br><span class="line">        self.tol = toler</span><br><span class="line">        self.m = shape(dataMatIn)[0]</span><br><span class="line">        self.alphas = mat(zeros((self.m, 1)))</span><br><span class="line">        self.b = 0</span><br><span class="line">        self.eCache = mat(zeros((self.m, 2))) #first column is valid flag</span><br><span class="line">        self.K = mat(zeros((self.m, self.m)))</span><br><span class="line">        for i in range(self.m):</span><br><span class="line">            self.K[:, i] = kernelTrans(self.X, self.X[i, :], kTup)</span><br><span class="line">        </span><br><span class="line">def calcEk(oS, k):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    计算E值并返回</span><br><span class="line">    :param oS:</span><br><span class="line">    :param k:</span><br><span class="line">    :return:</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    fXk = float(multiply(oS.alphas, oS.labelMat).T*oS.K[:, k] + oS.b)</span><br><span class="line">    Ek = fXk - float(oS.labelMat[k])</span><br><span class="line">    return Ek</span><br><span class="line">        </span><br><span class="line">def selectJ(i, oS, Ei):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    选择第二个alpha</span><br><span class="line">    :param i:</span><br><span class="line">    :param oS:</span><br><span class="line">    :param Ei:</span><br><span class="line">    :return:</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    maxK = -1</span><br><span class="line">    maxDeltaE = 0</span><br><span class="line">    Ej = 0</span><br><span class="line">    oS.eCache[i] = [1, Ei]</span><br><span class="line">    validEcacheList = nonzero(oS.eCache[:, 0].A)[0]</span><br><span class="line">    if (len(validEcacheList)) &gt; 1:</span><br><span class="line">        for k in validEcacheList:</span><br><span class="line">            if k == i:</span><br><span class="line">                continue</span><br><span class="line">            Ek = calcEk(oS, k)</span><br><span class="line">            deltaE = abs(Ei - Ek)</span><br><span class="line">            if (deltaE &gt; maxDeltaE):</span><br><span class="line">                maxK = k</span><br><span class="line">                maxDeltaE = deltaE</span><br><span class="line">                Ej = Ek</span><br><span class="line">        return maxK, Ej</span><br><span class="line">    else:</span><br><span class="line">        j = selectJrand(i, oS.m)</span><br><span class="line">        Ej = calcEk(oS, j)</span><br><span class="line">    return j, Ej</span><br><span class="line"></span><br><span class="line">def updateEk(oS, k):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    计算误差值并存入缓存</span><br><span class="line">    :param oS:</span><br><span class="line">    :param k:</span><br><span class="line">    :return:</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    Ek = calcEk(oS, k)</span><br><span class="line">    oS.eCache[k] = [1, Ek]</span><br><span class="line">        </span><br><span class="line">def innerL(i, oS):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    优化过程</span><br><span class="line">    :param i:</span><br><span class="line">    :param oS:</span><br><span class="line">    :return:</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    Ei = calcEk(oS, i)</span><br><span class="line">    if ((oS.labelMat[i]*Ei &lt; -oS.tol) and (oS.alphas[i] &lt; oS.C)) or ((oS.labelMat[i]*Ei &gt; oS.tol) and (oS.alphas[i] &gt; 0)):</span><br><span class="line">        j, Ej = selectJ(i, oS, Ei)</span><br><span class="line">        alphaIold = oS.alphas[i].copy(); alphaJold = oS.alphas[j].copy()</span><br><span class="line">        if (oS.labelMat[i] != oS.labelMat[j]):</span><br><span class="line">            L = max(0, oS.alphas[j] - oS.alphas[i])</span><br><span class="line">            H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i])</span><br><span class="line">        else:</span><br><span class="line">            L = max(0, oS.alphas[j] + oS.alphas[i] - oS.C)</span><br><span class="line">            H = min(oS.C, oS.alphas[j] + oS.alphas[i])</span><br><span class="line">        if L==H:</span><br><span class="line">            print(&quot;L==H&quot;)</span><br><span class="line">            return 0</span><br><span class="line">        eta = 2.0 * oS.K[i, j] - oS.K[i, i] - oS.K[j, j]</span><br><span class="line">        if eta &gt;= 0:</span><br><span class="line">            print(&quot;eta&gt;=0&quot;)</span><br><span class="line">            return 0</span><br><span class="line">        oS.alphas[j] -= oS.labelMat[j]*(Ei - Ej)/eta</span><br><span class="line">        oS.alphas[j] = clipAlpha(oS.alphas[j], H, L)</span><br><span class="line">        updateEk(oS, j)</span><br><span class="line">        if (abs(oS.alphas[j] - alphaJold) &lt; 0.00001):</span><br><span class="line">            print(&quot;j not moving enough&quot;)</span><br><span class="line">            return 0</span><br><span class="line">        oS.alphas[i] += oS.labelMat[j]*oS.labelMat[i]*(alphaJold - oS.alphas[j])</span><br><span class="line">        updateEk(oS, i)</span><br><span class="line">        b1 = oS.b - Ei - oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.K[i, i] - oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.K[i, j]</span><br><span class="line">        b2 = oS.b - Ej - oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.K[i, j] - oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.K[j, j]</span><br><span class="line">        if (0 &lt; oS.alphas[i]) and (oS.C &gt; oS.alphas[i]):</span><br><span class="line">            oS.b = b1</span><br><span class="line">        elif (0 &lt; oS.alphas[j]) and (oS.C &gt; oS.alphas[j]):</span><br><span class="line">            oS.b = b2</span><br><span class="line">        else:</span><br><span class="line">            oS.b = (b1 + b2)/2.0</span><br><span class="line">        return 1</span><br><span class="line">    else:</span><br><span class="line">        return 0</span><br><span class="line"></span><br><span class="line">def smoP(dataMatIn, classLabels, C, toler, maxIter,kTup=(&#x27;lin&#x27;, 0)):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    完整的SMO外循环</span><br><span class="line">    :param dataMatIn:</span><br><span class="line">    :param classLabels:</span><br><span class="line">    :param C:</span><br><span class="line">    :param toler:</span><br><span class="line">    :param maxIter:</span><br><span class="line">    :param kTup:</span><br><span class="line">    :return:</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    oS = optStruct(mat(dataMatIn), mat(classLabels).transpose(), C, toler, kTup)</span><br><span class="line">    iter = 0</span><br><span class="line">    entireSet = True</span><br><span class="line">    alphaPairsChanged = 0</span><br><span class="line">    while (iter &lt; maxIter) and ((alphaPairsChanged &gt; 0) or (entireSet)):</span><br><span class="line">        alphaPairsChanged = 0</span><br><span class="line">        if entireSet:</span><br><span class="line">            for i in range(oS.m):        </span><br><span class="line">                alphaPairsChanged += innerL(i,oS)</span><br><span class="line">                print(&quot;fullSet, iter: %d i:%d, pairs changed %d&quot; % (iter, i, alphaPairsChanged))</span><br><span class="line">            iter += 1</span><br><span class="line">        else:</span><br><span class="line">            nonBoundIs = nonzero((oS.alphas.A &gt; 0) * (oS.alphas.A &lt; C))[0]</span><br><span class="line">            for i in nonBoundIs:</span><br><span class="line">                alphaPairsChanged += innerL(i, oS)</span><br><span class="line">                print(&quot;non-bound, iter: %d i:%d, pairs changed %d&quot; % (iter, i, alphaPairsChanged))</span><br><span class="line">            iter += 1</span><br><span class="line">        if entireSet:</span><br><span class="line">            entireSet = False</span><br><span class="line">        elif (alphaPairsChanged == 0):</span><br><span class="line">            entireSet = True</span><br><span class="line">        print(&quot;iteration number: %d&quot; % iter)</span><br><span class="line">    return oS.b, oS.alphas</span><br><span class="line"></span><br><span class="line">def calcWs(alphas, dataArr, classLabels):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    利用计算出的alpha进行分类</span><br><span class="line">    :param alphas:</span><br><span class="line">    :param dataArr:</span><br><span class="line">    :param classLabels:</span><br><span class="line">    :return:</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    X = mat(dataArr)</span><br><span class="line">    labelMat = mat(classLabels).transpose()</span><br><span class="line">    m, n = shape(X)</span><br><span class="line">    w = zeros((n, 1))</span><br><span class="line">    for i in range(m):</span><br><span class="line">        w += multiply(alphas[i]*labelMat[i], X[i, :].T)</span><br><span class="line">    return w</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">for i in range(100):</span><br><span class="line">        if alphas[i] &gt; 0.0:</span><br><span class="line">            print(dateArr[i], labelArr[i])</span><br><span class="line">    ws = calcWs(alphas, dateArr, labelArr)</span><br><span class="line">    print(ws)</span><br><span class="line">    datmat = mat(dateArr)</span><br><span class="line">    print(datmat[0]*mat(ws) + b)</span><br></pre></td></tr></table></figure>

<p><img lazyload src="/images/loading.svg" data-src="15384718527848.jpg"></p>
<p>最后一行为测试结果，小于0属于-1类，大于0属于1类，等于0属于-1类</p>
<h4 id="0x03-kernel"><a href="#0x03-kernel" class="headerlink" title="0x03 kernel"></a>0x03 kernel</h4><p><img lazyload src="/images/loading.svg" data-src="15384719736061.jpg"></p>
<ul>
<li>将数据映射到高维空间</li>
</ul>
<p>将数据从一个特征空间转换到另一个特征空间<br>映射会将低维特征空间映射到高维空间</p>
<ul>
<li>径向基核函数</li>
</ul>
<p><img lazyload src="/images/loading.svg" data-src="15384721217035.jpg"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def kernelTrans(X, A, kTup):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    核函数</span><br><span class="line">    :param X:</span><br><span class="line">    :param A:</span><br><span class="line">    :param kTup:包含核函数信息的元组</span><br><span class="line">    :return:</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    m, n = shape(X)</span><br><span class="line">    K = mat(zeros((m, 1)))</span><br><span class="line">    if kTup[0]==&#x27;lin&#x27;:</span><br><span class="line">        K = X * A.T</span><br><span class="line">    elif kTup[0]==&#x27;rbf&#x27;:</span><br><span class="line">        for j in range(m):</span><br><span class="line">            deltaRow = X[j, :] - A</span><br><span class="line">            K[j] = deltaRow*deltaRow.T</span><br><span class="line">        K = exp(K/(-1*kTup[1]**2))</span><br><span class="line">    else:</span><br><span class="line">        raise NameError(&#x27;Houston We Have a Problem -- That Kernel is not recognized&#x27;)</span><br><span class="line">    return K</span><br></pre></td></tr></table></figure>

<p>测试</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">def testRbf(k1=1.3):</span><br><span class="line">    dataArr,labelArr = loadDataSet(&#x27;testSetRBF.txt&#x27;)</span><br><span class="line">    b,alphas = smoP(dataArr, labelArr, 200, 0.0001, 10000, (&#x27;rbf&#x27;, k1))</span><br><span class="line">    datMat=mat(dataArr)</span><br><span class="line">    labelMat = mat(labelArr).transpose()</span><br><span class="line">    svInd=nonzero(alphas.A&gt;0)[0]</span><br><span class="line">    sVs=datMat[svInd]</span><br><span class="line">    labelSV = labelMat[svInd]</span><br><span class="line">    print(&quot;there are %d Support Vectors&quot; % shape(sVs)[0])</span><br><span class="line">    m, n = shape(datMat)</span><br><span class="line">    errorCount = 0</span><br><span class="line">    for i in range(m):</span><br><span class="line">        kernelEval = kernelTrans(sVs, datMat[i, :], (&#x27;rbf&#x27;, k1))</span><br><span class="line">        predict = kernelEval.T * multiply(labelSV, alphas[svInd]) + b</span><br><span class="line">        if sign(predict) != sign(labelArr[i]):</span><br><span class="line">            errorCount += 1</span><br><span class="line">    print(&quot;the training error rate is: %f&quot; % (float(errorCount)/m))</span><br><span class="line">    dataArr, labelArr = loadDataSet(&#x27;testSetRBF2.txt&#x27;)</span><br><span class="line">    errorCount = 0</span><br><span class="line">    datMat=mat(dataArr)</span><br><span class="line">    labelMat = mat(labelArr).transpose()</span><br><span class="line">    m,n = shape(datMat)</span><br><span class="line">    for i in range(m):</span><br><span class="line">        kernelEval = kernelTrans(sVs, datMat[i, :], (&#x27;rbf&#x27;, k1))</span><br><span class="line">        predict = kernelEval.T * multiply(labelSV, alphas[svInd]) + b</span><br><span class="line">        if sign(predict) != sign(labelArr[i]):</span><br><span class="line">            errorCount += 1</span><br><span class="line">    print(&quot;the test error rate is: %f&quot; % (float(errorCount)/m))</span><br></pre></td></tr></table></figure>

<p><img lazyload src="/images/loading.svg" data-src="15384726353561.jpg"></p>
<h4 id="0x04-实例1"><a href="#0x04-实例1" class="headerlink" title="0x04 实例1"></a>0x04 实例1</h4><p>基于SVM的手写数字识别</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">def testDigits(kTup=(&#x27;rbf&#x27;, 10)):</span><br><span class="line">    dataArr,labelArr = loadImages(&#x27;../Ch02/digits/trainingDigits&#x27;)</span><br><span class="line">    b,alphas = smoP(dataArr, labelArr, 200, 0.0001, 10000, kTup)</span><br><span class="line">    datMat=mat(dataArr)</span><br><span class="line">    labelMat = mat(labelArr).transpose()</span><br><span class="line">    svInd=nonzero(alphas.A &gt; 0)[0]</span><br><span class="line">    sVs=datMat[svInd] </span><br><span class="line">    labelSV = labelMat[svInd]</span><br><span class="line">    print(&quot;there are %d Support Vectors&quot; % shape(sVs)[0])</span><br><span class="line">    m, n = shape(datMat)</span><br><span class="line">    errorCount = 0</span><br><span class="line">    for i in range(m):</span><br><span class="line">        kernelEval = kernelTrans(sVs, datMat[i, :], kTup)</span><br><span class="line">        predict = kernelEval.T * multiply(labelSV, alphas[svInd]) + b</span><br><span class="line">        if sign(predict) != sign(labelArr[i]):</span><br><span class="line">            errorCount += 1</span><br><span class="line">    print(&quot;the training error rate is: %f&quot; % (float(errorCount)/m))</span><br><span class="line">    dataArr, labelArr = loadImages(&#x27;testDigits&#x27;)</span><br><span class="line">    errorCount = 0</span><br><span class="line">    datMat=mat(dataArr)</span><br><span class="line">    labelMat = mat(labelArr).transpose()</span><br><span class="line">    m, n = shape(datMat)</span><br><span class="line">    for i in range(m):</span><br><span class="line">        kernelEval = kernelTrans(sVs, datMat[i, :], kTup)</span><br><span class="line">        predict=kernelEval.T * multiply(labelSV, alphas[svInd]) + b</span><br><span class="line">        if sign(predict)!=sign(labelArr[i]):</span><br><span class="line">            errorCount += 1</span><br><span class="line">    print(&quot;the test error rate is: %f&quot; % (float(errorCount)/m))</span><br></pre></td></tr></table></figure>


<p><img lazyload src="/images/loading.svg" data-src="15384734295024.jpg"></p>
<h4 id="0x05-实例2"><a href="#0x05-实例2" class="headerlink" title="0x05 实例2"></a>0x05 实例2</h4><p>XSS Detection</p>
<h5 id="0x01-数据"><a href="#0x01-数据" class="headerlink" title="0x01 数据"></a>0x01 数据</h5><p>在github上看到<a class="link" target="_blank" rel="noopener" href="https://github.com/SparkSharly/DL_for_xss">https://github.com/SparkSharly/DL_for_xss<i class="fas fa-external-link-alt"></i></a> 这个项目，感觉不错，学习一下，数据集项目中已经附带，就直接使用了</p>
<ul>
<li>eg. normal_examples.csv （20w+取部分）</li>
</ul>
<p><img lazyload src="/images/loading.svg" data-src="15364860425700.jpg"></p>
<ul>
<li>eg. xssed.csv （4W+取部分）</li>
</ul>
<p><img lazyload src="/images/loading.svg" data-src="15364859847558.jpg"></p>
<h5 id="0x02-分词"><a href="#0x02-分词" class="headerlink" title="0x02 分词"></a>0x02 分词</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">def GeneSeg(payload):</span><br><span class="line">    #数字泛化为&quot;0&quot;</span><br><span class="line">    payload=payload.lower()</span><br><span class="line">    payload=unquote(unquote(payload))</span><br><span class="line">    payload,num=re.subn(r&#x27;\d+&#x27;,&quot;0&quot;,payload)</span><br><span class="line">    #替换url为”http://u</span><br><span class="line">    payload,num=re.subn(r&#x27;(http|https)://[a-zA-Z0-9\.@&amp;/#!#\?]+&#x27;, &quot;http://u&quot;, payload)</span><br><span class="line">    #分词</span><br><span class="line">    r = &#x27;&#x27;&#x27;</span><br><span class="line">        (?x)[\w\.]+?\(</span><br><span class="line">        |\)</span><br><span class="line">        |&quot;\w+?&quot;</span><br><span class="line">        |&#x27;\w+?&#x27;</span><br><span class="line">        |http://\w</span><br><span class="line">        |&lt;/\w+&gt;</span><br><span class="line">        |&lt;\w+&gt;</span><br><span class="line">        |&lt;\w+</span><br><span class="line">        |\w+=</span><br><span class="line">        |&gt;</span><br><span class="line">        |[\w\.]+</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    return nltk.regexp_tokenize(payload, r)</span><br></pre></td></tr></table></figure>

<p><img lazyload src="/images/loading.svg" data-src="15364866937362.jpg"></p>
<h5 id="0x03-特征"><a href="#0x03-特征" class="headerlink" title="0x03 特征"></a>0x03 特征</h5><ul>
<li>建立xss语义模型，构建词汇表</li>
</ul>
<p>统计高频出现的300词构建词表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">words=[]</span><br><span class="line">datas=[]</span><br><span class="line">with open(&quot;data/xssed.csv&quot;,&quot;r&quot;,encoding=&quot;utf-8&quot;) as f:</span><br><span class="line">    reader=csv.DictReader(f,fieldnames=[&quot;payload&quot;])</span><br><span class="line">    for row in reader:</span><br><span class="line">        payload=row[&quot;payload&quot;]</span><br><span class="line">        word=GeneSeg(payload)</span><br><span class="line">        datas.append(word)</span><br><span class="line">        words+=word</span><br><span class="line"></span><br><span class="line">#构建数据集</span><br><span class="line">def build_dataset(datas,words):</span><br><span class="line">    count=[[&quot;UNK&quot;,-1]]</span><br><span class="line">    counter=Counter(words)</span><br><span class="line">    count.extend(counter.most_common(vocabulary_size-1))</span><br><span class="line">    #print(count)</span><br><span class="line">    vocabulary=[c[0] for c in count]</span><br><span class="line">    #print(vocabulary)</span><br><span class="line">    data_set=[]</span><br><span class="line">    for data in datas:</span><br><span class="line">        d_set=[]</span><br><span class="line">        for word in data:</span><br><span class="line">            if word in vocabulary:</span><br><span class="line">                d_set.append(word)</span><br><span class="line">            else:</span><br><span class="line">                d_set.append(&quot;UNK&quot;)</span><br><span class="line">                count[0][1]+=1</span><br><span class="line">        data_set.append(d_set)</span><br><span class="line">    print(data_set)</span><br></pre></td></tr></table></figure>

<ul>
<li>word2vec建模</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model=Word2Vec(data_set,size=embedding_size,window=skip_window,negative=num_sampled,iter=num_iter)</span><br></pre></td></tr></table></figure>

<p><img lazyload src="/images/loading.svg" data-src="15364964059172.jpg"></p>
<p>空间维度设置为32维</p>
<p><img lazyload src="/images/loading.svg" data-src="15364985492542.jpg"></p>
<p>查看建模结果，与<code>&lt;/script&gt;</code>最语义最相近的词</p>
<p><img lazyload src="/images/loading.svg" data-src="15364953220505.jpg"></p>
<ul>
<li>数据处理</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">def pre_process():</span><br><span class="line">    with open(vec_dir,&quot;rb&quot;) as f :</span><br><span class="line">        word2vec=pickle.load(f)</span><br><span class="line">        #词表（&#x27;UNK&#x27;: 0, &#x27;0&#x27;: 1）</span><br><span class="line">        dictionary=word2vec[&quot;dictionary&quot;]</span><br><span class="line">        #维度值</span><br><span class="line">        embeddings=word2vec[&quot;embeddings&quot;]</span><br><span class="line">        #反向词表（num和word调换，0: &#x27;UNK&#x27;, 1: &#x27;0&#x27;）</span><br><span class="line">        reverse_dictionary = word2vec[&quot;reverse_dictionary&quot;]</span><br><span class="line">    xssed_data=[]</span><br><span class="line">    normal_data=[]</span><br><span class="line">    with open(&quot;data/xssed.csv&quot;,&quot;r&quot;,encoding=&quot;utf-8&quot;) as f:</span><br><span class="line">        reader = csv.DictReader(f, fieldnames=[&quot;payload&quot;])</span><br><span class="line">        for row in reader:</span><br><span class="line">            payload=row[&quot;payload&quot;]</span><br><span class="line">            #分词[&#x27;search=&#x27;, &#x27;&lt;/script&gt;&#x27;, &#x27;&lt;img&#x27;, &#x27;src=&#x27;, &#x27;worksinchrome&#x27;, &#x27;colon&#x27;, &#x27;prompt&#x27;, &#x27;x0&#x27;, &#x27;0&#x27;, &#x27;x0&#x27;, &#x27;onerror=&#x27;, &#x27;eval(&#x27;, &#x27;src&#x27;, &#x27;)&#x27;, &#x27;&gt;&#x27;]</span><br><span class="line">            word=GeneSeg(payload)</span><br><span class="line">            xssed_data.append(word)</span><br><span class="line">    with open(&quot;data/normal_examples.csv&quot;,&quot;r&quot;,encoding=&quot;utf-8&quot;) as f:</span><br><span class="line">        reader = csv.DictReader(f, fieldnames=[&quot;payload&quot;])</span><br><span class="line">        for row in reader:</span><br><span class="line">            payload=row[&quot;payload&quot;]</span><br><span class="line">            word=GeneSeg(payload)</span><br><span class="line">            normal_data.append(word)</span><br><span class="line">    xssed_num=len(xssed_data)</span><br><span class="line">    normal_num=len(normal_data)</span><br><span class="line">    #生成标签[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</span><br><span class="line">    xssed_labels=[1]*xssed_num</span><br><span class="line">    normal_labels=[0]*normal_num</span><br><span class="line">    datas=xssed_data+normal_data</span><br><span class="line">    labels=xssed_labels+normal_labels</span><br><span class="line">    def to_index(data):</span><br><span class="line">        d_index=[]</span><br><span class="line">        for word in data:</span><br><span class="line">            if word in dictionary.keys():</span><br><span class="line">                d_index.append(dictionary[word])</span><br><span class="line">            else:</span><br><span class="line">                d_index.append(dictionary[&quot;UNK&quot;])</span><br><span class="line">        return d_index</span><br><span class="line">    #数据转换[23, 5, 34, 14, 0, 0, 0, 0, 1, 0, 81, 0, 0, 3, 2]</span><br><span class="line">    datas_index=[to_index(data) for data in datas]</span><br><span class="line">    #长度不足maxlen的用-1在前端填充</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    [[ -1  -1  -1 ...   0   3   2]</span><br><span class="line">    [ -1  -1  -1 ...  10  17   1]</span><br><span class="line">    [ -1  -1  -1 ... 150   0  71]</span><br><span class="line">    ...</span><br><span class="line">    [ -1  -1  -1 ...  11   2  55]</span><br><span class="line">    [ -1  -1  -1 ...   5  24   1]</span><br><span class="line">    [ -1  -1  -1 ...   1   3   5]]</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    datas_index=pad_sequences(datas_index,value=-1,maxlen=maxlen)</span><br><span class="line">    #从有序列表中选k个作为一个片段返回，eg.[7, 6, 3, 2, 5, 8, 0, 1, 10, 4, 9]</span><br><span class="line">    rand=random.sample(range(len(datas_index)),len(datas_index))</span><br><span class="line">    #数据简单随机排序</span><br><span class="line">    datas=[datas_index[index] for index in rand]</span><br><span class="line">    labels=[labels[index] for index in rand]</span><br><span class="line"></span><br><span class="line">    datas_embed=[]</span><br><span class="line">    #获取UNK的维度，本例中是32</span><br><span class="line">    dims=len(embeddings[&quot;UNK&quot;])</span><br><span class="line">    n=0</span><br><span class="line">    for data in datas:</span><br><span class="line">        data_embed = []</span><br><span class="line">        for d in data:</span><br><span class="line">            if d != -1:</span><br><span class="line">                #如果不是填充数据，就把真实纬度值替换</span><br><span class="line">                data_embed.extend(embeddings[reverse_dictionary[d]])</span><br><span class="line">            else:</span><br><span class="line">                data_embed.extend([0.0] * dims)</span><br><span class="line">        datas_embed.append(data_embed)</span><br><span class="line">        &#x27;&#x27;&#x27;</span><br><span class="line">        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, </span><br><span class="line">        0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, </span><br><span class="line">        0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,··· -0.5644003, 0.41219762, -1.2313833, -1.3566964, </span><br><span class="line">        -0.74316794, -1.2668883, 1.0586963, 1.5969143, 0.21956278, 1.1538218, -0.35007623, 0.21183407, </span><br><span class="line">        -0.53830135, 1.7361579, -0.08175806, -1.1915175, -1.7790002, -1.1044971, 0.40857738]</span><br><span class="line">        &#x27;&#x27;&#x27;</span><br><span class="line">        n+=1</span><br><span class="line">        if n%10000 ==0:</span><br><span class="line">            print(n)</span><br><span class="line">    #七成训练，三成测试 </span><br><span class="line">    train_datas,test_datas,train_labels,test_labels=train_test_split(datas_embed,labels,test_size=0.3)</span><br><span class="line">    return train_datas,test_datas,train_labels,test_labels</span><br></pre></td></tr></table></figure>


<h5 id="0x04-SVM训练"><a href="#0x04-SVM训练" class="headerlink" title="0x04 SVM训练"></a>0x04 SVM训练</h5><p>通过SVM算法进行模型训练</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">train_datas, train_labels=pre_process()</span><br><span class="line">print(&quot;Start Train Job! &quot;)</span><br><span class="line">start = time.time()</span><br><span class="line">model=LinearSVC()</span><br><span class="line">model = SVC(C=1.0, kernel=&quot;linear&quot;)</span><br><span class="line">model.fit(train_datas,train_labels)</span><br><span class="line">model.save(model_dir)</span><br><span class="line">end = time.time()</span><br><span class="line">print(&quot;Over train job in %f s&quot; % (end - start))</span><br><span class="line">print(&quot;Start Test Job!&quot;)</span><br><span class="line">start=time.time()</span><br><span class="line">pre=model.predict(test_datas)</span><br><span class="line">end=time.time()</span><br><span class="line">print(&quot;Over test job in %s s&quot;%(end-start))</span><br><span class="line">precision = precision_score(test_labels, pre)</span><br><span class="line">recall = recall_score(test_labels, pre)</span><br><span class="line">print(&quot;Precision score is :&quot;, precision)</span><br><span class="line">print(&quot;Recall score is :&quot;, recall)</span><br><span class="line">with open(model_dir,&quot;wb&quot;) as f:</span><br><span class="line">    pickle.dump(model,f,protocol=2)</span><br><span class="line">print(&quot;wirte to &quot;,model_dir)</span><br></pre></td></tr></table></figure>
<p>精确率和召回率：</p>
<p><img lazyload src="/images/loading.svg" data-src="15365002216054.jpg"></p>

                </div>

                <div class="home-article-meta-info-container">
    <div class="home-article-meta-info">
        <span><i class="fas fa-history"></i>&nbsp;<span class="home-article-date" data-date="Mon Jul 22 2019 21:00:00 GMT+0800">2019-07-22</span></span>
        
            <span class="home-article-category"><i class="fas fa-folder"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/categories/%E5%AE%89%E5%85%A8%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/">安全数据分析</a>&nbsp;
                        </li>
                    
                    
                </ul>
            </span>
        
        
            <span class="home-article-tag">
                <i class="fas fa-tags"></i>&nbsp;
                <ul>
                    
                        <li>
                            
                            <a href="/tags/ML/">ML</a>&nbsp;
                        </li>
                    
                </ul>
            </span>
        
    </div>

    <a href="/Support%20Vector%20Machines%EF%BC%88SVM%EF%BC%89/">Read more&nbsp;<i class="fas fa-angle-right"></i></a>
</div>

            </li>
        
    </ul>

    <div class="home-paginator">
        <div class="paginator">
    
        <a class="prev btn"
           href="/page/6/"
        >Prev</a>
    

    
        <a class="next btn"
           href="/page/8/"
        >Next</a>
    
</div>

    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
              <span>2016</span>&nbsp;-&nbsp;
            
            2024&nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;<a target="_blank" rel="noopener" href="//langu.xyz">AboutME:langu_xyz</a>
        </div>
        <!--
            <script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv">
                        Visitor Count&nbsp;<span id="busuanzi_value_site_uv"></span>&ensp;
                    </span>
                
                
                    <span id="busuanzi_container_site_pv">
                        Totalview&nbsp;<span id="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        -->
        <div class="theme-info info-item">
            Powered by <a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;|&nbsp;Theme&nbsp;<a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.4.3</a>
        </div>
        
    </div>
</footer>

        </div>
    </div>

    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-expand-width flex-center">
            <i class="fas fa-arrows-alt-h"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        
            <li class="tools-item rss flex-center">
                <a class="flex-center"
                   href="/atom.xml"
                   target="_blank"
                >
                    <i class="fas fa-rss"></i>
                </a>
            </li>
        

        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="fas fa-arrow-up"></i>
            </li>
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="Search..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>





<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>



    
<script src="/js/local-search.js"></script>




    
<script src="/js/code-copy.js"></script>




    
<script src="/js/lazyload.js"></script>



<div class="post-scripts pjax">
    
</div>


    
<script src="/js/libs/pjax.min.js"></script>

<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax'
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            KEEP.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            KEEP.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            KEEP.refresh();
        });
    });
</script>



</body>
</html>
